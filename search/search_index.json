{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Software notes The nature of this repository is applied, esp. setup of software in several categories. This distinguishes itself from Computational-Statistics which is somewhat more rudimentary. Genetic analysis: Association analysis , CRISPR , Next-generation sequence analysis , Pharmacogenomics , Proteome and transcriptome , Miscellaneous software , scRNA-Seq . Statistical learning Artificial intelligence While there are multiple sources (e.g., Linux archive, Anaconda and Linuxbrew) to get the pre-built executables up-running, they may work less well compared to those built from the latest source distributions. With the later option we can always download the latest release, make minor changes, test the documentation example(s) and give feedback to the developers. Recent usage notes are given in cambridge-ceu/csd3 .","title":"Overview"},{"location":"#software-notes","text":"The nature of this repository is applied, esp. setup of software in several categories. This distinguishes itself from Computational-Statistics which is somewhat more rudimentary. Genetic analysis: Association analysis , CRISPR , Next-generation sequence analysis , Pharmacogenomics , Proteome and transcriptome , Miscellaneous software , scRNA-Seq . Statistical learning Artificial intelligence While there are multiple sources (e.g., Linux archive, Anaconda and Linuxbrew) to get the pre-built executables up-running, they may work less well compared to those built from the latest source distributions. With the later option we can always download the latest release, make minor changes, test the documentation example(s) and give feedback to the developers. Recent usage notes are given in cambridge-ceu/csd3 .","title":"Software notes"},{"location":"AA/","text":"Association analysis Data management bgenix As documented, the current version requires gcc 4.7* so we proceed as follows, hg clone https://gavinband@bitbucket.org/gavinband/bgen -u master cd bgen module load gcc/4.7.2 ./waf configure --prefix=/scratch/jhz22 ./waf ./waf install See https://bitbucket.org/gavinband/bgen/overview. Single variant analysis eigensoft The PCA software for genomewide data is available from https://www.hsph.harvard.edu/alkes-price/software/ as well as Ubuntu. sudo apt install eigensoft The executables are eigenstrat, eigenstratQTL, smarteigenstrat, smartpca, pca, etc. GEMMA To build from source, https://github.com/genetics-statistics/GEMMA, the Makefile needs to change in places with OpenBLAS, /opt/OpenBLAS/. METAL Note METAL aligns alleles according to the first file processed. At least cmake 3.1 is required for the latest from GitHub, https://github.com/statgen/METAL, wget -qO- https://github.com/statgen/METAL/archive/2018-08-28.tar.gz | \\ tar xvfz - cd METAL-2018-08-28 mkdir build && cd build cmake .. make make test make install One can use ccmake . to change the prefix for installation but this does not appear to work and the executable is bin/metal. As with distribution 2011-03-25, http://csg.sph.umich.edu/abecasis/Metal/download/, options CUSTOMVARIABLE uses an output format of %g, leading to scientific notation of position, which is undesirable and we modify metal/Main.cpp from for (int j = 0; j < customVariables.Length(); j++) fprintf(f, \"\\t%g\", custom[j][marker]); to for (int j = 0; j < customVariables.Length(); j++) fprintf(f, \"\\t%-.15g\", custom[j][marker]); which is left-aligned with 15 places with %g though largely 11 is enough. The change can be tested by adding the following lines to examples/GlucoseExample/meta.txt. CUSTOMVARIABLE CHR LABEL CHR as CHR CUSTOMVARIABLE POS LABEL POS as POS CUSTOMVARIABLE N LABEL N as N CHROMOSOMELABEL CHR POSITIONLABEL POS TRACKPOSITIONS ON Nevertheless the CHR and POS thus retained are sums of individual studies involved for particular positions so their real values can be recovered from these divided by the number of - and + from the Direction column. In the case of N, the sum is just what we want. To wrap up, our testing code is as follows, ### illustration as in examples/GlucoseExample/metal.tbl of TRACKPOSITIONS and change in source ### the results are also sorted in accordance with METAL documentation cd examples/GlucoseExample ( echo CUSTOMVARIABLE CHR echo LABEL CHR as CHR echo CUSTOMVARIABLE POS echo LABEL POS as POS echo CUSTOMVARIABLE N echo LABEL N as N echo CHROMOSOMELABEL CHR echo POSITIONLABEL POS echo TRACKPOSITIONS ON echo OUTFILE metal- .tbl awk '!/\\#/' metal.txt ) > metal.metal metal metal.metal ( head -1 metal-1.tbl awk 'NR>1' metal-1.tbl | \\ awk ' { FS=OFS=\"\\t\" direction=$9 gsub(/\\?/,\"\",direction) n=length(direction) $10=$10/n $11=$11/n };1' | \\ sort -k10,10n -k11,11n ) > metal.tbl rm metal-1.tbl mv metal-1.tbl.info metal.tbl.info cd - Another extension relates to heterogeneity analysis, e.g., I 2 > 30 we require at least three studies each attaining P <= 0.05. In this case, we extend the direction field as in direction[marker] = z == 0.0 ? '0' : (z > 0.0 ? '+' : '-'); to direction[marker] = z == 0.0 ? '0' : (z > 0.0 ? '+' : '-'); direction[marker] = (fabs(z) * sqrt(w) < 1.959964) ? direction[marker] : (z > 0.0 ? 'p' : 'n'); for both ProcessFile() and ReProcessFile(). It is then relatively easy to filter on meta-analysis statistics, awk -f metal.awk 4E.BP1-1.tbl , where metal.awk has the following lines, { d3=$13; gsub(/?/,\"\",d3) if (length(d3) >= 3 && $18 >= 3500) if ($12 > -9.30103) print; else { if ($14 < 30) print; else { d3n=d3; d3p=d3; gsub(/+|-|p/,\"\",d3n); gsub(/+|-|n/,\"\",d3p); if (length(d3n) >= 3 || length(d3p) >= 3) print; } } } # R # > log10(5e-10) # [1] -9.30103 # head -1 METAL/4E.BP1-1.tbl | sed 's|\\t|\\n|g' | awk '{print \"#\" NR,$1}' #1 Chromosome #2 Position #3 MarkerName #4 Allele1 #5 Allele2 #6 Freq1 #7 FreqSE #8 MinFreq #9 MaxFreq #10 Effect #11 StdErr #12 log(P) #13 Direction #14 HetISq #15 HetChiSq #16 HetDf #17 logHetP #18 N METASOFT and ForestPMPlot Available from http://genetics.cs.ucla.edu/meta/ mkdir METASOFT wget -qO- http://genetics.cs.ucla.edu/meta/repository/2.0.1/Metasoft.zip | \\ unzip Metasoft.zip java -jar Metasoft.jar -input example.txt cd - The results are in file out . One can also work with ForestPMPLot similarly. mtag https://github.com/omeed-maghzian/mtag PyLMM The software is rare with its setup for GEI studies accounting for polygenic effects. pylmm It is necessary to get it going with code in the quick guide, git clone https://github.com/nickFurlotte/pylmm make the following changes, build/scripts-2.7/pylmmGWAS.py, line 207, from keep = True - v to keep = True ^ v pylmm/lmm.py, line 189, from if X0 == None: to if X0.all == None: ; line 193, from x = True - np.isnan(Y) to x = True ^ np.isnan(Y) ; line 272, from if X == None: X = self.X0t to if X.all == None: X = self.X0t . build/scripts-2.7/input.py, line 190, from x = True ^ np.isnan(G) to x = True ^ np.isnan(G) . python setup.py install use the documentation call, pylmmGWAS.py -v --bfile data/snps.132k.clean.noX --kfile data/snps.132k.clean.noX.pylmm.kin --phenofile data/snps.132k.clean.noX.fake.phenos out.foo pylmm_zarlab Make the following changes to lmm and lmmGWAS similar to pylmm, and then issue bash run_tests.sh . EReading SNP input... Read 1219 individuals from data/snps.132k.clean.noX.fam Reading kinship... Read the 1219 x 1219 kinship matrix in 1.139s 1 number of phenotypes read Traceback (most recent call last): File \"scripts/pylmmGWAS.py\", line 308, in <module> keep = True - v TypeError: numpy boolean subtract, the `-` operator, is deprecated, use the bitwise_xor, the `^` operator, or the logical_xor function instead. EReading PLINK input... Read 1219 individuals from data/snps.132k.clean.noX.fam Traceback (most recent call last): File \"scripts/pylmmKinship.py\", line 127, in <module> K_G = lmm.calculateKinshipIncremental(IN, numSNPs=options.numSNPs, AttributeError: 'module' object has no attribute 'calculateKinshipIncremental' EReading PLINK input... Read 1219 individuals from data/snps.132k.clean.noX.fam Traceback (most recent call last): File \"scripts/pylmmKinship.py\", line 127, in <module> K_G = lmm.calculateKinshipIncremental(IN, numSNPs=options.numSNPs, AttributeError: 'module' object has no attribute 'calculateKinshipIncremental' ====================================================================== ERROR: test_GWAS (tests.test_lmm.test_lmm) ---------------------------------------------------------------------- Traceback (most recent call last): File \"/home/jhz22/D/genetics/ucla/pylmm_zarlab/tests/test_lmm.py\", line 41, in test_GWAS TS,PS = lmm.GWAS(Y,snps,K,REML=True,refit=True) File \"/home/jhz22/D/genetics/ucla/pylmm_zarlab/pylmm/lmm.py\", line 192, in GWAS L = LMM(Y, K, Kva, Kve, X0) File \"/home/jhz22/D/genetics/ucla/pylmm_zarlab/pylmm/lmm.py\", line 301, in __init__ x = True - np.isnan(Y) TypeError: numpy boolean subtract, the `-` operator, is deprecated, use the bitwise_xor, the `^` operator, or the logical_xor function instead. ====================================================================== ERROR: test_calculateKinship (tests.test_lmm.test_lmm) ---------------------------------------------------------------------- Traceback (most recent call last): File \"/home/jhz22/D/genetics/ucla/pylmm_zarlab/tests/test_lmm.py\", line 25, in test_calculateKinship K = lmm.calculateKinship(snps) File \"/home/jhz22/D/genetics/ucla/pylmm_zarlab/pylmm/lmm.py\", line 135, in calculateKinship mn = W[True - np.isnan(W[:, i]), i].mean() TypeError: numpy boolean subtract, the `-` operator, is deprecated, use the bitwise_xor, the `^` operator, or the logical_xor function instead. ====================================================================== ERROR: test_pylmmGWASScript (tests.test_pylmmGWAS.test_pylmmGWAS) ---------------------------------------------------------------------- Traceback (most recent call last): File \"/home/jhz22/D/genetics/ucla/pylmm_zarlab/tests/test_pylmmGWAS.py\", line 24, in test_pylmmGWASScript with (open(self._outputFile, 'r')) as ansFile: IOError: [Errno 2] No such file or directory: 'data/pylmmGWASTestOutput' ====================================================================== ERROR: test_pylmmKinshipScript1 (tests.test_pylmmKinship.test_pylmmKinship) ---------------------------------------------------------------------- Traceback (most recent call last): File \"/home/jhz22/D/genetics/ucla/pylmm_zarlab/tests/test_pylmmKinship.py\", line 24, in test_pylmmKinshipScript1 K = np.fromfile(open(self._outputFile, 'r'), sep=\" \") IOError: [Errno 2] No such file or directory: 'data/pylmmKinshipTestOutput' ====================================================================== ERROR: test_pylmmKinshipScript2 (tests.test_pylmmKinship.test_pylmmKinship) ---------------------------------------------------------------------- Traceback (most recent call last): File \"/home/jhz22/D/genetics/ucla/pylmm_zarlab/tests/test_pylmmKinship.py\", line 38, in test_pylmmKinshipScript2 K = np.fromfile(open(self._outputFile, 'r'), sep=\" \") IOError: [Errno 2] No such file or directory: 'data/pylmmKinshipTestOutput' ---------------------------------------------------------------------- Ran 5 tests in 2.776s FAILED (errors=5) We can have a test of GxE analysis as this, sudo python setup.py install cd pylmm python pylmm_GXE.py In general, we can see options for GxE analysis from command pylmmGWAS.py under bash. seqMeta seqMeta: Meta-Analysis of Region-Based Tests of Rare DNA Variants, https://cran.r-project.org/web/packages/seqMeta/index.html. Quanto https://preventivemedicine.usc.edu/download-quanto/ QUICKTEST See https://wp.unil.ch/sgg/quicktest/ for the latest with support for bgen v1.2. SNPTEST Available from https://mathgen.stats.ox.ac.uk/genetics_software/snptest/snptest.html#download, e.g., wget http://www.well.ox.ac.uk/~gav/resources/snptest_v2.5.4-beta3_CentOS6.6_x86_64_static.tgz tar xvfz nptest_v2.5.4-beta3_CentOS6.6_x86_64_static.tgz It is possible that one would get error messages !! Error in function: PerVariantComputationManager::get_phenotypes(), argument(s): phenotype_spec=IL.18R1___Q13478:P. !! Quitting. !! Error (HaltProgramWithReturnCode): but they would go away with -method em for instance. SUGEN Genetic Association Analysis Under Complex Survey Sampling, https://github.com/dragontaoran/SUGEN. swiss Software to help identify overlap between association scan results and GWAS hit catalogs. sudo apt install libz-dev pip install git+https://github.com/welchr/swiss.git@v1.0.0 One may try options such as --install_options=\"--prefix=\"\". In case the $HOME directory does not have sufficient space, one can issue swiss --download-data on a system that does, then upload, rsync -av --partial .local/share/swiss login.hpc.cam.ac.uk:$HOME/.local/share or select particular files, sync -av --partial .local/share/swiss/data/ld/1000g.phase3.hg38.EUR.shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz* \\ login.hpc.cam.ac.uk:$HOME/.local/share/swiss/data/ld but this could be tricked with making $HOME/.local/share a symbolic pointing to a directory that can hold more than ~40GB data (reminscent fo VEP), then the pip command above can be called with the --user option. To test, follow these, git clone https://github.com/statgen/swiss cd swiss/test swiss --list-files swiss --list-ld-sources swiss --list-gwas-cats swiss --list-gwas-traits --gwas-cat data/gwascat_ebi_GRCh37p13.tab swiss --assoc data/top_hit_is_gwas.tab --variant-col EPACTS --pval-col PVAL \\ --dist-clump --clump-dist 250000 --clump-p 5e-08 --skip-gwas --out test swiss --assoc data/test_hg19.gz --multi-assoc --trait SM --build hg19 \\ --ld-clump-source 1000G_2014-11_EUR --ld-gwas-source 1000G_2014-11_EUR --gwas-cat data/gwascat_ebi_GRCh37p13.tab \\ --ld-clump --clump-p 1e-10 --out test swiss --assoc data/test_hg38.gz --gwas-cat data/gwascat_ebi_GRCh38p7.tab --variant-col VARIANT \\ --chrom-col CHROM --pos-col POS --trait BMI --build hg38 \\ --ld-clump-source 1000G_2014-11_EUR --ld-gwas-source 1000G_2014-11_EUR \\ --ld-clump --clump-p 5e-08 --out test and consult the online documentation. Population struction fineSTRUCTURE https://people.maths.bris.ac.uk/~madjl/finestructure/index.html HLA imputation HLA*IMP:02 Download source from https://oxfordhla.well.ox.ac.uk/hla/tool/main sudo apt install libgd sudo apt install libgtk-3* then start Perl, sudo perl -MCPAN -e shell followed by install GD install Alien::wxWidgets install Moose install List::MoreUtils install Wx::Mini install Wx::Perl::Packager We can also use cpan, but the installation fails under Ubuntu 18.04. Finemapping CAVIAR/eCAVIAR Installation is made from GitHub in the usual way, git clone https://github.com/fhormoz/caviar.git The software requires libgsl and liblapack which can be installed as follows, sudo apt install liblapack-dev sudo apt install libgsl-dev Once this is done, one can proceed with the compiling, cd caviar/CAVIAR-C++ make cd - CAVIAR -l CAVIAR-C++/sample_data/50_LD.txt -z CAVIAR-C++/sample_data/50_Z.txt -o 50 CAVIAR -l CAVIAR-C++/sample_data/DDB1.top100.sig.SNPs.ld -z CAVIAR-C++/sample_data/DDB1.top100.sig.SNPs.ZScores -o 100 eCAVIAR -l CAVIAR-C++/sample_data/GWAS.ADGC.MC.AD.IGAP.stage1.hg19.chr.11.121344805.121517613.CHRPOSREFALT.LD.ld \\ -l CAVIAR-C++/sample_data/eQTL.CARDIOGENICS.MC.AD.IGAP.stage1.hg19.chr.11.121344805.121517613.CHRPOSREFALT.LD.ld \\ -z CAVIAR-C++/sample_data/GWAS.MC.AD.IGAP.stage1.hg19.chr.11.121344805.121517613.CHRPOSREFALT.Z.txt \\ -z eQTL.CARDIOGENICS.MC.AD.IGAP.stage1.hg19.chr.11.121344805.121517613.CHRPOSREFALT.ILMN_1810712.NM_015313.1.ARHGEF12.Z.txt \\ -o 75 It may be necessary to alter Makefile to point to appropriate -I -L for lapack, for instance. The references are eCAVIAR Hormozdiari F, van de Bunt M, Segr\u00e8 AV, Li X, Joo JWJ, Bilow M, Sul JH, Sankararaman S, Pasaniuc B, Eskin E (2016). Colocalization of GWAS and eQTL Signals Detects Target Genes. Am J Hum Genet 99(6):1245-1260. CAVIAR Hormozdiari F, Kostem E, Kang EY, Pasaniuc B, Eskin E (2014). Identifying causal variants at loci with multiple signals of association. Genetics 198(2):497-508. Both are available from https://github.com/fhormoz/caviar. CAVIARBF wget https://bitbucket.org/Wenan/caviarbf/get/7e428645be5e.zip unzip 7e428645be5e.zip cd Wenan-caviarbf-7e428645be5e make ln -sf $PWD/caviarbf $HOME/bin/caviarbf ln -sf $PWD/model_search $HOME/bin/model_search ./install_r_package.sh cd caviarbf-r-package R --no-save <<END install.packages(\"glmnet\") END R CMD INSTALL caviarbf_0.2.1.tar.gz ./test.sh cd - ./test.sh finemap finemap 1.4 is available from http://www.christianbenner.com/finemap_v1.4_x86_64.tgz. finemap_v1.4_x86_64 --sss --in-files example/master We could also keep the screen output with finemap_v1.4_x86_64 --sss --in-files example/master 2>&1 | tee test.log . LDSTORE v2.0 is available from http://www.christianbenner.com/ldstore_v2.0_x86_64.tgz. ldstore_v2.0_x86_64 --in-files example/master --write-bdose --bdose-version 1.1 ldstore_v2.0_x86_64 --in-files example/master --write-bcor --read-bdose ldstore_v2.0_x86_64 --in-files example/master --bcor-to-text gchromVar Cell type specific enrichments using finemapped variants and quantitative epigenetic data, https://github.com/caleblareau/gchromVAR; see https://github.com/caleblareau/singlecell_bloodtraits for examples. It requires chromVarmotifs from https://github.com/GreenleafLab/chromVARmotifs, which requires gcc 5.2.0. JAM Setup The package is available from https://github.com/pjnewcombe/R2BGLiMS. Note that JAM requires Java 1.8 so call to Java -jar inside the function needs to reflect this, not straightforward with install_github() from devtools but one needs to clone the package, modify the R source code and then install, git clone https://github.com/pjnewcombe/R2BGLiMS ### in case you have java-1.6 you will need to change to java-1.8 in R2BGLiMS/R/R2BGLiMS.R ### and possibly add other options, e.g., ### /usr/lib/jvm/java-8-oracle/bin/java -Xmx4G ### sed -i 's|\\\"java|\\\"/usr/lib/jvm/java-8-oracle/bin/java -Xmx4G||g' R2BGLiMS/R/R2BGLiMS.R ### sudo R CMD INSTALL R2BGLiMS -l $R_LIBS ### if R_LIBS is not set, the default can be used, e.g., $HOME/R R CMD INSTALL R2BGLiMS As shown, it might well be necessary to add options to the Java command-line. Compiling The information is unavailable from the documentation, but can be achieved this with netbeans or in steps. # 21-7-2017 MRC-Epid JHZ export BGLiMS=/genetics/bin/BGLiMS export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.131-0.b11.el6_9.x86_64 # Jama cd $BGLiMS $JAVA_HOME/bin/javac Jama/util/*java $JAVA_HOME/bin/javac -classpath $BGLiMS Jama/*java $JAVA_HOME/bin/jar cvf Jama.jar Jama/*class Jama/util/*class # ssj export SSJHOME=ssj export LD_LIBRARY_PATH=$SSJHOME/lib:$LD_LIBRARY_PATH export CLASSPATH=.:$SSJHOME/lib/ssj.jar:$SSJHOME/lib/colt.jar:$SSJHOME/lib/tcode.jar:$CLASSPATH # MyClassLibrary cd $BGLiMS/MyClassLibrary/src ln -sf $BGLiMS/Jama $JAVA_HOME/bin/javac Methods/*java $JAVA_HOME/bin/javac Objects/*java $JAVA_HOME/bin/jar cvf MyClassLibrary.jar Methods/*class Objects/*class # BGLiMS.jar cd $BGLiMS/src $JAVA_HOME/bin/javac bglims/*java $JAVA_HOME/bin/jar cvf bglims.jar bglims/*class cd $BGLiMS ln -sf $BGLiMS/src/bglims.jar ln -sf MyClassLibrary/src/MyClassLibrary.jar # $JAVA_HOME/bin/jar cvf BGLiMS.jar bglims.jar MyClassLibrary.jar Functional annotation ANNOVAR See https://cambridge-ceu.github.io/csd3/applications/ANNOVAR.html . DAVID https://david.ncifcrf.gov/ fgwas git clone https://github.com/joepickrell/fgwas cd fgwas # sed -i 's/1.14/1.15/g' configure ./configure --prefix=/scratch/jhz22 make sudo make install src/fgwas -i test_data/test_LDL.fgwas_in.gz -w ens_coding_exon git clone https://github.com/joepickrell/1000-genomes git clone https://github.com/joepickrell/1000-genomes-genetic-maps In case boots is unavailable or not up-to-date, it is necessary to install, e.g., wget https://dl.bintray.com/boostorg/release/1.69.0/source/boost_1_69_0.tar.gz tar xvfz boost_1_69_0.tar.gz cd boost_1_60_0 ./bootstrap.sh --prefix=/scratch/jhz22 --exec-prefix=/scratch/jhz22 ./b2 install Nevertheless it is also required to have other dependencies in place. GARFIELD Web: https://www.ebi.ac.uk/birney-srv/GARFIELD/ wget -qO- https://www.ebi.ac.uk/birney-srv/GARFIELD/package-v2/garfield-v2.tar.gz | \\ tar xvfz wget -qO- https://www.ebi.ac.uk/birney-srv/GARFIELD/package-v2/garfield-data.tar.gz | \\ tar xvfz cd garfield-v2 bash garfield Additional details are described in the documentation, https://www.ebi.ac.uk/birney-srv/GARFIELD/documentation-v2/GARFIELD-v2.pdf. gnomAD Web site: https://gnomad.broadinstitute.org/downloads . To use gsutil , following these steps, module load python/3.7 virtualenv py37 source py37/bin/activate pip install gsutil gsutil ls gs://gnomad-public/release gsutil cp gs://gnomad-public/release/2.1.1/constraint/gnomad.v2.1.1.lof_metrics.by_gene.txt.bgz . gsutil cp gs://gnomad-public/release/2.1.1/constraint/gnomad.v2.1.1.lof_metrics.by_transcript.txt.bgz . UCSC has a description here . HaploReg Web: https://pubs.broadinstitute.org/mammals/haploreg/haploreg.php (data files, https://pubs.broadinstitute.org/mammals/haploreg/data/) HaploReg is a tool for exploring annotations of the noncoding genome at variants on haplotype blocks, such as candidate regulatory SNPs at disease-associated loci. Using LD information from the 1000 Genomes Project, linked SNPs and small indels can be visualized along with chromatin state and protein binding annotation from the Roadmap Epigenomics and ENCODE projects, sequence conservation across mammals, the effect of SNPs on regulatory motifs, and the effect of SNPs on expression from eQTL studies. HaploReg is designed for researchers developing mechanistic hypotheses of the impact of non-coding variants on clinical phenotypes and normal variation. morpheus See https://software.broadinstitute.org/morpheus/ . PolyPhen and PolyPhen-2 See http://genetics.bwh.harvard.edu/pph/ and http://genetics.bwh.harvard.edu/pph2/. UNPHASED See https://sites.google.com/site/fdudbridge/software/unphased-3-1. VEP The description is available from http://www.ensembl.org/info/docs/tools/vep/script/vep_download.html. git clone https://github.com/Ensembl/ensembl-vep.git cd ensembl-vep git pull git checkout release/92 perl INSTALL.pl The last line requires modules DBI, Build as described in the LANGUAGES section of Computational-Statistics . Lastly, VEP requires .vep directory at $HOME which can be derived from a centrally-installed VEP under Linux, ln -s /genetics/ensembl-vep/.vep $HOME/.vep ln -s /genetics/ensembl-vep/vep $HOME/bin/vep assuming /genetics/ensembl-vep contains the software. It is slow to get those databases, so one may prefer to get them directly from ftp://ftp.ensembl.org/pub/release-92/variation/VEP/ and unpack into the .vep directory. We can now execute an example, vep -i examples/homo_sapiens_GRCh37.vcf -o out.txt -offline Recent notes on ANNOVAR and VEP are available from here, https://cambridge-ceu.github.io/csd3/applications/VEP.html . R-packages See R-packages section. Pathway analysis DEPICT See the GIANT+Biobank BMI analysis . Installation and documentation example The official site, https://data.broadinstitute.org/mpg/depict/documentation.html has links on DEPICT_v1_rel194.tar.gz , which contains 1000Genomes and other data unavailable from depict_140721.tar.bz2 . wget https://data.broadinstitute.org/mpg/depict/depict_download/bundles/DEPICT_v1_rel194.tar.gz tar xvfz DEPICT_v1_rel194.tar.gz export CWD=$(pwd) where the package is unpacked into the DEPICT/ directory containing the data/ subdirectory. We also note down current working directory with CWD . The source package from GitHub has more features such as cutoff_type to be p-values in network analysis; the code git clone https://github.com/perslab/depict cd depict wget https://data.broadinstitute.org/mpg/depict/depict_download/collections/ld0.5_collection_1000genomespilot_depict_150429.txt.gz mkdir -p data/collections mv ld0.5* data/collections sed 's|/cvar/jhlab/tp/DEPICT|/home/jhz22/Downloads/depict|g;s|label_for_output_files: ldl_teslovich_nature2010|label_for_output_files: test|g; s|/cvar/jhlab/tp/tools/plink/plink-1.09-Sep2015-x86_64/plink|/home/jhz22/bin/plink|g' example/ldl_teslovich_nature2010.cfg > test.cfg src/python/depict.py test.cfg adds ld0.5_collection_1000genomespilot_depict_150429.txt.gz and produces results prefixed with test_ using the LDL data. Since the documentation example above does not give the full results, data directory packaged with DEPICT_v1_rel194.tar.gz above is called to remedy with a minor change, mv data data.sav ln -s $CWD/DEPICT/data to test.cfg for a re-run. sed -i 's|data/reconstituted_genesets/reconstituted_genesets_example.txt|data/reconstituted_genesets/reconstituted_genesets_150901.binary|g' test.cfg src/python/depict.py test.cfg PLINK. PLINK-1.9 , with --clump option, has to be used rather than PLINK2 since itdrops the --clump option. NB template.cfg is from src/python rather than .cfg from example. Python 2.7.*. After installation, the following change is needed: from .sort() to .sort_values() in network_plot.py and depict_library.py. It is necessary to download additional files for network analysis -- in my case, downloads via Firefox do not work and I used wget instead. To explore possibility to replicate the Supplementary Figure 9 of the Scott paper -- the number of significant pathways seemed to fall short of the FDR<=0.05 criterion, see SUMSTATS for setup. Under Windows, gzip.exe is also required at the working directory or %path% plus some changes over directory specifications. We can then execute python depict.py BMI.cfg For tissue plot, one can use pdftopng from XpdfReader (or convert/magick from ImageMagick) to obtain .png files to be incorporated into Excel workbook. For network plot, the python package scikit-learn is required. sudo pip install scikit-learn Recompile This may be necessary for large collection of significant variants, e.g., GIANT+UKB height summary statistics (height_loci.txt has 2,184 lines including header). Start netbeans and open project from depict/src/java, fixing links to colt.jar, commons-math-2.0.jar, Jama-1.0.2.jar, jsci-core.jar, JSciCore.jar, jsc.jar from lib/. Additional notes PW-pipeline puts together many changes and is streamlined with other software. MAGMA A generic setup is available from PW-pipeline , while section CAD of the Omics-analysis repository provides a much simplified version. PASCAL When there is issue with xianyi-OpenBLAS-v0.2.12-0-g7e4e195.zip shipped with PASCAL.zip , as described in vdi.md or git clone https://github.com/xianyi/OpenBLAS it is recommended to use the GitHub version . Change to settings.txt is necessary since by default pathway analysis is disabled. Again we use the BMI summary statistics from GIANT, wget -qO- http://portals.broadinstitute.org/collaboration/giant/images/1/15/SNP_gwas_mc_merge_nogc.tbl.uniq.gz | \\ gunzip -c | cut -f1,7 | awk -vFS=\"\\t\" -vOFS=\"\\t\" '(NR>1)' > BMI.pval Pascal --pval=BMI.pval ToppGene A portal for gene list enrichment analysis and candidate gene prioritization based on functional annotations and protein interactions network See https://toppgene.cchmc.org/ . VEGAS2 It is relatively slow with web interface https://vegas2.qimrberghofer.edu.au, so we would like to try the command-line counterpart. Make sure R packages corpcor and mvtnorm are available, then proceed with # driver download wget https://vegas2.qimrberghofer.edu.au/vegas2v2 # documentation example -- unzip does not accept input from console so we do in two steps wget https://vegas2.qimrberghofer.edu.au/VEGAS2v2example.zip unzip -j VEGAS2v2example.zip # gene-based association perl vegas2v2 -G -snpandp example.txt -custom $PWD/example -glist example.glist -genelist example.genelist -out example # pathway-based association awk '(NR>1){OFS=\"\\t\";gsub(/\"/,\"\",$0);print $2,$8}' example.out > Example.geneandp vegas2v2 -P -geneandp Example.geneandp -glist example.glist -geneandpath Example.vegas2pathSYM -out Example # further setup wget https://vegas2.qimrberghofer.edu.au/biosystems20160324.vegas2pathSYM wget https://vegas2.qimrberghofer.edu.au/glist-hg19 wget -qO- https://vegas2.qimrberghofer.edu.au/g1000p3_EUR.tar.gz | tar xvfz - Somehow the binary files following -custom option needs to be absolute path. The last line downloads and unpacks the LD reference data for European (EUR) population; other options include AFR, AMR, EAS, SAS. Mendelian randomisation Bias and Type 1 error rate for Mendelian randomization with sample overlap, https://sb452.shinyapps.io/overlap/ Power calculation. https://shiny.cnsgenomics.com/mRnd/ . LCV Software in Matlab and R for Latent Causal Variable model inferring genetically causal relationships using GWAS data. https://github.com/lukejoconnor/LCV See R-packages section. Polygenic modeling GCTA It is possible to use dosage, a Bash function is as follows, function INTERVAL_dosage() { if [ ! -f nodup/${pr}.gen.gz ]; then qctool -g nodup/${pr}.bgen -og nodup/${pr}.gen.gz; fi gunzip -c nodup/${pr}.gen.gz | \\ awk -v sample=${sample} ' { N=(NF-6)/3 for(i=1;i<=N;i++) dosage[NR,i]=$((i-1)*3+8)+2*$((i-1)*3+9) } END { i=0; while (getline gf < sample) { split(gf,a); i++ id[i]=a[1] } close(sample) for(i=1;i<=N;i++) { printf id[i+2] \" ML_DOSE\"; for(j=1;j<=NR;j++) printf \" \" dosage[j,i]; printf \"\\n\" } }' | \\ gzip -f > nodup/${pr}.dosage.gz ( echo SNP Al1 Al2 Freq1 MAF Quality Rsq qctool -g ${pr}.bgen -snp-stats -osnp - | \\ sed '1,9d' | \\ cut -f2,5,6,12,14,17,18 | \\ sed 's/\\t/ /g;s/NA/0/g' ) | \\ grep -v Completed | \\ gzip -f > nodup/${pr}.info.gz } gcta-1.9 --dosage-mach-gz nodup/$pr.dosage.gz nodup/$pr.info.gz --make-grm-bin --out nodup/${pr} where pr' is input file root and sample` is the associate sample file from which sample IDs are extracted. When the imputed genotyeps are MaCH-based, it is possible to use DosageConverter . ## assuming you use hpc-work/ with a subdirectory called bin/ cd /rds/user/$USER/hpc-work/ git clone https://github.com/Santy-8128/DosageConvertor cd DosageConverter pip install cget --user module load cmake-3.8.1-gcc-4.8.5-zz55m7x ./install.sh cd /rds/user/$USER/hpc-work/bin/ ln -s /rds/user/$USER/hpc-work/DosageConvertor/release-build/DosageConvertor ## testing DosageConvertor --vcfDose test/TestDataImputedVCF.dose.vcf.gz \\ --info test/TestDataImputedVCF.info \\ --prefix test \\ --type mach gunzip -c test.mach.dose.gz | wc -l DosageConvertor --vcfDose test/TestDataImputedVCF.dose.vcf.gz \\ --info test/TestDataImputedVCF.info \\ --prefix test \\ --type plink gunzip -c test.plink.dosage.gz | wc -l so the MaCH dosage file is individual x genotype whereas PLINK dosage file is genotype x individual. HESS HESS (Heritability Estimation from Summary Statistics) is now available from https://github.com/huwenboshi/hess and has a web page at https://huwenboshi.github.io/hess-0.5/#hess Some popular Python packages such as pandas as well as PySnpTools are required, e.g., pip install pandas or python -m pip install pysnptools and it is doable for the latter with source at https://github.com/MicrosoftGenomics/PySnpTools. For the GIANT height data, we had success with the following script, #!/bin/bash export HEIGHT=https://portals.broadinstitute.org/collaboration/giant/images/0/01/GIANT_HEIGHT_Wood_et_al_2014_publicrelease_HapMapCeuFreq.txt.gz wget -qO- $HEIGHT | \\ awk 'NR>1' | \\ sort -k1,1 | \\ join -13 -21 snp150.txt - | \\ awk '($9!=\"X\" && $9!=\"Y\" && $9!=\"Un\"){if(NR==1) print \"SNP CHR BP A1 A2 Z N\"; else print $1,$2,$3,$4,$5,$7/$8,$10}' > height.tsv.gz # SNP - rs ID of the SNP (e.g. rs62442). # CHR - Chromosome number of the SNP. This should be a number between 1 and 22. # BP - Base pair position of the SNP. # A1 - Effect allele of the SNP. The sign of the Z-score is with respect to this allele. # A2 - The other allele of the SNP. # Z - The Z-score of the SNP. # N - Sample size of the SNP. for chrom in $(seq 22) do python hess.py \\ --local-hsqg height \\ --chrom $chrom \\ --bfile 1kg_eur_1pct/1kg_eur_1pct_chr${chrom} \\ --partition nygcresearch-ldetect-data-ac125e47bf7f/EUR/fourier_ls-chr${chrom}.bed \\ --out step1 done python hess.py --prefix step1 --reinflate-lambda-gc 1 --tot-hsqg 0.8 0.2 --out step2 where snp150.txt from UCSC is described at the SUMSTATS repository, https://github.com/jinghuazhao/SUMSTATS. ldetect It can proceed as indicated sudo pip3 install ldetect into /usr/local/lib/python3.6/dist-packages, or git clone https://bitbucket.org/nygcresearch/ldetect cd ldetect # for super user # sudo python3 setup.py install python3 setup.py install --user git clone https://bitbucket.org/nygcresearch/ldetect-data cd ldetect-data for pop in AFR ASN EUR do awk ' { OFS=\"\\t\" if (NR==1) print \"#chrom\", \"Start\", \"End\", \"Region\" else print $1, $2, $3, \"region\" NR-1 }' $pop/fourier_ls-all.bed > $pop.bed done cd - Population-specific approximately independent LD blocks are given. A much condensed version of the documentation example is as follows, python3 P00_00_partition_chromosome.py example_data/chr2.interpolated_genetic_map.gz 379 example_data/cov_matrix/scripts/chr2_partitions tabix -h ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20110521/ALL.chr2.phase1_release_v3.20101123.snps_indels_svs.genotypes.vcf.gz 2:39967768-40067768 | python3 P00_01_calc_covariance.py example_data/chr2.interpolated_genetic_map.gz example_data/eurinds.txt 11418 1e-7 example_data/cov_matrix/chr2/chr2.39967768.40067768.gz python3 P01_matrix_to_vector_pipeline.py --dataset_path=example_data/cov_matrix/ --name=chr2 --out_fname=example_data/vector/vector-EUR-chr2-39967768-40067768.txt.gz python3 P02_minima_pipeline.py --input_fname=example_data/vector/vector-EUR-chr2-39967768-40067768.txt.gz --chr_name=chr2 --dataset_path=example_data/cov_matrix/ --n_snps_bw_bpoints=50 --out_fname=example_data/minima/minima-EUR-chr2-50-39967768-40067768.pickle python3 P03_extract_bpoints.py --name=chr2 --dataset_path=example_data/cov_matrix/ --subset=fourier_ls --input_pickle_fname=example_data/minima/minima-EUR-chr2-50-39967768-40067768.pickle > example_data/bed/EUR-chr2-50-39967768-40067768.bed LDpred It is rather simple to install, e.g., pip install ldpred or pip install --user ldpred Nevertheless it is rather laborous to read through the documentation and we try the GitHub version as well. git clone https://github.com/bvilhjal/ldpred cd ldpred cd ldpred python test.py rather than a Bash version from scratch which is also possible, cd ldpred export test=ZDNCEO python coord_genotypes.py --gf=../test_data/LDpred_data_p0.001_train_0 \\ --vgf=../test_data/LDpred_data_p0.001_test_0 \\ --ssf=../test_data/LDpred_data_p0.001_ss_0.txt \\ --N=10000 \\ --out=$test.coord.hdf5 python LDpred.py --coord=$test.coord.hdf5 \\ --ld_radius=100 \\ --local_ld_file_prefix=$test \\ --PS=0.001 \\ --N=10000 \\ --out=$test python validate.py --vgf=../test_data/LDpred_data_p0.001_test_0 \\ --rf=$test \\ --out=$test where --gf=PLINK_LD_REF_GENOTYPE_FILE, --vgf=PLINK_VAL_GENOTYPE_FILE, --ssf=SUM_STATS_FILE, --NSS_SAMPLE_SIZE, the approximate number of individuals used for calculating the GWAS summary statistics, --ld_radiud=LD_RADIUS, the number of SNPs on each side of the focal SNP for which LD should be adjusted, --PS=FRACTION_CAUSAL, A list of comma separated (without space) values between 1 and 0, excluding 0. 1 corresponds to the infinitesimal model and will yield results similar to LDpred-inf. Default is --PS=1,0.3,0.1,0.03,0.01,0.003,0.001,0.0003,0.0001, --rf=RESULT_FILE_PREFIX: SNP weights file ldsc Partitioned heritability The wiki documentation script really should be as follows, export GIANT_BMI=GIANT_BMI_Speliotes2010_publicrelease_HapMapCeuFreq.txt setup() { if [ ! -f $GIANT_BMI ]; then wget http://portals.broadinstitute.org/collaboration/giant/images/b/b7/$GIANT_BMI.gz gunzip $GIANT_BMI.gz fi if [ ! -f w_hm3.snplist ]; then wget https://data.broadinstitute.org/alkesgroup/LDSCORE/w_hm3.snplist.bz2 bzip2 -d w_hm3.snplist.bz2 fi python munge_sumstats.py --sumstats $GIANT_BMI --merge-alleles w_hm3.snplist --out BMI --a1-inc } # It fails to munge, so we use brute-force. For P-to-Z implementation in C/C++, see # https://stackoverflow.com/questions/27830995/inverse-cumulative-distribution-function-in-c # https://stackoverflow.com/questions/22834998/what-reference-should-i-use-to-use-erf-erfc-function # It turned out that an older version of pandas is required here, see the GIANT+UKB BMI example awk 'NR>1' $GIANT_BMI > 1 awk 'NR>1' w_hm3.snplist | sort -k1,1 | join -j1 1 - | awk -f CLEAN_ZSCORES.awk > BMI.sumstats R --vanilla -q <<END BMI <- read.table(\"BMI.sumstats\",col.names=c(\"SNP\",\"A1\",\"A2\",\"Z\",\"N\")) BMI <- within(BMI, {Z=sign(Z)*qnorm(abs(Z)/2)}) z <- gzfile(\"BMI.sumstats.gz\",\"w\") write.table(BMI,file=z,quote=FALSE,row.names=FALSE) close(z) END where we use CLEAN_ZSCORES.awk to align SNPs between sumstats and reference. Now the partition heritability and cell-type group analysis proceed as follows, python ldsc.py --h2 BMI.sumstats.gz\\ --ref-ld-chr baseline_v1.1/baseline.\\ --w-ld-chr 1000G_Phase3_weights_hm3_no_MHC/weights.hm3_noMHC.\\ --overlap-annot\\ --frqfile-chr 1000G_Phase3_frq/1000G.EUR.QC.\\ --out BMI_baseline python ldsc.py --h2 BMI.sumstats.gz\\ --w-ld-chr 1000G_Phase3_weights_hm3_no_MHC/weights.hm3_noMHC.\\ --ref-ld-chr 1000G_Phase3_cell_type_groups/cell_type_group.3.,baseline_v1.1/baseline.\\ --overlap-annot\\ --frqfile-chr 1000G_Phase3_frq/1000G.EUR.QC.\\ --out BMI_CNS\\ --print-coefficients NB it is assumed that all the required data have been made available. Test The mysterious commands shown in the wiki documentation are actually realised after this, sudo apt install python-nose MiXeR https://github.com/precimed/mixer PLINK2 Both PLINK 1.90 beta and PLINK 2.00 alpha have issue with .grm.bin.N which is shorter than expected for GCTA. The problem is insidious but would prevent chromosome-specific GRMs to be combined. Nevertheless there is no such problem with its --make-grm-list which allows for the possibility to use --mgrm-list option to combine chromosome-specific GRMs. Note also the way to use individual's IDs in PLINK2. PRrice-2 Source https://github.com/choishingwan/PRSice and documentation, https://choishingwan.github.io/PRSice/ (scripts pgs.sh ). see also https://github.com/pgormley/polygenic-risk-scores. R-packages See R-packages section. PheWAS CLARITE See https://github.com/HallLab for the R and Python packages, both linking RNHANES. Lucas AM, et al. CLARITE facilitates the quality control and analysis process for EWAS of metabolic-related traits. Fron Genet . See also wiki resources section of Omics-analysis as well as implementations in R-packages section. Transcriptome-wide association analysis (TWAS) IronThrone-GoT https://github.com/landau-lab/IronThrone-GoT Nam AS, et al. (2019). Somatic mutations and cell identity linked by Genotyping of Transcriptomes. *Nature\" 571: 355\u2013360. MetaXcan / S-PrediXcan Issues with more recent version of Python 2.7 The issue was raised to the MetaXcan GitHub repository for Ubuntu 18.04 and Python 2.7.15r1 Fedora 27 and Python 2.7.15 such that logging.getLogger() was not found. This was due to confusion between Logging and Python module logging, and fixed by renaming Logging.py to myLogging.py and then adjusting the call from Logging to myLogging in M03_betas.py, M04_zscores.py and MetaXcan.py, etc. It looks the recent version of Python is stricter, which is somewhat expected as with most other compilers. Similar issues were raised while maintaining R packages for complaints from g++ 8.xx (to be shipped with Fedora 28) which is otherwise OK with g++ 7.x.x. Use of the latest databases While it is possible to use the web interface, https://cloud.hakyimlab.org/user_main, to achieve greater flexibility, the latest databases can be downloaded locally from PredictDB Data Repository . For instance with GTEx-V7_HapMap-2017-11-29.tar.gz , we can do the following steps, mkdir GTEx-V7_HapMap-2017-11-29 cd GTEx-V7_HapMap-2017-11-29 wget https://s3.amazonaws.com/predictdb2/GTEx-V7_HapMap-2017-11-29.tar.gz tar xvfz GTEx-V7_HapMap-2017-11-29.tar.gz and adjust for the documentation example ./MetaXcan.py \\ --model_db_path data/DGN-WB_0.5.db \\ --covariance data/covariance.DGN-WB_0.5.txt.gz \\ --gwas_folder data/GWAS \\ --gwas_file_pattern \".*gz\" \\ --snp_column SNP \\ --effect_allele_column A1 \\ --non_effect_allele_column A2 \\ --beta_column BETA \\ --pvalue_column P \\ --output_file results/test.csv as follows, ./MetaXcan.py \\ --model_db_path /home/jhz22/D/genetics/hakyimlab/ftp/GTEx-V7_HapMap-2017-11-29/gtex_v7_Brain_Amygdala_imputed_europeans_tw_0.5_signif.db \\ --covariance /home/jhz22/D/genetics/hakyimlab/ftp/GTEx-V7_HapMap-2017-11-29/gtex_v7_Brain_Amygdala_imputed_eur_covariances.txt.gz \\ --gwas_folder data/GWAS \\ --gwas_file_pattern \".*gz\" \\ --snp_column SNP \\ --effect_allele_column A1 \\ --non_effect_allele_column A2 \\ --beta_column BETA \\ --pvalue_column P \\ --output_file results/V7.csv Examining weights and related information PredictDB FAQs point to a utility in PrediXcan for query, however it is handy to use sqlite3 directory as has been demonstrated in my TWAS-pipeline . In this case, we can create a utility, called query-db.sql here, .tables .separator \"\\t\" .header on .output weights.txt select * from weights; .output extra.txt select * from extra; used as follows, sqlite3 gtex_v7_Brain_Amygdala_imputed_europeans_tw_0.5_signif.db < query-db.sql and the weights and extra information are available from files weights.txt and extra.txt, respectively. FUSION This section follows http://gusevlab.org/projects/fusion/ and is more compact. To install we do, # Software git clone https://github.com/gusevlab/fusion_twas cd fusion_twas # LD reference wget -qO- https://data.broadinstitute.org/alkesgroup/FUSION/LDREF.tar.bz2 | tar xjvf - # Gene expression / splicing weights; GTEx weights can be obtained similarly mkdir WEIGHTS cd WEIGHTS for wgt in NTR.BLOOD.RNAARR YFS.BLOOD.RNAARR METSIM.ADIPOSE.RNASEQ CMC.BRAIN.RNASEQ CMC.BRAIN.RNASEQ_SPLICING do wget -qO- https://data.broadinstitute.org/alkesgroup/FUSION/WGT/$wgt.tar.bz2 | tar xfj - done # for weight generation only assuming availability of libgfortran.so.3 # wget -qO- https://github.com/genetics-statistics/GEMMA/releases/download/v0.96/gemma.linux.gz | gunzip -c > $HOME/bin/gemma # ln -s ./ output and add R packages, library(devtools) install_github(\"gabraham/plink2R/plink2R\",args=\"--library=/usr/local/lib/R/site-library/\") install.packages(c('optparse','RColorBrewer'),INSTALL_opts=\"--library /usr/local/lib/R/site-library/\") # for weight generation # install.packages('glmnet',INSTALL_opts=\"--library /usr/local/lib/R/site-library/\") # for joint likelihood mapping # install_github(\"cotsapaslab/jlim/jlimR\",args=\"/usr/local/lib/R/site-library/\") # for colocalisation # install.packages(\"coloc\",INSTALL_opts=\"/usr/local/lib/R/site-library/\") The documentation example for association test, its main use, is then furnished with wget https://data.broadinstitute.org/alkesgroup/FUSION/SUM/PGC2.SCZ.sumstats Rscript FUSION.assoc_test.R \\ --sumstats PGC2.SCZ.sumstats \\ --weights ./WEIGHTS/NTR.BLOOD.RNAARR.pos \\ --weights_dir ./WEIGHTS/ \\ --ref_ld_chr ./LDREF/1000G.EUR. \\ --chr 22 \\ --out PGC2.SCZ.22.dat We could simplify the recent GEUV example script as follows, #!/usr/bin/bash export FUSION=${HPC_WORK}/fusion_twas for d in work GEUV; do if [ ! -d ${FUSION}/${d} ]; then mkdir ${FUSION}/${d}; fi; done cd ${FUSION}/work rm * ln -sf . output export LDREF=/rds/user/jhz22/hpc-work/fusion_twas/LDREF export PRE_GEXP=${HPC_WORK}/fusion_twas/GD462.GeneQuantRPKM.50FN.samplename.resk10.txt gunzip -c $PRE_GEXP.gz | awk '! ($3 ~ \"X\") && NR >1' | while read PARAM; do export CHR=$(echo $PARAM | awk '{ print $3 }') export P0=$(echo $PARAM | awk '{ print $4 - 0.5e6 }') export P1=$(echo $PARAM | awk '{ print $4 + 0.5e6 }') export OUT=$(echo $PARAM | awk '{ print $1 }') echo $CHR $P0 $P1 $OUT echo $PARAM | tr ' ' '\\n' | tail -n+5 | paste <(gunzip -c $PRE_GEXP.gz | head -n1 | tr '\\t' '\\n' | tail -n+5 | awk '{ print $1,$1 }') - > $OUT.pheno plink2 --bfile $LDREF/1000G.EUR.$CHR --pheno $OUT.pheno --make-bed --out $OUT --chr $CHR --from-bp $P0 --to-bp $P1 > /dev/null Rscript ${FUSION}/FUSION.compute_weights.R --bfile $OUT --tmp $OUT.tmp --out $FUSION/GEUV/$OUT \\ --save_hsq --hsq_p 0.1 --models blup,lasso,top1,enet --verbose 2 done # https://www.ebi.ac.uk/arrayexpress/experiments/E-GEUV-1/files/analysis_results/ Note that gcta_nr_robust , plink2 and gemma are already in the searching path and we tricked GEMMA with ln -sf . output with the current working directory. A useful utility Rscript utils/make_score.R WEIGHTS/CMC.BRAIN.RNASEQ/CMC.MC4R.wgt.RDat > CMC.MC4R.score plink --bfile genotype-file --score CMC.MC4R.score 1 2 4 See additional information from the FUSION documentation. ExPecto Software for predicting expression effects of human genome variants ab initio from sequence. git clone https://github.com/FunctionLab/ExPecto cd ExPecto sudo pip install -r requirements.txt sh download_resources.h tar fxz resources.tar.gz python chromatin.py example/example.vcf python predict.py --coorFile example/example.vcf --geneFile example/example.vcf.bed.sorted.bed.closestgene --snpEffectFilePattern example/example.vcf.shift_SHIFT.diff.h5 --modelList resources/modellist --output output.csv python train.py --expFile resources/geneanno.exp.csv --targetIndex 1 --output model.adipose enloc Available from https://github.com/xqwen/integrative. More recent version is fastenloc ; also related are dap and torus . For instance torus can be installed as follows, git clone https://github.com/xqwen/torus/ cd torus module load gsl module load boost/1.49.0-gcc4.9.1 module load zlib cd src make make static mv torus torus.static ${HPC_work}/bin cd - and for the documentaion example on height, we have torus -d Height.torus.zval.gz --load_zval -dump_pip Height.gwas.pip gzip Height.gwas.pip fastenloc -eqtl gtex_v8.eqtl_annot.vcf.gz -gwas Height.gwas.pip.gz -prefix Height sort -grk6 Height.enloc.sig.out Note that these use hg38 references provided, and it is possible to generate the hg19 counterparts via script gtex_v8_hg19.sh . FINEMAP colocalization pipeline This is available from https://bitbucket.org/mgloud/production_coloc_pipeline, note also https://github.com/boxiangliu/locuscomparer. GWAS-PW Available from https://github.com/joepickrell/gwas-pw. The installation is straightforward after boost library is available. We can use the following code for the documentation example, gwas-pw -i example_data/aam_height_example.gz -bed ${HPC_WORK}/ldetect/ldetect-data/EUR.bed -phenos AAM HEIGHT -o example_data/aam_height where EUR.bed contains the information for approximately independent LD blocks. INFERNO / SparkINFERNO Short for (INFERring the molecular mechanisms of NOncoding genetic variants, it is available from https://bitbucket.org/wanglab-upenn/INFERNO and it also has a web interface, http://inferno.lisanwanglab.org/index.php. SparkINFERNO is described here, https://bitbucket.org/wanglab-upenn/sparkinferno/ . Web http://inferno.lisanwanglab.org/index.php . R packages biMM It is a software for bivariate lineax mixed model (LMM), https://www.mv.helsinki.fi/home/mjxpirin/download.html pSI, available from CRAN and http://genetics.wustl.edu/jdlab/psi_package/ with supplementary data http://genetics.wustl.edu/jdlab/files/2014/01/pSI.data_1.0.tar_.gz. Epigenomics Avocado Project page, https://noble.gs.washington.edu/proj/avocado/ and Software, https://bitbucket.org/noblelab/avocado/src/master/ To install python setup.py install --prefix=/rds/user/jhz22/hpc-work # insert this line into .bashrc export PYTHONPATH=$PYTHONPATH:/rds/user/jhz22/hpc-work/lib/python2.7/site-packages/ combined-pvalues A library to combine, analyze, group and correct p-values in BED files. Unique tools involve correction for spatial autocorrelation. This is useful for ChIP-Seq probes and Tiling arrays, or any data with spatial correlation. Software, https://github.com/brentp/combined-pvalues Pedersen BS, Schwartz DA, Yang IV, Kechris KJ. Comb-p: software for combining, analyzing, grouping and correcting spatially correlated P-values Bioinformatics 28(22):2986\u20132988, https://doi.org/10.1093/bioinformatics/bts545 ChromImpute Website, http://www.biolchem.ucla.edu/labs/ernst/ChromImpute/ and Source, https://github.com/jernst98/ChromImpute Ernst J, Kellis M. Large-scale imputation of epigenomic datasets for systematic annotation of diverse human tissues. Nature Biotechnology, 33:364-376, 2015 EpiAlign Software, https://github.com/zzz3639/EpiAlign Web, http://shiny.stat.ucla.edu:3838/EpiAlign/ Ge X, Zhang H, Xie L, Li WV, Kwon SB, Li JJ EpiAlign: an alignment-based bioinformatic tool for comparing chromatin state sequences. Nucleic Acids Res. 2019 Apr 24. doi: 10.1093/nar/gkz287. R-packages See https://github.com/jinghuazhao/Computational-Statistics for general information. Bioconductor This includes Biobase, BSGenome, edgeR, limma, Rsubread, STRINGdb. CRAN This includes DCGL. GSMR It refers to Generalised Summary-data-based Mendelian Randomisation, http://cnsgenomics.com/software/gsmr/, available both as part of GCTA and R package. install.packages(\"http://cnsgenomics.com/software/gsmr/static/gsmr_1.0.6.tar.gz\",repos=NULL,type=\"source\") with test data, http://cnsgenomics.com/software/gsmr/static/test_data.zip. Script for the documentation example is tallied here as gsmr_example.R . MendelianRandomization The following are necessary to enable its installation, sudo apt install curl sudo apt install libcurl4-openssl-dev sudo apt install libssl-dev sudo apt install libgmp-dev and then we have install.packages(\"MendelianRandomization\") The vignette (.R, .Rmd, .pdf) can be seen from /usr/local/lib/R/site-library/MendelianRandomization/doc/Vignette_MR.* We can now call with rstudio /usr/local/lib/R/site-library/MendelianRandomization/doc/Vignette_MR.R & We can use the 97 SNPs from GIANT as described in SUMSTATS as two subsets and obtain association informaiton in _PhenoScanner_GWAS.csv using PhenoScanner as well as the extract.pheno.csv() to build a MR analysis for BMI-T2D, say. TwoSampleMR This is standard and furnished as follows, library(devtools) install_github('MRCIEU/TwoSampleMR') The following is adapted from Dimou NL, Tsilidis KK (2018). A Primer in Mendelian Randomization Methodology with a Focus on Utilizing Published Summary Association Data in Evangelou E (ed) Genetic Epidemiology-Methods and Protocols. Springer, Chapter 13, pp211-230. BMI and T2D library(TwoSampleMR) ao <- available_outcomes() subset(ao,id%in%c(2,24)) ao2 <- subset(ao,id==2) exposure_dat <- extract_instruments(ao2$id) outcome_dat <- extract_outcome_data(exposure_dat$SNP, 24, proxies = 1, rsq = 0.8, align_alleles = 1, palindromes = 1, maf_threshold = 0.3) dat <- harmonise_data(exposure_dat, outcome_dat, action = 2) mr_results <- mr(dat) mr_heterogeneity <- mr_heterogeneity(dat) mr_pleiotropy_test <- mr_pleiotropy_test(dat) res_single <- mr_singlesnp(dat) res_loo <- mr_leaveoneout(dat) p1 <- mr_scatter_plot(mr_results, dat) p2 <- mr_forest_plot(res_single) p3 <- mr_leaveoneout_plot(res_loo) p4 <- mr_funnel_plot(res_single) library(MendelianRandomization) MRInputObject <- with(dat, mr_input(bx = beta.exposure, bxse = se.exposure, by = beta.outcome, byse = se.outcome, exposure = \"Body mass index\", outcome = \"Type 2 diabetes\", snps = SNP)) IVW <- mr_ivw(MRInputObject, model = \"default\", robust = FALSE, penalized = FALSE, weights = \"simple\", distribution = \"normal\", alpha = 0.05) Egger <- mr_egger(MRInputObject, robust = FALSE, penalized = FALSE, distribution = \"normal\", alpha = 0.05) MaxLik <- mr_maxlik(MRInputObject, model = \"default\", distribution = \"normal\", alpha = 0.05) Median <- mr_median(MRInputObject, weighting = \"weighted\", distribution = \"normal\", alpha = 0.05, iterations = 10000, seed = 314159265) MR_all <- mr_allmethods(MRInputObject, method = \"all\") p <- mr_plot(MRInputObject, error = TRUE, orientate = FALSE, interactive = TRUE, labels = TRUE, line = \"ivw\") pdf(\"BMI-T2D.pdf\") p1 p2 p3 p4 p dev.off() ACE-APOE-CRP.R illustrates the use of MRInstruments, linking some established proteins. BLR An extensive use is reported in the JSS paper from the Mixed-Models repository. PheWAS See https://github.com/PheWAS/ coloc It requires snpStats that can be installed with biocLite(). There is complaint about calling vignette() from Ubuntu; however it is otherwise smooth with help.start() . Here we run examples modified from the documentation, # coloc, large (>0.05) p.value.chisquare indicates traits are compatible with colocalisation # https://cran.r-project.org/web/packages/coloc/vignettes/vignette.html set.seed(1) X1 <- matrix(rbinom(1200,1,0.4),ncol=2) X2 <- matrix(rbinom(1000,1,0.6),ncol=2) colnames(X1) <- colnames(X2) <- c(\"f1\",\"f2\") Y1 <- rnorm(600,apply(X1,1,sum),2) Y2 <- rnorm(500,2*apply(X2,1,sum),5) summary(lm1 <- lm(Y1~f1+f2,data=as.data.frame(X1))) summary(lm2 <- lm(Y2~f1+f2,data=as.data.frame(X2))) require(coloc) ## intuitive test for proportionality ct <- coloc.test(lm1,lm2, plots.extra=list(x=c(\"eta\",\"theta\"), y=c(\"lhood\",\"lhood\"))) summary(ct) b1 <- coef(lm1) b2 <- coef(lm2) v1 <- vcov(lm1) v2 <- vcov(lm2) coloc.test.summary(b1,b2,v1,v2) # some Bayesian flavour ct.bayes <- coloc.test(lm1,lm2, plots.extra=list(x=c(\"eta\",\"theta\"), y=c(\"lhood\",\"lhood\")),bayes=TRUE) ci(ct.bayes) par(mfrow=c(2,2)) plot(ct) plot(ct.bayes) cc.bayes <- coloc.test(lm1,lm2, plots.extra=list(x=c(\"eta\",\"theta\"), y=c(\"lhood\",\"lhood\")), bayes=TRUE, bayes.factor=list(c(-0.1,1), c(0.9,1.1))) ci(cc.bayes) ## Bayesian approach, esp. when only p values are available abf <- coloc.abf(list(beta=b1, varbeta=diag(v1), N=nrow(X1), sdY=sd(Y1), type=\"quant\"), list(beta=b2, varbeta=diag(v2), N=nrow(X2), sdY=sd(Y2), type=\"quant\")) abf Developmental version of the package is available as follows, if(!require(\"remotes\")) install.packages(\"remotes\") remotes::install_github(\"chr1swallace/coloc\") garfield Web site: https://www.ebi.ac.uk/birney-srv/GARFIELD/ Again it can be installed with biocLite(\"garfield\") and vignette be seen similarly to coloc . GWAS analysis of regulatory or functional information enrichment with LD correction. Briefly, it is a method that leverages GWAS findings with regulatory or functional annotations (primarily from ENCODE and Roadmap epigenomics data) to find features relevant to a phenotype of interest. It performs greedy pruning of GWAS SNPs (LD r2 > 0.1) and then annotates them based on functional information overlap. Next, it quantifies Fold Enrichment (FE) at various GWAS significance cutoffs and assesses them by permutation testing, while matching for minor allele frequency, distance to nearest transcription start site and number of LD proxies (r2 > 0.8). The documentation example is run as follows, garfield.run(\"tmp\", data.dir=system.file(\"extdata\",package = \"garfield\"), trait=\"trait\",run.option = \"prep\", chrs = c(22), exclude = c(895, 975, 976, 977, 978, 979, 98)) garfield.run(\"tmp\", data.dir=system.file(\"extdata\",package = \"garfield\"), run.option = \"perm\", nperm = 1000, thresh = c(0.001, 1e-04, 1e-05), pt_thresh = c(1e-04, 1e-05), maf.bins = 2, tags.bins = 3, tss.bins = 3, prep.file = \"tmp.prep\", optim_mode = TRUE, minit = 100, thresh_perm = 0.05) if (file.exists(\"tmp.perm\")){ perm = read.table(\"tmp.perm\", header=TRUE) head(perm) } else { print(\"Error: tmp.perm does not exist!\") } We have the Crohn's disease example, # download data and decompress system(\"wget https://www.ebi.ac.uk/birney-srv/GARFIELD/package/garfield-data.tar.gz\") system(\"tar -zxvf garfield-data.tar.gz\") # if downloaded in current working directory use the following to execute # garfield, otherwise please change data.dir location garfield.run(\"cd-meta.output\", data.dir=\"garfield-data\", trait=\"cd-meta\", run.option = \"prep\", chrs = c(1:22), exclude = c(895, 975, 976, 977, 978, 979, 980)) # garfield.run(\"cd-meta.output\", data.dir=\"garfield-data\", run.option = \"perm\", nperm = 100000, thresh = c(0.1,0.01,0.001, 1e-04, 1e-05, 1e-06, 1e-07, 1e-08), pt_thresh = c(1e-05, 1e-06, 1e-07, 1e-08), maf.bins = 5, tags.bins = 5, tss.bins = 5, prep.file = \"cd-meta.output.prep\", optim_mode = TRUE, minit = 100, thresh_perm = 0.0001) # garfield.plot(\"cd-meta.output.perm\", num_perm = 100000, output_prefix = \"cd-meta.output\", plot_title = \"Crohn's Disease\", filter = 10, tr = -log10(0.05/498)) GenomicSEM GenomicSEM fits structural equation models based on the summary statistics obtained from genome wide association studies (GWAS). gtx The version at https://github.com/tobyjohnson/gtx has more updates to its counterpart at CRAN. HIBAG Currently it is archived at CRAN but can be downloaded from GitHub, https://github.com/cran/HIBAG devtools::install_github(\"cran/HIBAG\") However it is now available from Bioconductor. hyprcoloc It is a package for hypothesis prioritisation multi-trait colocalization, available from https://github.com/jrs95/hyprcoloc. R --no-save -q <<END # Regression coefficients and standard errors from ten GWAS studies (Traits 1-5, 6-8 & 9-10 colocalize) betas <- hyprcoloc::test.betas head(betas) ses <- hyprcoloc::test.ses head(ses) # Trait names and SNP IDs traits <- paste0(\"T\", 1:10) rsid <- rownames(betas) # Colocalisation analyses results <- hyprcoloc::hyprcoloc(betas, ses, trait.names=traits, snp.id=rsid) END meta The following code, courtesy of the package developer, generates three forest plots, library(meta) ## Generic inverse-variance meta-analysis ## (first two arguments: treatment estimate and its standard error) ## m1 <- metagen(1:10, rep(0.1, 10), sm = \"MD\", studlab = LETTERS[1:10]) ## Use update.meta() to re-run meta-analysis with additional argument ## m1.subset <- update(m1, subset = 1:5) ## m1.exclude <- update(m1, exclude = 6:10) pdf(\"forest1-all.pdf\", width = 8.75, height = 4) forest(m1, colgap.forest.left = \"1cm\") grid::grid.text(\"All studies\", 0.5, 0.94, gp = grid::gpar(cex = 1.5)) forest(m1.subset, colgap.forest.left = \"1cm\") grid::grid.text(\"Subset of studies\", 0.5, 0.9, gp = grid::gpar(cex = 1.5)) forest(m1.exclude, colgap.forest.left = \"1cm\") grid::grid.text(\"Exclude studies\", 0.5, 0.94, gp = grid::gpar(cex = 1.5)) dev.off() moloc moloc: multiple trait co-localization, available from https://github.com/clagiamba/moloc, can be installed with library(devtools) install_github(\"clagiamba/moloc\") rjags The legacy way to install is R-devel CMD INSTALL --configure-args=\"--with-jags-prefix=/usr/local --with-jags-libdir=/usr/local/lib --with-jags-includedir=/usr/local/include\" rjags but it might not work and the currently preferred way to set up is via pkg-config, e.g., export PKG_CONFIG_PATH=/usr/local/lib/pkgconfig pkg-config --moderversion jags R-devel CMD INSTALL --configure-args='--enable-rpath' rjags where rjags contains files for the package. Once this is done one can proceed with install.packages(\"R2jags\") , etc. sva The package contains function ComBat.R from https://www.bu.edu/jlab/wp-assets/ComBat/Download.html as described in the following paper. Johnson, WE, Rabinovic, A, and Li, C (2007). Adjusting batch effects in microarray expression data using Empirical Bayes methods. Biostatistics 8(1):118-127 source(\"https://bioconductor.org/biocLite.R\") biocLite(\"sva\") browseVignettes(\"sva\") EBSeq The Bioconductor page is here, http://www.bioconductor.org/packages/devel/bioc/html/EBSeq.html. --- eQTL, epigenome-wide association study (EWAS). --- CpGassoc https://CRAN.R-project.org/package=CpGassoc fastQTL http://fastqtl.sourceforge.net/files/FastQTL-2.165.linux.tgz missMethy http://bioconductor.org/packages/release/bioc/html/missMethyl.html MultiABEL Multi-Trait Genome-Wide Association Analysis https://github.com/xiashen/MultiABEL OmnibusFisher The p-values of SNPs, RNA expressions and DNA methylations are calculated by kernel machine (KM) regression. The correlation between different omics data are taken into account. This method can be applied to either samples with all three types of omics data or samples with two types. https://CRAN.R-project.org/package=OmnibusFisher PredictABEL See https://CRAN.R-project.org/package=PredictABEL . Kundu S, et al. (2014). Estimating the predictive ability of genetic risk models in simulated data based on published results from genome-wide association studies. Front Genet 2014, 5: 179, https://doi.org/10.3389/fgene.2014.00179 . QCGWAS It can be installed from CRAN. The sample is fairly easy to get going # 23-11-2018 JHZ library(QCGWAS) path <- \"/home/jhz22/R/QCGWAS/data\" files <- file.path(path,dir(path)) load(files[1]) load(files[2]) head(gwa_sample,5) head(header_translations,20) write.table(gwa_sample,file=\"test\",row.names=FALSE,quote=FALSE) QCresults <- QC_GWAS(\"test\", header_translations = header_translations, save_final_dataset = TRUE) # allele-frequency threshold=0.05, HWEp=1e-4, call rate0.99, imputation quality=0.4 QCresults <- QC_GWAS(\"test\", header_translations = header_translations, save_final_dataset = TRUE, HQfilter_FRQ = 0.05, HQfilter_HWE = 10^-4, HQfilter_cal = 0.99, HQfilter_imp = 0.4, NAfilter = TRUE) # filters for the QQ-plot QCresults <- QC_GWAS(\"test\", header_translations = header_translations, save_final_dataset = TRUE, HQfilter_FRQ = 0.01, HQfilter_HWE = 10^-6, HQfilter_cal = 0.95, HQfilter_imp = 0.3, QQfilter_FRQ = c(NA, 0.01, 0.03, 0.05, 3), QQfilter_HWE = c(NA, 10^-6, 10^-4), QQfilter_cal = c(NA, 0.95, 0.98, 0.99), QQfilter_imp = c(NA, 0.3, 0.5, 0.7, 0.9), NAfilter = TRUE) # HapMap allele reference -- but it does not work and should be # https://ftp.hapmap.org/hapmap/frequencies/2010-08_phaseII+III/allele_freqs_chr2_CEU_r28_nr.b36_fwd.txt.gz # based on https://bioinformatics.mdanderson.org/Software/VariantTools/mirror/annoDB/hapmap_CEU_freq.ann # add options method=\"curl\", extra=\"--insecure\" to download.file create_hapmap_reference(dir = \".\", download_hapmap = TRUE, download_subset = \"CEU\", filename = \"hapmap\", save_txt = FALSE, save_rdata = TRUE) # a new QC with HapMap QCresults <- QC_GWAS(\"test\", header_translations = header_translations, save_final_dataset = TRUE, HQfilter_FRQ = 0.01, HQfilter_HWE = 10^-6, HQfilter_cal = 0.95, HQfilter_imp = 0.3, QQfilter_FRQ = c(NA, 0.01, 0.03, 0.05, 3), QQfilter_HWE = c(NA, 10^-6, 10^-4), QQfilter_cal = c(NA, 0.95, 0.98, 0.99), QQfilter_imp = c(NA, 0.3, 0.5, 0.7, 0.9), NAfilter = TRUE, allele_ref_std = \"hapmap.RData\", allele_name_std = \"HapMap\", remove_mismatches = TRUE, check_ambiguous_alleles = FALSE) # An alternative allele reference QCresults <- QC_GWAS(\"test\", header_translations = header_translations, save_final_dataset = TRUE, HQfilter_FRQ = 0.01, HQfilter_HWE = 10^-6, HQfilter_cal = 0.95, HQfilter_imp = 0.3, QQfilter_FRQ = c(NA, 0.01, 0.03, 0.05, 3), QQfilter_HWE = c(NA, 10^-6, 10^-4), QQfilter_cal = c(NA, 0.95, 0.98, 0.99), QQfilter_imp = c(NA, 0.3, 0.5, 0.7, 0.9), NAfilter = TRUE, allele_ref_std = \"hapmap.RData\", allele_name_std = \"HapMap\", remove_mismatches = TRUE, allele_ref_alt = NULL, allele_name_alt = \"alternative\", update_alt = TRUE, update_savename = \"ref_alternative\", update_as_rdata = TRUE) # and QC with it QCresults <- QC_GWAS(\"test\", header_translations = header_translations, save_final_dataset = TRUE, HQfilter_FRQ = 0.01, HQfilter_HWE = 10^-6, HQfilter_cal = 0.95, HQfilter_imp = 0.3, QQfilter_FRQ = c(NA, 0.01, 0.03, 0.05, 3), QQfilter_HWE = c(NA, 10^-6, 10^-4), QQfilter_cal = c(NA, 0.95, 0.98, 0.99), QQfilter_imp = c(NA, 0.3, 0.5, 0.7, 0.9), NAfilter = TRUE, allele_ref_std = \"hapmap.RData\", allele_name_std = \"HapMap\", remove_mismatches = TRUE, allele_ref_alt = \"ref_alternative.RData\", allele_name_alt = \"alternative\", update_alt = TRUE, update_as_rdata = TRUE, backup_alt = TRUE) # automatic loading hapmap_ref <- read.table(\"hapmap_ref.txt\", header = TRUE, as.is = TRUE) alternative_ref <- read.table(\"alt_ref.txt\", header = TRUE, as.is = TRUE) QCresults <- QC_GWAS(\"test\", header_translations = \"headers.txt\", out_header = \"new_headers.txt\", allele_ref_std = hapmap_ref, allele_ref_alt = alternative_ref, update_alt = TRUE, update_as_rdata = FALSE, update_savename = \"alt_ref\") # automatic QC of multiple files QC_series( data_files= c(\"data1.txt\",\"data2.txt\",\"data3.txt\"), output_filenames = c(\"output1.txt\",\"output2.txt\",\"output3.txt\"), dir_data = \"preQC\", dir_output = \"postQC\", dir_references = \"QC_files\", header_translations = header_translations, save_final_dataset = TRUE, HQfilter_FRQ = 0.01, HQfilter_HWE = 10^-6, HQfilter_cal = 0.95, HQfilter_imp = 0.3, QQfilter_FRQ = c(NA, 0.01, 0.03, 0.05, 3), QQfilter_HWE = c(NA, 10^-6, 10^-4), QQfilter_cal = c(NA, 0.95, 0.98, 0.99), QQfilter_imp = c(NA, 0.3, 0.5, 0.7, 0.9), NAfilter = TRUE, allele_ref_std = \"ref_hapmap.RData\", allele_name_std = \"HapMap\", remove_mismatches = TRUE, allele_ref_alt = \"ref_alternative.RData\", allele_name_alt = \"alternative\", update_alt = TRUE, update_as_rdata = TRUE, backup_alt = TRUE) Note the changes required with the HapMap reference, i.e., download.file(url = paste0(\"https://ftp.hapmap.org/hapmap/frequencies/2010-08_phaseII+III/\", \"allele_freqs_chr\", dn, \"_\", download_subset, \"_r28_nr.b36_fwd.txt.gz\"), method = \"curl\", extra = \"--insecure\", destfile = paste0(dir, \"/allele_freqs_chr\", dn, \"_\", download_subset, \"_r28_nr.b36_fwd.txt.gz\")) ftp://ftp.ncbi.nlm.nih.gov/hapmap/frequencies/2010-08_phaseII+III/ might also work. SAIGE The address of GitHub repository is here, https://github.com/weizhouUMICH Information including installation is described here, https://github.com/weizhouUMICH/SAIGE/wiki/Genetic-association-tests-using-SAIGE. Locally, we therefore check for the desired gcc, cmake and boost and proceed as follows, module avail gcc module avail cmake module avail boost # gcc > 5.5 and cmake > 3.8.1 module load gcc-6.1.0-gcc-4.8.5-jusvegv cmake-3.8.1-gcc-4.8.5-zz55m7 # boost 1.58.0 and R 3.6.0 are described in Computationl_Statistics repository. export LD_LIBRARY_PATH=/rds-d4/user/jhz22/hpc-work/boost_1_58_0/stage/lib:$LD_LIBRARY_PATH # we actually use the binary disrtibution directly wget https://github.com/weizhouUMICH/SAIGE/archive/v0.35.8.2.tar.gz tar tvfz v0.35.8.2.tar.gz cd SAIGE-0.35.8.2 tar xvfz SAIGE_0.35.8.2_R_x86_64-pc-linux-gnu.tar.gz mv SAIGE /rds-d4/user/jhz22/hpc-work/R R --no-save <<END library(SAIGE) END Since the required packages Rcpp and RcppParallel are relatively easy to deal with, with which we then simply load the packague as usual. A recent description is given here, https://cambridge-ceu.github.io/csd3/applications/SAIGE.html . ensemblVEP There is no particular difficulty, simply use BiocManager::install(\"ensemblVEP\") .","title":"Association analysis"},{"location":"AA/#association-analysis","text":"","title":"Association analysis"},{"location":"AA/#data-management","text":"","title":"Data management"},{"location":"AA/#bgenix","text":"As documented, the current version requires gcc 4.7* so we proceed as follows, hg clone https://gavinband@bitbucket.org/gavinband/bgen -u master cd bgen module load gcc/4.7.2 ./waf configure --prefix=/scratch/jhz22 ./waf ./waf install See https://bitbucket.org/gavinband/bgen/overview.","title":"bgenix"},{"location":"AA/#single-variant-analysis","text":"","title":"Single variant analysis"},{"location":"AA/#eigensoft","text":"The PCA software for genomewide data is available from https://www.hsph.harvard.edu/alkes-price/software/ as well as Ubuntu. sudo apt install eigensoft The executables are eigenstrat, eigenstratQTL, smarteigenstrat, smartpca, pca, etc.","title":"eigensoft"},{"location":"AA/#gemma","text":"To build from source, https://github.com/genetics-statistics/GEMMA, the Makefile needs to change in places with OpenBLAS, /opt/OpenBLAS/.","title":"GEMMA"},{"location":"AA/#metal","text":"Note METAL aligns alleles according to the first file processed. At least cmake 3.1 is required for the latest from GitHub, https://github.com/statgen/METAL, wget -qO- https://github.com/statgen/METAL/archive/2018-08-28.tar.gz | \\ tar xvfz - cd METAL-2018-08-28 mkdir build && cd build cmake .. make make test make install One can use ccmake . to change the prefix for installation but this does not appear to work and the executable is bin/metal. As with distribution 2011-03-25, http://csg.sph.umich.edu/abecasis/Metal/download/, options CUSTOMVARIABLE uses an output format of %g, leading to scientific notation of position, which is undesirable and we modify metal/Main.cpp from for (int j = 0; j < customVariables.Length(); j++) fprintf(f, \"\\t%g\", custom[j][marker]); to for (int j = 0; j < customVariables.Length(); j++) fprintf(f, \"\\t%-.15g\", custom[j][marker]); which is left-aligned with 15 places with %g though largely 11 is enough. The change can be tested by adding the following lines to examples/GlucoseExample/meta.txt. CUSTOMVARIABLE CHR LABEL CHR as CHR CUSTOMVARIABLE POS LABEL POS as POS CUSTOMVARIABLE N LABEL N as N CHROMOSOMELABEL CHR POSITIONLABEL POS TRACKPOSITIONS ON Nevertheless the CHR and POS thus retained are sums of individual studies involved for particular positions so their real values can be recovered from these divided by the number of - and + from the Direction column. In the case of N, the sum is just what we want. To wrap up, our testing code is as follows, ### illustration as in examples/GlucoseExample/metal.tbl of TRACKPOSITIONS and change in source ### the results are also sorted in accordance with METAL documentation cd examples/GlucoseExample ( echo CUSTOMVARIABLE CHR echo LABEL CHR as CHR echo CUSTOMVARIABLE POS echo LABEL POS as POS echo CUSTOMVARIABLE N echo LABEL N as N echo CHROMOSOMELABEL CHR echo POSITIONLABEL POS echo TRACKPOSITIONS ON echo OUTFILE metal- .tbl awk '!/\\#/' metal.txt ) > metal.metal metal metal.metal ( head -1 metal-1.tbl awk 'NR>1' metal-1.tbl | \\ awk ' { FS=OFS=\"\\t\" direction=$9 gsub(/\\?/,\"\",direction) n=length(direction) $10=$10/n $11=$11/n };1' | \\ sort -k10,10n -k11,11n ) > metal.tbl rm metal-1.tbl mv metal-1.tbl.info metal.tbl.info cd - Another extension relates to heterogeneity analysis, e.g., I 2 > 30 we require at least three studies each attaining P <= 0.05. In this case, we extend the direction field as in direction[marker] = z == 0.0 ? '0' : (z > 0.0 ? '+' : '-'); to direction[marker] = z == 0.0 ? '0' : (z > 0.0 ? '+' : '-'); direction[marker] = (fabs(z) * sqrt(w) < 1.959964) ? direction[marker] : (z > 0.0 ? 'p' : 'n'); for both ProcessFile() and ReProcessFile(). It is then relatively easy to filter on meta-analysis statistics, awk -f metal.awk 4E.BP1-1.tbl , where metal.awk has the following lines, { d3=$13; gsub(/?/,\"\",d3) if (length(d3) >= 3 && $18 >= 3500) if ($12 > -9.30103) print; else { if ($14 < 30) print; else { d3n=d3; d3p=d3; gsub(/+|-|p/,\"\",d3n); gsub(/+|-|n/,\"\",d3p); if (length(d3n) >= 3 || length(d3p) >= 3) print; } } } # R # > log10(5e-10) # [1] -9.30103 # head -1 METAL/4E.BP1-1.tbl | sed 's|\\t|\\n|g' | awk '{print \"#\" NR,$1}' #1 Chromosome #2 Position #3 MarkerName #4 Allele1 #5 Allele2 #6 Freq1 #7 FreqSE #8 MinFreq #9 MaxFreq #10 Effect #11 StdErr #12 log(P) #13 Direction #14 HetISq #15 HetChiSq #16 HetDf #17 logHetP #18 N","title":"METAL"},{"location":"AA/#metasoft-and-forestpmplot","text":"Available from http://genetics.cs.ucla.edu/meta/ mkdir METASOFT wget -qO- http://genetics.cs.ucla.edu/meta/repository/2.0.1/Metasoft.zip | \\ unzip Metasoft.zip java -jar Metasoft.jar -input example.txt cd - The results are in file out . One can also work with ForestPMPLot similarly.","title":"METASOFT and ForestPMPlot"},{"location":"AA/#mtag","text":"https://github.com/omeed-maghzian/mtag","title":"mtag"},{"location":"AA/#pylmm","text":"The software is rare with its setup for GEI studies accounting for polygenic effects. pylmm It is necessary to get it going with code in the quick guide, git clone https://github.com/nickFurlotte/pylmm make the following changes, build/scripts-2.7/pylmmGWAS.py, line 207, from keep = True - v to keep = True ^ v pylmm/lmm.py, line 189, from if X0 == None: to if X0.all == None: ; line 193, from x = True - np.isnan(Y) to x = True ^ np.isnan(Y) ; line 272, from if X == None: X = self.X0t to if X.all == None: X = self.X0t . build/scripts-2.7/input.py, line 190, from x = True ^ np.isnan(G) to x = True ^ np.isnan(G) . python setup.py install use the documentation call, pylmmGWAS.py -v --bfile data/snps.132k.clean.noX --kfile data/snps.132k.clean.noX.pylmm.kin --phenofile data/snps.132k.clean.noX.fake.phenos out.foo pylmm_zarlab Make the following changes to lmm and lmmGWAS similar to pylmm, and then issue bash run_tests.sh . EReading SNP input... Read 1219 individuals from data/snps.132k.clean.noX.fam Reading kinship... Read the 1219 x 1219 kinship matrix in 1.139s 1 number of phenotypes read Traceback (most recent call last): File \"scripts/pylmmGWAS.py\", line 308, in <module> keep = True - v TypeError: numpy boolean subtract, the `-` operator, is deprecated, use the bitwise_xor, the `^` operator, or the logical_xor function instead. EReading PLINK input... Read 1219 individuals from data/snps.132k.clean.noX.fam Traceback (most recent call last): File \"scripts/pylmmKinship.py\", line 127, in <module> K_G = lmm.calculateKinshipIncremental(IN, numSNPs=options.numSNPs, AttributeError: 'module' object has no attribute 'calculateKinshipIncremental' EReading PLINK input... Read 1219 individuals from data/snps.132k.clean.noX.fam Traceback (most recent call last): File \"scripts/pylmmKinship.py\", line 127, in <module> K_G = lmm.calculateKinshipIncremental(IN, numSNPs=options.numSNPs, AttributeError: 'module' object has no attribute 'calculateKinshipIncremental' ====================================================================== ERROR: test_GWAS (tests.test_lmm.test_lmm) ---------------------------------------------------------------------- Traceback (most recent call last): File \"/home/jhz22/D/genetics/ucla/pylmm_zarlab/tests/test_lmm.py\", line 41, in test_GWAS TS,PS = lmm.GWAS(Y,snps,K,REML=True,refit=True) File \"/home/jhz22/D/genetics/ucla/pylmm_zarlab/pylmm/lmm.py\", line 192, in GWAS L = LMM(Y, K, Kva, Kve, X0) File \"/home/jhz22/D/genetics/ucla/pylmm_zarlab/pylmm/lmm.py\", line 301, in __init__ x = True - np.isnan(Y) TypeError: numpy boolean subtract, the `-` operator, is deprecated, use the bitwise_xor, the `^` operator, or the logical_xor function instead. ====================================================================== ERROR: test_calculateKinship (tests.test_lmm.test_lmm) ---------------------------------------------------------------------- Traceback (most recent call last): File \"/home/jhz22/D/genetics/ucla/pylmm_zarlab/tests/test_lmm.py\", line 25, in test_calculateKinship K = lmm.calculateKinship(snps) File \"/home/jhz22/D/genetics/ucla/pylmm_zarlab/pylmm/lmm.py\", line 135, in calculateKinship mn = W[True - np.isnan(W[:, i]), i].mean() TypeError: numpy boolean subtract, the `-` operator, is deprecated, use the bitwise_xor, the `^` operator, or the logical_xor function instead. ====================================================================== ERROR: test_pylmmGWASScript (tests.test_pylmmGWAS.test_pylmmGWAS) ---------------------------------------------------------------------- Traceback (most recent call last): File \"/home/jhz22/D/genetics/ucla/pylmm_zarlab/tests/test_pylmmGWAS.py\", line 24, in test_pylmmGWASScript with (open(self._outputFile, 'r')) as ansFile: IOError: [Errno 2] No such file or directory: 'data/pylmmGWASTestOutput' ====================================================================== ERROR: test_pylmmKinshipScript1 (tests.test_pylmmKinship.test_pylmmKinship) ---------------------------------------------------------------------- Traceback (most recent call last): File \"/home/jhz22/D/genetics/ucla/pylmm_zarlab/tests/test_pylmmKinship.py\", line 24, in test_pylmmKinshipScript1 K = np.fromfile(open(self._outputFile, 'r'), sep=\" \") IOError: [Errno 2] No such file or directory: 'data/pylmmKinshipTestOutput' ====================================================================== ERROR: test_pylmmKinshipScript2 (tests.test_pylmmKinship.test_pylmmKinship) ---------------------------------------------------------------------- Traceback (most recent call last): File \"/home/jhz22/D/genetics/ucla/pylmm_zarlab/tests/test_pylmmKinship.py\", line 38, in test_pylmmKinshipScript2 K = np.fromfile(open(self._outputFile, 'r'), sep=\" \") IOError: [Errno 2] No such file or directory: 'data/pylmmKinshipTestOutput' ---------------------------------------------------------------------- Ran 5 tests in 2.776s FAILED (errors=5) We can have a test of GxE analysis as this, sudo python setup.py install cd pylmm python pylmm_GXE.py In general, we can see options for GxE analysis from command pylmmGWAS.py under bash.","title":"PyLMM"},{"location":"AA/#seqmeta","text":"seqMeta: Meta-Analysis of Region-Based Tests of Rare DNA Variants, https://cran.r-project.org/web/packages/seqMeta/index.html.","title":"seqMeta"},{"location":"AA/#quanto","text":"https://preventivemedicine.usc.edu/download-quanto/","title":"Quanto"},{"location":"AA/#quicktest","text":"See https://wp.unil.ch/sgg/quicktest/ for the latest with support for bgen v1.2.","title":"QUICKTEST"},{"location":"AA/#snptest","text":"Available from https://mathgen.stats.ox.ac.uk/genetics_software/snptest/snptest.html#download, e.g., wget http://www.well.ox.ac.uk/~gav/resources/snptest_v2.5.4-beta3_CentOS6.6_x86_64_static.tgz tar xvfz nptest_v2.5.4-beta3_CentOS6.6_x86_64_static.tgz It is possible that one would get error messages !! Error in function: PerVariantComputationManager::get_phenotypes(), argument(s): phenotype_spec=IL.18R1___Q13478:P. !! Quitting. !! Error (HaltProgramWithReturnCode): but they would go away with -method em for instance.","title":"SNPTEST"},{"location":"AA/#sugen","text":"Genetic Association Analysis Under Complex Survey Sampling, https://github.com/dragontaoran/SUGEN.","title":"SUGEN"},{"location":"AA/#swiss","text":"Software to help identify overlap between association scan results and GWAS hit catalogs. sudo apt install libz-dev pip install git+https://github.com/welchr/swiss.git@v1.0.0 One may try options such as --install_options=\"--prefix=\"\". In case the $HOME directory does not have sufficient space, one can issue swiss --download-data on a system that does, then upload, rsync -av --partial .local/share/swiss login.hpc.cam.ac.uk:$HOME/.local/share or select particular files, sync -av --partial .local/share/swiss/data/ld/1000g.phase3.hg38.EUR.shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz* \\ login.hpc.cam.ac.uk:$HOME/.local/share/swiss/data/ld but this could be tricked with making $HOME/.local/share a symbolic pointing to a directory that can hold more than ~40GB data (reminscent fo VEP), then the pip command above can be called with the --user option. To test, follow these, git clone https://github.com/statgen/swiss cd swiss/test swiss --list-files swiss --list-ld-sources swiss --list-gwas-cats swiss --list-gwas-traits --gwas-cat data/gwascat_ebi_GRCh37p13.tab swiss --assoc data/top_hit_is_gwas.tab --variant-col EPACTS --pval-col PVAL \\ --dist-clump --clump-dist 250000 --clump-p 5e-08 --skip-gwas --out test swiss --assoc data/test_hg19.gz --multi-assoc --trait SM --build hg19 \\ --ld-clump-source 1000G_2014-11_EUR --ld-gwas-source 1000G_2014-11_EUR --gwas-cat data/gwascat_ebi_GRCh37p13.tab \\ --ld-clump --clump-p 1e-10 --out test swiss --assoc data/test_hg38.gz --gwas-cat data/gwascat_ebi_GRCh38p7.tab --variant-col VARIANT \\ --chrom-col CHROM --pos-col POS --trait BMI --build hg38 \\ --ld-clump-source 1000G_2014-11_EUR --ld-gwas-source 1000G_2014-11_EUR \\ --ld-clump --clump-p 5e-08 --out test and consult the online documentation.","title":"swiss"},{"location":"AA/#population-struction","text":"","title":"Population struction"},{"location":"AA/#finestructure","text":"https://people.maths.bris.ac.uk/~madjl/finestructure/index.html","title":"fineSTRUCTURE"},{"location":"AA/#hla-imputation","text":"","title":"HLA imputation"},{"location":"AA/#hlaimp02","text":"Download source from https://oxfordhla.well.ox.ac.uk/hla/tool/main sudo apt install libgd sudo apt install libgtk-3* then start Perl, sudo perl -MCPAN -e shell followed by install GD install Alien::wxWidgets install Moose install List::MoreUtils install Wx::Mini install Wx::Perl::Packager We can also use cpan, but the installation fails under Ubuntu 18.04.","title":"HLA*IMP:02"},{"location":"AA/#finemapping","text":"","title":"Finemapping"},{"location":"AA/#caviarecaviar","text":"Installation is made from GitHub in the usual way, git clone https://github.com/fhormoz/caviar.git The software requires libgsl and liblapack which can be installed as follows, sudo apt install liblapack-dev sudo apt install libgsl-dev Once this is done, one can proceed with the compiling, cd caviar/CAVIAR-C++ make cd - CAVIAR -l CAVIAR-C++/sample_data/50_LD.txt -z CAVIAR-C++/sample_data/50_Z.txt -o 50 CAVIAR -l CAVIAR-C++/sample_data/DDB1.top100.sig.SNPs.ld -z CAVIAR-C++/sample_data/DDB1.top100.sig.SNPs.ZScores -o 100 eCAVIAR -l CAVIAR-C++/sample_data/GWAS.ADGC.MC.AD.IGAP.stage1.hg19.chr.11.121344805.121517613.CHRPOSREFALT.LD.ld \\ -l CAVIAR-C++/sample_data/eQTL.CARDIOGENICS.MC.AD.IGAP.stage1.hg19.chr.11.121344805.121517613.CHRPOSREFALT.LD.ld \\ -z CAVIAR-C++/sample_data/GWAS.MC.AD.IGAP.stage1.hg19.chr.11.121344805.121517613.CHRPOSREFALT.Z.txt \\ -z eQTL.CARDIOGENICS.MC.AD.IGAP.stage1.hg19.chr.11.121344805.121517613.CHRPOSREFALT.ILMN_1810712.NM_015313.1.ARHGEF12.Z.txt \\ -o 75 It may be necessary to alter Makefile to point to appropriate -I -L for lapack, for instance. The references are eCAVIAR Hormozdiari F, van de Bunt M, Segr\u00e8 AV, Li X, Joo JWJ, Bilow M, Sul JH, Sankararaman S, Pasaniuc B, Eskin E (2016). Colocalization of GWAS and eQTL Signals Detects Target Genes. Am J Hum Genet 99(6):1245-1260. CAVIAR Hormozdiari F, Kostem E, Kang EY, Pasaniuc B, Eskin E (2014). Identifying causal variants at loci with multiple signals of association. Genetics 198(2):497-508. Both are available from https://github.com/fhormoz/caviar.","title":"CAVIAR/eCAVIAR"},{"location":"AA/#caviarbf","text":"wget https://bitbucket.org/Wenan/caviarbf/get/7e428645be5e.zip unzip 7e428645be5e.zip cd Wenan-caviarbf-7e428645be5e make ln -sf $PWD/caviarbf $HOME/bin/caviarbf ln -sf $PWD/model_search $HOME/bin/model_search ./install_r_package.sh cd caviarbf-r-package R --no-save <<END install.packages(\"glmnet\") END R CMD INSTALL caviarbf_0.2.1.tar.gz ./test.sh cd - ./test.sh","title":"CAVIARBF"},{"location":"AA/#finemap","text":"finemap 1.4 is available from http://www.christianbenner.com/finemap_v1.4_x86_64.tgz. finemap_v1.4_x86_64 --sss --in-files example/master We could also keep the screen output with finemap_v1.4_x86_64 --sss --in-files example/master 2>&1 | tee test.log . LDSTORE v2.0 is available from http://www.christianbenner.com/ldstore_v2.0_x86_64.tgz. ldstore_v2.0_x86_64 --in-files example/master --write-bdose --bdose-version 1.1 ldstore_v2.0_x86_64 --in-files example/master --write-bcor --read-bdose ldstore_v2.0_x86_64 --in-files example/master --bcor-to-text","title":"finemap"},{"location":"AA/#gchromvar","text":"Cell type specific enrichments using finemapped variants and quantitative epigenetic data, https://github.com/caleblareau/gchromVAR; see https://github.com/caleblareau/singlecell_bloodtraits for examples. It requires chromVarmotifs from https://github.com/GreenleafLab/chromVARmotifs, which requires gcc 5.2.0.","title":"gchromVar"},{"location":"AA/#jam","text":"Setup The package is available from https://github.com/pjnewcombe/R2BGLiMS. Note that JAM requires Java 1.8 so call to Java -jar inside the function needs to reflect this, not straightforward with install_github() from devtools but one needs to clone the package, modify the R source code and then install, git clone https://github.com/pjnewcombe/R2BGLiMS ### in case you have java-1.6 you will need to change to java-1.8 in R2BGLiMS/R/R2BGLiMS.R ### and possibly add other options, e.g., ### /usr/lib/jvm/java-8-oracle/bin/java -Xmx4G ### sed -i 's|\\\"java|\\\"/usr/lib/jvm/java-8-oracle/bin/java -Xmx4G||g' R2BGLiMS/R/R2BGLiMS.R ### sudo R CMD INSTALL R2BGLiMS -l $R_LIBS ### if R_LIBS is not set, the default can be used, e.g., $HOME/R R CMD INSTALL R2BGLiMS As shown, it might well be necessary to add options to the Java command-line. Compiling The information is unavailable from the documentation, but can be achieved this with netbeans or in steps. # 21-7-2017 MRC-Epid JHZ export BGLiMS=/genetics/bin/BGLiMS export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.131-0.b11.el6_9.x86_64 # Jama cd $BGLiMS $JAVA_HOME/bin/javac Jama/util/*java $JAVA_HOME/bin/javac -classpath $BGLiMS Jama/*java $JAVA_HOME/bin/jar cvf Jama.jar Jama/*class Jama/util/*class # ssj export SSJHOME=ssj export LD_LIBRARY_PATH=$SSJHOME/lib:$LD_LIBRARY_PATH export CLASSPATH=.:$SSJHOME/lib/ssj.jar:$SSJHOME/lib/colt.jar:$SSJHOME/lib/tcode.jar:$CLASSPATH # MyClassLibrary cd $BGLiMS/MyClassLibrary/src ln -sf $BGLiMS/Jama $JAVA_HOME/bin/javac Methods/*java $JAVA_HOME/bin/javac Objects/*java $JAVA_HOME/bin/jar cvf MyClassLibrary.jar Methods/*class Objects/*class # BGLiMS.jar cd $BGLiMS/src $JAVA_HOME/bin/javac bglims/*java $JAVA_HOME/bin/jar cvf bglims.jar bglims/*class cd $BGLiMS ln -sf $BGLiMS/src/bglims.jar ln -sf MyClassLibrary/src/MyClassLibrary.jar # $JAVA_HOME/bin/jar cvf BGLiMS.jar bglims.jar MyClassLibrary.jar","title":"JAM"},{"location":"AA/#functional-annotation","text":"","title":"Functional annotation"},{"location":"AA/#annovar","text":"See https://cambridge-ceu.github.io/csd3/applications/ANNOVAR.html .","title":"ANNOVAR"},{"location":"AA/#david","text":"https://david.ncifcrf.gov/","title":"DAVID"},{"location":"AA/#fgwas","text":"git clone https://github.com/joepickrell/fgwas cd fgwas # sed -i 's/1.14/1.15/g' configure ./configure --prefix=/scratch/jhz22 make sudo make install src/fgwas -i test_data/test_LDL.fgwas_in.gz -w ens_coding_exon git clone https://github.com/joepickrell/1000-genomes git clone https://github.com/joepickrell/1000-genomes-genetic-maps In case boots is unavailable or not up-to-date, it is necessary to install, e.g., wget https://dl.bintray.com/boostorg/release/1.69.0/source/boost_1_69_0.tar.gz tar xvfz boost_1_69_0.tar.gz cd boost_1_60_0 ./bootstrap.sh --prefix=/scratch/jhz22 --exec-prefix=/scratch/jhz22 ./b2 install Nevertheless it is also required to have other dependencies in place.","title":"fgwas"},{"location":"AA/#garfield","text":"Web: https://www.ebi.ac.uk/birney-srv/GARFIELD/ wget -qO- https://www.ebi.ac.uk/birney-srv/GARFIELD/package-v2/garfield-v2.tar.gz | \\ tar xvfz wget -qO- https://www.ebi.ac.uk/birney-srv/GARFIELD/package-v2/garfield-data.tar.gz | \\ tar xvfz cd garfield-v2 bash garfield Additional details are described in the documentation, https://www.ebi.ac.uk/birney-srv/GARFIELD/documentation-v2/GARFIELD-v2.pdf.","title":"GARFIELD"},{"location":"AA/#gnomad","text":"Web site: https://gnomad.broadinstitute.org/downloads . To use gsutil , following these steps, module load python/3.7 virtualenv py37 source py37/bin/activate pip install gsutil gsutil ls gs://gnomad-public/release gsutil cp gs://gnomad-public/release/2.1.1/constraint/gnomad.v2.1.1.lof_metrics.by_gene.txt.bgz . gsutil cp gs://gnomad-public/release/2.1.1/constraint/gnomad.v2.1.1.lof_metrics.by_transcript.txt.bgz . UCSC has a description here .","title":"gnomAD"},{"location":"AA/#haploreg","text":"Web: https://pubs.broadinstitute.org/mammals/haploreg/haploreg.php (data files, https://pubs.broadinstitute.org/mammals/haploreg/data/) HaploReg is a tool for exploring annotations of the noncoding genome at variants on haplotype blocks, such as candidate regulatory SNPs at disease-associated loci. Using LD information from the 1000 Genomes Project, linked SNPs and small indels can be visualized along with chromatin state and protein binding annotation from the Roadmap Epigenomics and ENCODE projects, sequence conservation across mammals, the effect of SNPs on regulatory motifs, and the effect of SNPs on expression from eQTL studies. HaploReg is designed for researchers developing mechanistic hypotheses of the impact of non-coding variants on clinical phenotypes and normal variation.","title":"HaploReg"},{"location":"AA/#morpheus","text":"See https://software.broadinstitute.org/morpheus/ .","title":"morpheus"},{"location":"AA/#polyphen-and-polyphen-2","text":"See http://genetics.bwh.harvard.edu/pph/ and http://genetics.bwh.harvard.edu/pph2/.","title":"PolyPhen and PolyPhen-2"},{"location":"AA/#unphased","text":"See https://sites.google.com/site/fdudbridge/software/unphased-3-1.","title":"UNPHASED"},{"location":"AA/#vep","text":"The description is available from http://www.ensembl.org/info/docs/tools/vep/script/vep_download.html. git clone https://github.com/Ensembl/ensembl-vep.git cd ensembl-vep git pull git checkout release/92 perl INSTALL.pl The last line requires modules DBI, Build as described in the LANGUAGES section of Computational-Statistics . Lastly, VEP requires .vep directory at $HOME which can be derived from a centrally-installed VEP under Linux, ln -s /genetics/ensembl-vep/.vep $HOME/.vep ln -s /genetics/ensembl-vep/vep $HOME/bin/vep assuming /genetics/ensembl-vep contains the software. It is slow to get those databases, so one may prefer to get them directly from ftp://ftp.ensembl.org/pub/release-92/variation/VEP/ and unpack into the .vep directory. We can now execute an example, vep -i examples/homo_sapiens_GRCh37.vcf -o out.txt -offline Recent notes on ANNOVAR and VEP are available from here, https://cambridge-ceu.github.io/csd3/applications/VEP.html .","title":"VEP"},{"location":"AA/#r-packages","text":"See R-packages section.","title":"R-packages"},{"location":"AA/#pathway-analysis","text":"","title":"Pathway analysis"},{"location":"AA/#depict","text":"See the GIANT+Biobank BMI analysis . Installation and documentation example The official site, https://data.broadinstitute.org/mpg/depict/documentation.html has links on DEPICT_v1_rel194.tar.gz , which contains 1000Genomes and other data unavailable from depict_140721.tar.bz2 . wget https://data.broadinstitute.org/mpg/depict/depict_download/bundles/DEPICT_v1_rel194.tar.gz tar xvfz DEPICT_v1_rel194.tar.gz export CWD=$(pwd) where the package is unpacked into the DEPICT/ directory containing the data/ subdirectory. We also note down current working directory with CWD . The source package from GitHub has more features such as cutoff_type to be p-values in network analysis; the code git clone https://github.com/perslab/depict cd depict wget https://data.broadinstitute.org/mpg/depict/depict_download/collections/ld0.5_collection_1000genomespilot_depict_150429.txt.gz mkdir -p data/collections mv ld0.5* data/collections sed 's|/cvar/jhlab/tp/DEPICT|/home/jhz22/Downloads/depict|g;s|label_for_output_files: ldl_teslovich_nature2010|label_for_output_files: test|g; s|/cvar/jhlab/tp/tools/plink/plink-1.09-Sep2015-x86_64/plink|/home/jhz22/bin/plink|g' example/ldl_teslovich_nature2010.cfg > test.cfg src/python/depict.py test.cfg adds ld0.5_collection_1000genomespilot_depict_150429.txt.gz and produces results prefixed with test_ using the LDL data. Since the documentation example above does not give the full results, data directory packaged with DEPICT_v1_rel194.tar.gz above is called to remedy with a minor change, mv data data.sav ln -s $CWD/DEPICT/data to test.cfg for a re-run. sed -i 's|data/reconstituted_genesets/reconstituted_genesets_example.txt|data/reconstituted_genesets/reconstituted_genesets_150901.binary|g' test.cfg src/python/depict.py test.cfg PLINK. PLINK-1.9 , with --clump option, has to be used rather than PLINK2 since itdrops the --clump option. NB template.cfg is from src/python rather than .cfg from example. Python 2.7.*. After installation, the following change is needed: from .sort() to .sort_values() in network_plot.py and depict_library.py. It is necessary to download additional files for network analysis -- in my case, downloads via Firefox do not work and I used wget instead. To explore possibility to replicate the Supplementary Figure 9 of the Scott paper -- the number of significant pathways seemed to fall short of the FDR<=0.05 criterion, see SUMSTATS for setup. Under Windows, gzip.exe is also required at the working directory or %path% plus some changes over directory specifications. We can then execute python depict.py BMI.cfg For tissue plot, one can use pdftopng from XpdfReader (or convert/magick from ImageMagick) to obtain .png files to be incorporated into Excel workbook. For network plot, the python package scikit-learn is required. sudo pip install scikit-learn Recompile This may be necessary for large collection of significant variants, e.g., GIANT+UKB height summary statistics (height_loci.txt has 2,184 lines including header). Start netbeans and open project from depict/src/java, fixing links to colt.jar, commons-math-2.0.jar, Jama-1.0.2.jar, jsci-core.jar, JSciCore.jar, jsc.jar from lib/. Additional notes PW-pipeline puts together many changes and is streamlined with other software.","title":"DEPICT"},{"location":"AA/#magma","text":"A generic setup is available from PW-pipeline , while section CAD of the Omics-analysis repository provides a much simplified version.","title":"MAGMA"},{"location":"AA/#pascal","text":"When there is issue with xianyi-OpenBLAS-v0.2.12-0-g7e4e195.zip shipped with PASCAL.zip , as described in vdi.md or git clone https://github.com/xianyi/OpenBLAS it is recommended to use the GitHub version . Change to settings.txt is necessary since by default pathway analysis is disabled. Again we use the BMI summary statistics from GIANT, wget -qO- http://portals.broadinstitute.org/collaboration/giant/images/1/15/SNP_gwas_mc_merge_nogc.tbl.uniq.gz | \\ gunzip -c | cut -f1,7 | awk -vFS=\"\\t\" -vOFS=\"\\t\" '(NR>1)' > BMI.pval Pascal --pval=BMI.pval","title":"PASCAL"},{"location":"AA/#toppgene","text":"A portal for gene list enrichment analysis and candidate gene prioritization based on functional annotations and protein interactions network See https://toppgene.cchmc.org/ .","title":"ToppGene"},{"location":"AA/#vegas2","text":"It is relatively slow with web interface https://vegas2.qimrberghofer.edu.au, so we would like to try the command-line counterpart. Make sure R packages corpcor and mvtnorm are available, then proceed with # driver download wget https://vegas2.qimrberghofer.edu.au/vegas2v2 # documentation example -- unzip does not accept input from console so we do in two steps wget https://vegas2.qimrberghofer.edu.au/VEGAS2v2example.zip unzip -j VEGAS2v2example.zip # gene-based association perl vegas2v2 -G -snpandp example.txt -custom $PWD/example -glist example.glist -genelist example.genelist -out example # pathway-based association awk '(NR>1){OFS=\"\\t\";gsub(/\"/,\"\",$0);print $2,$8}' example.out > Example.geneandp vegas2v2 -P -geneandp Example.geneandp -glist example.glist -geneandpath Example.vegas2pathSYM -out Example # further setup wget https://vegas2.qimrberghofer.edu.au/biosystems20160324.vegas2pathSYM wget https://vegas2.qimrberghofer.edu.au/glist-hg19 wget -qO- https://vegas2.qimrberghofer.edu.au/g1000p3_EUR.tar.gz | tar xvfz - Somehow the binary files following -custom option needs to be absolute path. The last line downloads and unpacks the LD reference data for European (EUR) population; other options include AFR, AMR, EAS, SAS.","title":"VEGAS2"},{"location":"AA/#mendelian-randomisation","text":"Bias and Type 1 error rate for Mendelian randomization with sample overlap, https://sb452.shinyapps.io/overlap/ Power calculation. https://shiny.cnsgenomics.com/mRnd/ .","title":"Mendelian randomisation"},{"location":"AA/#lcv","text":"Software in Matlab and R for Latent Causal Variable model inferring genetically causal relationships using GWAS data. https://github.com/lukejoconnor/LCV See R-packages section.","title":"LCV"},{"location":"AA/#polygenic-modeling","text":"","title":"Polygenic modeling"},{"location":"AA/#gcta","text":"It is possible to use dosage, a Bash function is as follows, function INTERVAL_dosage() { if [ ! -f nodup/${pr}.gen.gz ]; then qctool -g nodup/${pr}.bgen -og nodup/${pr}.gen.gz; fi gunzip -c nodup/${pr}.gen.gz | \\ awk -v sample=${sample} ' { N=(NF-6)/3 for(i=1;i<=N;i++) dosage[NR,i]=$((i-1)*3+8)+2*$((i-1)*3+9) } END { i=0; while (getline gf < sample) { split(gf,a); i++ id[i]=a[1] } close(sample) for(i=1;i<=N;i++) { printf id[i+2] \" ML_DOSE\"; for(j=1;j<=NR;j++) printf \" \" dosage[j,i]; printf \"\\n\" } }' | \\ gzip -f > nodup/${pr}.dosage.gz ( echo SNP Al1 Al2 Freq1 MAF Quality Rsq qctool -g ${pr}.bgen -snp-stats -osnp - | \\ sed '1,9d' | \\ cut -f2,5,6,12,14,17,18 | \\ sed 's/\\t/ /g;s/NA/0/g' ) | \\ grep -v Completed | \\ gzip -f > nodup/${pr}.info.gz } gcta-1.9 --dosage-mach-gz nodup/$pr.dosage.gz nodup/$pr.info.gz --make-grm-bin --out nodup/${pr} where pr' is input file root and sample` is the associate sample file from which sample IDs are extracted. When the imputed genotyeps are MaCH-based, it is possible to use DosageConverter . ## assuming you use hpc-work/ with a subdirectory called bin/ cd /rds/user/$USER/hpc-work/ git clone https://github.com/Santy-8128/DosageConvertor cd DosageConverter pip install cget --user module load cmake-3.8.1-gcc-4.8.5-zz55m7x ./install.sh cd /rds/user/$USER/hpc-work/bin/ ln -s /rds/user/$USER/hpc-work/DosageConvertor/release-build/DosageConvertor ## testing DosageConvertor --vcfDose test/TestDataImputedVCF.dose.vcf.gz \\ --info test/TestDataImputedVCF.info \\ --prefix test \\ --type mach gunzip -c test.mach.dose.gz | wc -l DosageConvertor --vcfDose test/TestDataImputedVCF.dose.vcf.gz \\ --info test/TestDataImputedVCF.info \\ --prefix test \\ --type plink gunzip -c test.plink.dosage.gz | wc -l so the MaCH dosage file is individual x genotype whereas PLINK dosage file is genotype x individual.","title":"GCTA"},{"location":"AA/#hess","text":"HESS (Heritability Estimation from Summary Statistics) is now available from https://github.com/huwenboshi/hess and has a web page at https://huwenboshi.github.io/hess-0.5/#hess Some popular Python packages such as pandas as well as PySnpTools are required, e.g., pip install pandas or python -m pip install pysnptools and it is doable for the latter with source at https://github.com/MicrosoftGenomics/PySnpTools. For the GIANT height data, we had success with the following script, #!/bin/bash export HEIGHT=https://portals.broadinstitute.org/collaboration/giant/images/0/01/GIANT_HEIGHT_Wood_et_al_2014_publicrelease_HapMapCeuFreq.txt.gz wget -qO- $HEIGHT | \\ awk 'NR>1' | \\ sort -k1,1 | \\ join -13 -21 snp150.txt - | \\ awk '($9!=\"X\" && $9!=\"Y\" && $9!=\"Un\"){if(NR==1) print \"SNP CHR BP A1 A2 Z N\"; else print $1,$2,$3,$4,$5,$7/$8,$10}' > height.tsv.gz # SNP - rs ID of the SNP (e.g. rs62442). # CHR - Chromosome number of the SNP. This should be a number between 1 and 22. # BP - Base pair position of the SNP. # A1 - Effect allele of the SNP. The sign of the Z-score is with respect to this allele. # A2 - The other allele of the SNP. # Z - The Z-score of the SNP. # N - Sample size of the SNP. for chrom in $(seq 22) do python hess.py \\ --local-hsqg height \\ --chrom $chrom \\ --bfile 1kg_eur_1pct/1kg_eur_1pct_chr${chrom} \\ --partition nygcresearch-ldetect-data-ac125e47bf7f/EUR/fourier_ls-chr${chrom}.bed \\ --out step1 done python hess.py --prefix step1 --reinflate-lambda-gc 1 --tot-hsqg 0.8 0.2 --out step2 where snp150.txt from UCSC is described at the SUMSTATS repository, https://github.com/jinghuazhao/SUMSTATS.","title":"HESS"},{"location":"AA/#ldetect","text":"It can proceed as indicated sudo pip3 install ldetect into /usr/local/lib/python3.6/dist-packages, or git clone https://bitbucket.org/nygcresearch/ldetect cd ldetect # for super user # sudo python3 setup.py install python3 setup.py install --user git clone https://bitbucket.org/nygcresearch/ldetect-data cd ldetect-data for pop in AFR ASN EUR do awk ' { OFS=\"\\t\" if (NR==1) print \"#chrom\", \"Start\", \"End\", \"Region\" else print $1, $2, $3, \"region\" NR-1 }' $pop/fourier_ls-all.bed > $pop.bed done cd - Population-specific approximately independent LD blocks are given. A much condensed version of the documentation example is as follows, python3 P00_00_partition_chromosome.py example_data/chr2.interpolated_genetic_map.gz 379 example_data/cov_matrix/scripts/chr2_partitions tabix -h ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20110521/ALL.chr2.phase1_release_v3.20101123.snps_indels_svs.genotypes.vcf.gz 2:39967768-40067768 | python3 P00_01_calc_covariance.py example_data/chr2.interpolated_genetic_map.gz example_data/eurinds.txt 11418 1e-7 example_data/cov_matrix/chr2/chr2.39967768.40067768.gz python3 P01_matrix_to_vector_pipeline.py --dataset_path=example_data/cov_matrix/ --name=chr2 --out_fname=example_data/vector/vector-EUR-chr2-39967768-40067768.txt.gz python3 P02_minima_pipeline.py --input_fname=example_data/vector/vector-EUR-chr2-39967768-40067768.txt.gz --chr_name=chr2 --dataset_path=example_data/cov_matrix/ --n_snps_bw_bpoints=50 --out_fname=example_data/minima/minima-EUR-chr2-50-39967768-40067768.pickle python3 P03_extract_bpoints.py --name=chr2 --dataset_path=example_data/cov_matrix/ --subset=fourier_ls --input_pickle_fname=example_data/minima/minima-EUR-chr2-50-39967768-40067768.pickle > example_data/bed/EUR-chr2-50-39967768-40067768.bed","title":"ldetect"},{"location":"AA/#ldpred","text":"It is rather simple to install, e.g., pip install ldpred or pip install --user ldpred Nevertheless it is rather laborous to read through the documentation and we try the GitHub version as well. git clone https://github.com/bvilhjal/ldpred cd ldpred cd ldpred python test.py rather than a Bash version from scratch which is also possible, cd ldpred export test=ZDNCEO python coord_genotypes.py --gf=../test_data/LDpred_data_p0.001_train_0 \\ --vgf=../test_data/LDpred_data_p0.001_test_0 \\ --ssf=../test_data/LDpred_data_p0.001_ss_0.txt \\ --N=10000 \\ --out=$test.coord.hdf5 python LDpred.py --coord=$test.coord.hdf5 \\ --ld_radius=100 \\ --local_ld_file_prefix=$test \\ --PS=0.001 \\ --N=10000 \\ --out=$test python validate.py --vgf=../test_data/LDpred_data_p0.001_test_0 \\ --rf=$test \\ --out=$test where --gf=PLINK_LD_REF_GENOTYPE_FILE, --vgf=PLINK_VAL_GENOTYPE_FILE, --ssf=SUM_STATS_FILE, --NSS_SAMPLE_SIZE, the approximate number of individuals used for calculating the GWAS summary statistics, --ld_radiud=LD_RADIUS, the number of SNPs on each side of the focal SNP for which LD should be adjusted, --PS=FRACTION_CAUSAL, A list of comma separated (without space) values between 1 and 0, excluding 0. 1 corresponds to the infinitesimal model and will yield results similar to LDpred-inf. Default is --PS=1,0.3,0.1,0.03,0.01,0.003,0.001,0.0003,0.0001, --rf=RESULT_FILE_PREFIX: SNP weights file","title":"LDpred"},{"location":"AA/#ldsc","text":"Partitioned heritability The wiki documentation script really should be as follows, export GIANT_BMI=GIANT_BMI_Speliotes2010_publicrelease_HapMapCeuFreq.txt setup() { if [ ! -f $GIANT_BMI ]; then wget http://portals.broadinstitute.org/collaboration/giant/images/b/b7/$GIANT_BMI.gz gunzip $GIANT_BMI.gz fi if [ ! -f w_hm3.snplist ]; then wget https://data.broadinstitute.org/alkesgroup/LDSCORE/w_hm3.snplist.bz2 bzip2 -d w_hm3.snplist.bz2 fi python munge_sumstats.py --sumstats $GIANT_BMI --merge-alleles w_hm3.snplist --out BMI --a1-inc } # It fails to munge, so we use brute-force. For P-to-Z implementation in C/C++, see # https://stackoverflow.com/questions/27830995/inverse-cumulative-distribution-function-in-c # https://stackoverflow.com/questions/22834998/what-reference-should-i-use-to-use-erf-erfc-function # It turned out that an older version of pandas is required here, see the GIANT+UKB BMI example awk 'NR>1' $GIANT_BMI > 1 awk 'NR>1' w_hm3.snplist | sort -k1,1 | join -j1 1 - | awk -f CLEAN_ZSCORES.awk > BMI.sumstats R --vanilla -q <<END BMI <- read.table(\"BMI.sumstats\",col.names=c(\"SNP\",\"A1\",\"A2\",\"Z\",\"N\")) BMI <- within(BMI, {Z=sign(Z)*qnorm(abs(Z)/2)}) z <- gzfile(\"BMI.sumstats.gz\",\"w\") write.table(BMI,file=z,quote=FALSE,row.names=FALSE) close(z) END where we use CLEAN_ZSCORES.awk to align SNPs between sumstats and reference. Now the partition heritability and cell-type group analysis proceed as follows, python ldsc.py --h2 BMI.sumstats.gz\\ --ref-ld-chr baseline_v1.1/baseline.\\ --w-ld-chr 1000G_Phase3_weights_hm3_no_MHC/weights.hm3_noMHC.\\ --overlap-annot\\ --frqfile-chr 1000G_Phase3_frq/1000G.EUR.QC.\\ --out BMI_baseline python ldsc.py --h2 BMI.sumstats.gz\\ --w-ld-chr 1000G_Phase3_weights_hm3_no_MHC/weights.hm3_noMHC.\\ --ref-ld-chr 1000G_Phase3_cell_type_groups/cell_type_group.3.,baseline_v1.1/baseline.\\ --overlap-annot\\ --frqfile-chr 1000G_Phase3_frq/1000G.EUR.QC.\\ --out BMI_CNS\\ --print-coefficients NB it is assumed that all the required data have been made available. Test The mysterious commands shown in the wiki documentation are actually realised after this, sudo apt install python-nose","title":"ldsc"},{"location":"AA/#mixer","text":"https://github.com/precimed/mixer","title":"MiXeR"},{"location":"AA/#plink2","text":"Both PLINK 1.90 beta and PLINK 2.00 alpha have issue with .grm.bin.N which is shorter than expected for GCTA. The problem is insidious but would prevent chromosome-specific GRMs to be combined. Nevertheless there is no such problem with its --make-grm-list which allows for the possibility to use --mgrm-list option to combine chromosome-specific GRMs. Note also the way to use individual's IDs in PLINK2.","title":"PLINK2"},{"location":"AA/#prrice-2","text":"Source https://github.com/choishingwan/PRSice and documentation, https://choishingwan.github.io/PRSice/ (scripts pgs.sh ). see also https://github.com/pgormley/polygenic-risk-scores.","title":"PRrice-2"},{"location":"AA/#r-packages_1","text":"See R-packages section.","title":"R-packages"},{"location":"AA/#phewas","text":"","title":"PheWAS"},{"location":"AA/#clarite","text":"See https://github.com/HallLab for the R and Python packages, both linking RNHANES. Lucas AM, et al. CLARITE facilitates the quality control and analysis process for EWAS of metabolic-related traits. Fron Genet . See also wiki resources section of Omics-analysis as well as implementations in R-packages section.","title":"CLARITE"},{"location":"AA/#transcriptome-wide-association-analysis-twas","text":"","title":"Transcriptome-wide association analysis (TWAS)"},{"location":"AA/#ironthrone-got","text":"https://github.com/landau-lab/IronThrone-GoT Nam AS, et al. (2019). Somatic mutations and cell identity linked by Genotyping of Transcriptomes. *Nature\" 571: 355\u2013360.","title":"IronThrone-GoT"},{"location":"AA/#metaxcan-s-predixcan","text":"Issues with more recent version of Python 2.7 The issue was raised to the MetaXcan GitHub repository for Ubuntu 18.04 and Python 2.7.15r1 Fedora 27 and Python 2.7.15 such that logging.getLogger() was not found. This was due to confusion between Logging and Python module logging, and fixed by renaming Logging.py to myLogging.py and then adjusting the call from Logging to myLogging in M03_betas.py, M04_zscores.py and MetaXcan.py, etc. It looks the recent version of Python is stricter, which is somewhat expected as with most other compilers. Similar issues were raised while maintaining R packages for complaints from g++ 8.xx (to be shipped with Fedora 28) which is otherwise OK with g++ 7.x.x. Use of the latest databases While it is possible to use the web interface, https://cloud.hakyimlab.org/user_main, to achieve greater flexibility, the latest databases can be downloaded locally from PredictDB Data Repository . For instance with GTEx-V7_HapMap-2017-11-29.tar.gz , we can do the following steps, mkdir GTEx-V7_HapMap-2017-11-29 cd GTEx-V7_HapMap-2017-11-29 wget https://s3.amazonaws.com/predictdb2/GTEx-V7_HapMap-2017-11-29.tar.gz tar xvfz GTEx-V7_HapMap-2017-11-29.tar.gz and adjust for the documentation example ./MetaXcan.py \\ --model_db_path data/DGN-WB_0.5.db \\ --covariance data/covariance.DGN-WB_0.5.txt.gz \\ --gwas_folder data/GWAS \\ --gwas_file_pattern \".*gz\" \\ --snp_column SNP \\ --effect_allele_column A1 \\ --non_effect_allele_column A2 \\ --beta_column BETA \\ --pvalue_column P \\ --output_file results/test.csv as follows, ./MetaXcan.py \\ --model_db_path /home/jhz22/D/genetics/hakyimlab/ftp/GTEx-V7_HapMap-2017-11-29/gtex_v7_Brain_Amygdala_imputed_europeans_tw_0.5_signif.db \\ --covariance /home/jhz22/D/genetics/hakyimlab/ftp/GTEx-V7_HapMap-2017-11-29/gtex_v7_Brain_Amygdala_imputed_eur_covariances.txt.gz \\ --gwas_folder data/GWAS \\ --gwas_file_pattern \".*gz\" \\ --snp_column SNP \\ --effect_allele_column A1 \\ --non_effect_allele_column A2 \\ --beta_column BETA \\ --pvalue_column P \\ --output_file results/V7.csv Examining weights and related information PredictDB FAQs point to a utility in PrediXcan for query, however it is handy to use sqlite3 directory as has been demonstrated in my TWAS-pipeline . In this case, we can create a utility, called query-db.sql here, .tables .separator \"\\t\" .header on .output weights.txt select * from weights; .output extra.txt select * from extra; used as follows, sqlite3 gtex_v7_Brain_Amygdala_imputed_europeans_tw_0.5_signif.db < query-db.sql and the weights and extra information are available from files weights.txt and extra.txt, respectively.","title":"MetaXcan / S-PrediXcan"},{"location":"AA/#fusion","text":"This section follows http://gusevlab.org/projects/fusion/ and is more compact. To install we do, # Software git clone https://github.com/gusevlab/fusion_twas cd fusion_twas # LD reference wget -qO- https://data.broadinstitute.org/alkesgroup/FUSION/LDREF.tar.bz2 | tar xjvf - # Gene expression / splicing weights; GTEx weights can be obtained similarly mkdir WEIGHTS cd WEIGHTS for wgt in NTR.BLOOD.RNAARR YFS.BLOOD.RNAARR METSIM.ADIPOSE.RNASEQ CMC.BRAIN.RNASEQ CMC.BRAIN.RNASEQ_SPLICING do wget -qO- https://data.broadinstitute.org/alkesgroup/FUSION/WGT/$wgt.tar.bz2 | tar xfj - done # for weight generation only assuming availability of libgfortran.so.3 # wget -qO- https://github.com/genetics-statistics/GEMMA/releases/download/v0.96/gemma.linux.gz | gunzip -c > $HOME/bin/gemma # ln -s ./ output and add R packages, library(devtools) install_github(\"gabraham/plink2R/plink2R\",args=\"--library=/usr/local/lib/R/site-library/\") install.packages(c('optparse','RColorBrewer'),INSTALL_opts=\"--library /usr/local/lib/R/site-library/\") # for weight generation # install.packages('glmnet',INSTALL_opts=\"--library /usr/local/lib/R/site-library/\") # for joint likelihood mapping # install_github(\"cotsapaslab/jlim/jlimR\",args=\"/usr/local/lib/R/site-library/\") # for colocalisation # install.packages(\"coloc\",INSTALL_opts=\"/usr/local/lib/R/site-library/\") The documentation example for association test, its main use, is then furnished with wget https://data.broadinstitute.org/alkesgroup/FUSION/SUM/PGC2.SCZ.sumstats Rscript FUSION.assoc_test.R \\ --sumstats PGC2.SCZ.sumstats \\ --weights ./WEIGHTS/NTR.BLOOD.RNAARR.pos \\ --weights_dir ./WEIGHTS/ \\ --ref_ld_chr ./LDREF/1000G.EUR. \\ --chr 22 \\ --out PGC2.SCZ.22.dat We could simplify the recent GEUV example script as follows, #!/usr/bin/bash export FUSION=${HPC_WORK}/fusion_twas for d in work GEUV; do if [ ! -d ${FUSION}/${d} ]; then mkdir ${FUSION}/${d}; fi; done cd ${FUSION}/work rm * ln -sf . output export LDREF=/rds/user/jhz22/hpc-work/fusion_twas/LDREF export PRE_GEXP=${HPC_WORK}/fusion_twas/GD462.GeneQuantRPKM.50FN.samplename.resk10.txt gunzip -c $PRE_GEXP.gz | awk '! ($3 ~ \"X\") && NR >1' | while read PARAM; do export CHR=$(echo $PARAM | awk '{ print $3 }') export P0=$(echo $PARAM | awk '{ print $4 - 0.5e6 }') export P1=$(echo $PARAM | awk '{ print $4 + 0.5e6 }') export OUT=$(echo $PARAM | awk '{ print $1 }') echo $CHR $P0 $P1 $OUT echo $PARAM | tr ' ' '\\n' | tail -n+5 | paste <(gunzip -c $PRE_GEXP.gz | head -n1 | tr '\\t' '\\n' | tail -n+5 | awk '{ print $1,$1 }') - > $OUT.pheno plink2 --bfile $LDREF/1000G.EUR.$CHR --pheno $OUT.pheno --make-bed --out $OUT --chr $CHR --from-bp $P0 --to-bp $P1 > /dev/null Rscript ${FUSION}/FUSION.compute_weights.R --bfile $OUT --tmp $OUT.tmp --out $FUSION/GEUV/$OUT \\ --save_hsq --hsq_p 0.1 --models blup,lasso,top1,enet --verbose 2 done # https://www.ebi.ac.uk/arrayexpress/experiments/E-GEUV-1/files/analysis_results/ Note that gcta_nr_robust , plink2 and gemma are already in the searching path and we tricked GEMMA with ln -sf . output with the current working directory. A useful utility Rscript utils/make_score.R WEIGHTS/CMC.BRAIN.RNASEQ/CMC.MC4R.wgt.RDat > CMC.MC4R.score plink --bfile genotype-file --score CMC.MC4R.score 1 2 4 See additional information from the FUSION documentation.","title":"FUSION"},{"location":"AA/#expecto","text":"Software for predicting expression effects of human genome variants ab initio from sequence. git clone https://github.com/FunctionLab/ExPecto cd ExPecto sudo pip install -r requirements.txt sh download_resources.h tar fxz resources.tar.gz python chromatin.py example/example.vcf python predict.py --coorFile example/example.vcf --geneFile example/example.vcf.bed.sorted.bed.closestgene --snpEffectFilePattern example/example.vcf.shift_SHIFT.diff.h5 --modelList resources/modellist --output output.csv python train.py --expFile resources/geneanno.exp.csv --targetIndex 1 --output model.adipose","title":"ExPecto"},{"location":"AA/#enloc","text":"Available from https://github.com/xqwen/integrative. More recent version is fastenloc ; also related are dap and torus . For instance torus can be installed as follows, git clone https://github.com/xqwen/torus/ cd torus module load gsl module load boost/1.49.0-gcc4.9.1 module load zlib cd src make make static mv torus torus.static ${HPC_work}/bin cd - and for the documentaion example on height, we have torus -d Height.torus.zval.gz --load_zval -dump_pip Height.gwas.pip gzip Height.gwas.pip fastenloc -eqtl gtex_v8.eqtl_annot.vcf.gz -gwas Height.gwas.pip.gz -prefix Height sort -grk6 Height.enloc.sig.out Note that these use hg38 references provided, and it is possible to generate the hg19 counterparts via script gtex_v8_hg19.sh .","title":"enloc"},{"location":"AA/#finemap-colocalization-pipeline","text":"This is available from https://bitbucket.org/mgloud/production_coloc_pipeline, note also https://github.com/boxiangliu/locuscomparer.","title":"FINEMAP colocalization pipeline"},{"location":"AA/#gwas-pw","text":"Available from https://github.com/joepickrell/gwas-pw. The installation is straightforward after boost library is available. We can use the following code for the documentation example, gwas-pw -i example_data/aam_height_example.gz -bed ${HPC_WORK}/ldetect/ldetect-data/EUR.bed -phenos AAM HEIGHT -o example_data/aam_height where EUR.bed contains the information for approximately independent LD blocks.","title":"GWAS-PW"},{"location":"AA/#inferno-sparkinferno","text":"Short for (INFERring the molecular mechanisms of NOncoding genetic variants, it is available from https://bitbucket.org/wanglab-upenn/INFERNO and it also has a web interface, http://inferno.lisanwanglab.org/index.php. SparkINFERNO is described here, https://bitbucket.org/wanglab-upenn/sparkinferno/ . Web http://inferno.lisanwanglab.org/index.php .","title":"INFERNO / SparkINFERNO"},{"location":"AA/#r-packages_2","text":"","title":"R packages"},{"location":"AA/#bimm","text":"It is a software for bivariate lineax mixed model (LMM), https://www.mv.helsinki.fi/home/mjxpirin/download.html pSI, available from CRAN and http://genetics.wustl.edu/jdlab/psi_package/ with supplementary data http://genetics.wustl.edu/jdlab/files/2014/01/pSI.data_1.0.tar_.gz.","title":"biMM"},{"location":"AA/#epigenomics","text":"Avocado Project page, https://noble.gs.washington.edu/proj/avocado/ and Software, https://bitbucket.org/noblelab/avocado/src/master/ To install python setup.py install --prefix=/rds/user/jhz22/hpc-work # insert this line into .bashrc export PYTHONPATH=$PYTHONPATH:/rds/user/jhz22/hpc-work/lib/python2.7/site-packages/ combined-pvalues A library to combine, analyze, group and correct p-values in BED files. Unique tools involve correction for spatial autocorrelation. This is useful for ChIP-Seq probes and Tiling arrays, or any data with spatial correlation. Software, https://github.com/brentp/combined-pvalues Pedersen BS, Schwartz DA, Yang IV, Kechris KJ. Comb-p: software for combining, analyzing, grouping and correcting spatially correlated P-values Bioinformatics 28(22):2986\u20132988, https://doi.org/10.1093/bioinformatics/bts545 ChromImpute Website, http://www.biolchem.ucla.edu/labs/ernst/ChromImpute/ and Source, https://github.com/jernst98/ChromImpute Ernst J, Kellis M. Large-scale imputation of epigenomic datasets for systematic annotation of diverse human tissues. Nature Biotechnology, 33:364-376, 2015 EpiAlign Software, https://github.com/zzz3639/EpiAlign Web, http://shiny.stat.ucla.edu:3838/EpiAlign/ Ge X, Zhang H, Xie L, Li WV, Kwon SB, Li JJ EpiAlign: an alignment-based bioinformatic tool for comparing chromatin state sequences. Nucleic Acids Res. 2019 Apr 24. doi: 10.1093/nar/gkz287.","title":"Epigenomics"},{"location":"AA/#r-packages_3","text":"See https://github.com/jinghuazhao/Computational-Statistics for general information. Bioconductor This includes Biobase, BSGenome, edgeR, limma, Rsubread, STRINGdb. CRAN This includes DCGL. GSMR It refers to Generalised Summary-data-based Mendelian Randomisation, http://cnsgenomics.com/software/gsmr/, available both as part of GCTA and R package. install.packages(\"http://cnsgenomics.com/software/gsmr/static/gsmr_1.0.6.tar.gz\",repos=NULL,type=\"source\") with test data, http://cnsgenomics.com/software/gsmr/static/test_data.zip. Script for the documentation example is tallied here as gsmr_example.R . MendelianRandomization The following are necessary to enable its installation, sudo apt install curl sudo apt install libcurl4-openssl-dev sudo apt install libssl-dev sudo apt install libgmp-dev and then we have install.packages(\"MendelianRandomization\") The vignette (.R, .Rmd, .pdf) can be seen from /usr/local/lib/R/site-library/MendelianRandomization/doc/Vignette_MR.* We can now call with rstudio /usr/local/lib/R/site-library/MendelianRandomization/doc/Vignette_MR.R & We can use the 97 SNPs from GIANT as described in SUMSTATS as two subsets and obtain association informaiton in _PhenoScanner_GWAS.csv using PhenoScanner as well as the extract.pheno.csv() to build a MR analysis for BMI-T2D, say. TwoSampleMR This is standard and furnished as follows, library(devtools) install_github('MRCIEU/TwoSampleMR') The following is adapted from Dimou NL, Tsilidis KK (2018). A Primer in Mendelian Randomization Methodology with a Focus on Utilizing Published Summary Association Data in Evangelou E (ed) Genetic Epidemiology-Methods and Protocols. Springer, Chapter 13, pp211-230.","title":"R-packages"},{"location":"AA/#bmi-and-t2d","text":"library(TwoSampleMR) ao <- available_outcomes() subset(ao,id%in%c(2,24)) ao2 <- subset(ao,id==2) exposure_dat <- extract_instruments(ao2$id) outcome_dat <- extract_outcome_data(exposure_dat$SNP, 24, proxies = 1, rsq = 0.8, align_alleles = 1, palindromes = 1, maf_threshold = 0.3) dat <- harmonise_data(exposure_dat, outcome_dat, action = 2) mr_results <- mr(dat) mr_heterogeneity <- mr_heterogeneity(dat) mr_pleiotropy_test <- mr_pleiotropy_test(dat) res_single <- mr_singlesnp(dat) res_loo <- mr_leaveoneout(dat) p1 <- mr_scatter_plot(mr_results, dat) p2 <- mr_forest_plot(res_single) p3 <- mr_leaveoneout_plot(res_loo) p4 <- mr_funnel_plot(res_single) library(MendelianRandomization) MRInputObject <- with(dat, mr_input(bx = beta.exposure, bxse = se.exposure, by = beta.outcome, byse = se.outcome, exposure = \"Body mass index\", outcome = \"Type 2 diabetes\", snps = SNP)) IVW <- mr_ivw(MRInputObject, model = \"default\", robust = FALSE, penalized = FALSE, weights = \"simple\", distribution = \"normal\", alpha = 0.05) Egger <- mr_egger(MRInputObject, robust = FALSE, penalized = FALSE, distribution = \"normal\", alpha = 0.05) MaxLik <- mr_maxlik(MRInputObject, model = \"default\", distribution = \"normal\", alpha = 0.05) Median <- mr_median(MRInputObject, weighting = \"weighted\", distribution = \"normal\", alpha = 0.05, iterations = 10000, seed = 314159265) MR_all <- mr_allmethods(MRInputObject, method = \"all\") p <- mr_plot(MRInputObject, error = TRUE, orientate = FALSE, interactive = TRUE, labels = TRUE, line = \"ivw\") pdf(\"BMI-T2D.pdf\") p1 p2 p3 p4 p dev.off() ACE-APOE-CRP.R illustrates the use of MRInstruments, linking some established proteins. BLR An extensive use is reported in the JSS paper from the Mixed-Models repository. PheWAS See https://github.com/PheWAS/ coloc It requires snpStats that can be installed with biocLite(). There is complaint about calling vignette() from Ubuntu; however it is otherwise smooth with help.start() . Here we run examples modified from the documentation, # coloc, large (>0.05) p.value.chisquare indicates traits are compatible with colocalisation # https://cran.r-project.org/web/packages/coloc/vignettes/vignette.html set.seed(1) X1 <- matrix(rbinom(1200,1,0.4),ncol=2) X2 <- matrix(rbinom(1000,1,0.6),ncol=2) colnames(X1) <- colnames(X2) <- c(\"f1\",\"f2\") Y1 <- rnorm(600,apply(X1,1,sum),2) Y2 <- rnorm(500,2*apply(X2,1,sum),5) summary(lm1 <- lm(Y1~f1+f2,data=as.data.frame(X1))) summary(lm2 <- lm(Y2~f1+f2,data=as.data.frame(X2))) require(coloc) ## intuitive test for proportionality ct <- coloc.test(lm1,lm2, plots.extra=list(x=c(\"eta\",\"theta\"), y=c(\"lhood\",\"lhood\"))) summary(ct) b1 <- coef(lm1) b2 <- coef(lm2) v1 <- vcov(lm1) v2 <- vcov(lm2) coloc.test.summary(b1,b2,v1,v2) # some Bayesian flavour ct.bayes <- coloc.test(lm1,lm2, plots.extra=list(x=c(\"eta\",\"theta\"), y=c(\"lhood\",\"lhood\")),bayes=TRUE) ci(ct.bayes) par(mfrow=c(2,2)) plot(ct) plot(ct.bayes) cc.bayes <- coloc.test(lm1,lm2, plots.extra=list(x=c(\"eta\",\"theta\"), y=c(\"lhood\",\"lhood\")), bayes=TRUE, bayes.factor=list(c(-0.1,1), c(0.9,1.1))) ci(cc.bayes) ## Bayesian approach, esp. when only p values are available abf <- coloc.abf(list(beta=b1, varbeta=diag(v1), N=nrow(X1), sdY=sd(Y1), type=\"quant\"), list(beta=b2, varbeta=diag(v2), N=nrow(X2), sdY=sd(Y2), type=\"quant\")) abf Developmental version of the package is available as follows, if(!require(\"remotes\")) install.packages(\"remotes\") remotes::install_github(\"chr1swallace/coloc\") garfield Web site: https://www.ebi.ac.uk/birney-srv/GARFIELD/ Again it can be installed with biocLite(\"garfield\") and vignette be seen similarly to coloc . GWAS analysis of regulatory or functional information enrichment with LD correction. Briefly, it is a method that leverages GWAS findings with regulatory or functional annotations (primarily from ENCODE and Roadmap epigenomics data) to find features relevant to a phenotype of interest. It performs greedy pruning of GWAS SNPs (LD r2 > 0.1) and then annotates them based on functional information overlap. Next, it quantifies Fold Enrichment (FE) at various GWAS significance cutoffs and assesses them by permutation testing, while matching for minor allele frequency, distance to nearest transcription start site and number of LD proxies (r2 > 0.8). The documentation example is run as follows, garfield.run(\"tmp\", data.dir=system.file(\"extdata\",package = \"garfield\"), trait=\"trait\",run.option = \"prep\", chrs = c(22), exclude = c(895, 975, 976, 977, 978, 979, 98)) garfield.run(\"tmp\", data.dir=system.file(\"extdata\",package = \"garfield\"), run.option = \"perm\", nperm = 1000, thresh = c(0.001, 1e-04, 1e-05), pt_thresh = c(1e-04, 1e-05), maf.bins = 2, tags.bins = 3, tss.bins = 3, prep.file = \"tmp.prep\", optim_mode = TRUE, minit = 100, thresh_perm = 0.05) if (file.exists(\"tmp.perm\")){ perm = read.table(\"tmp.perm\", header=TRUE) head(perm) } else { print(\"Error: tmp.perm does not exist!\") } We have the Crohn's disease example, # download data and decompress system(\"wget https://www.ebi.ac.uk/birney-srv/GARFIELD/package/garfield-data.tar.gz\") system(\"tar -zxvf garfield-data.tar.gz\") # if downloaded in current working directory use the following to execute # garfield, otherwise please change data.dir location garfield.run(\"cd-meta.output\", data.dir=\"garfield-data\", trait=\"cd-meta\", run.option = \"prep\", chrs = c(1:22), exclude = c(895, 975, 976, 977, 978, 979, 980)) # garfield.run(\"cd-meta.output\", data.dir=\"garfield-data\", run.option = \"perm\", nperm = 100000, thresh = c(0.1,0.01,0.001, 1e-04, 1e-05, 1e-06, 1e-07, 1e-08), pt_thresh = c(1e-05, 1e-06, 1e-07, 1e-08), maf.bins = 5, tags.bins = 5, tss.bins = 5, prep.file = \"cd-meta.output.prep\", optim_mode = TRUE, minit = 100, thresh_perm = 0.0001) # garfield.plot(\"cd-meta.output.perm\", num_perm = 100000, output_prefix = \"cd-meta.output\", plot_title = \"Crohn's Disease\", filter = 10, tr = -log10(0.05/498)) GenomicSEM GenomicSEM fits structural equation models based on the summary statistics obtained from genome wide association studies (GWAS). gtx The version at https://github.com/tobyjohnson/gtx has more updates to its counterpart at CRAN. HIBAG Currently it is archived at CRAN but can be downloaded from GitHub, https://github.com/cran/HIBAG devtools::install_github(\"cran/HIBAG\") However it is now available from Bioconductor. hyprcoloc It is a package for hypothesis prioritisation multi-trait colocalization, available from https://github.com/jrs95/hyprcoloc. R --no-save -q <<END # Regression coefficients and standard errors from ten GWAS studies (Traits 1-5, 6-8 & 9-10 colocalize) betas <- hyprcoloc::test.betas head(betas) ses <- hyprcoloc::test.ses head(ses) # Trait names and SNP IDs traits <- paste0(\"T\", 1:10) rsid <- rownames(betas) # Colocalisation analyses results <- hyprcoloc::hyprcoloc(betas, ses, trait.names=traits, snp.id=rsid) END meta The following code, courtesy of the package developer, generates three forest plots, library(meta) ## Generic inverse-variance meta-analysis ## (first two arguments: treatment estimate and its standard error) ## m1 <- metagen(1:10, rep(0.1, 10), sm = \"MD\", studlab = LETTERS[1:10]) ## Use update.meta() to re-run meta-analysis with additional argument ## m1.subset <- update(m1, subset = 1:5) ## m1.exclude <- update(m1, exclude = 6:10) pdf(\"forest1-all.pdf\", width = 8.75, height = 4) forest(m1, colgap.forest.left = \"1cm\") grid::grid.text(\"All studies\", 0.5, 0.94, gp = grid::gpar(cex = 1.5)) forest(m1.subset, colgap.forest.left = \"1cm\") grid::grid.text(\"Subset of studies\", 0.5, 0.9, gp = grid::gpar(cex = 1.5)) forest(m1.exclude, colgap.forest.left = \"1cm\") grid::grid.text(\"Exclude studies\", 0.5, 0.94, gp = grid::gpar(cex = 1.5)) dev.off() moloc moloc: multiple trait co-localization, available from https://github.com/clagiamba/moloc, can be installed with library(devtools) install_github(\"clagiamba/moloc\") rjags The legacy way to install is R-devel CMD INSTALL --configure-args=\"--with-jags-prefix=/usr/local --with-jags-libdir=/usr/local/lib --with-jags-includedir=/usr/local/include\" rjags but it might not work and the currently preferred way to set up is via pkg-config, e.g., export PKG_CONFIG_PATH=/usr/local/lib/pkgconfig pkg-config --moderversion jags R-devel CMD INSTALL --configure-args='--enable-rpath' rjags where rjags contains files for the package. Once this is done one can proceed with install.packages(\"R2jags\") , etc. sva The package contains function ComBat.R from https://www.bu.edu/jlab/wp-assets/ComBat/Download.html as described in the following paper. Johnson, WE, Rabinovic, A, and Li, C (2007). Adjusting batch effects in microarray expression data using Empirical Bayes methods. Biostatistics 8(1):118-127 source(\"https://bioconductor.org/biocLite.R\") biocLite(\"sva\") browseVignettes(\"sva\") EBSeq The Bioconductor page is here, http://www.bioconductor.org/packages/devel/bioc/html/EBSeq.html.","title":"BMI and T2D"},{"location":"AA/#-eqtl-epigenome-wide-association-study-ewas-","text":"CpGassoc https://CRAN.R-project.org/package=CpGassoc fastQTL http://fastqtl.sourceforge.net/files/FastQTL-2.165.linux.tgz missMethy http://bioconductor.org/packages/release/bioc/html/missMethyl.html MultiABEL Multi-Trait Genome-Wide Association Analysis https://github.com/xiashen/MultiABEL OmnibusFisher The p-values of SNPs, RNA expressions and DNA methylations are calculated by kernel machine (KM) regression. The correlation between different omics data are taken into account. This method can be applied to either samples with all three types of omics data or samples with two types. https://CRAN.R-project.org/package=OmnibusFisher PredictABEL See https://CRAN.R-project.org/package=PredictABEL . Kundu S, et al. (2014). Estimating the predictive ability of genetic risk models in simulated data based on published results from genome-wide association studies. Front Genet 2014, 5: 179, https://doi.org/10.3389/fgene.2014.00179 . QCGWAS It can be installed from CRAN. The sample is fairly easy to get going # 23-11-2018 JHZ library(QCGWAS) path <- \"/home/jhz22/R/QCGWAS/data\" files <- file.path(path,dir(path)) load(files[1]) load(files[2]) head(gwa_sample,5) head(header_translations,20) write.table(gwa_sample,file=\"test\",row.names=FALSE,quote=FALSE) QCresults <- QC_GWAS(\"test\", header_translations = header_translations, save_final_dataset = TRUE) # allele-frequency threshold=0.05, HWEp=1e-4, call rate0.99, imputation quality=0.4 QCresults <- QC_GWAS(\"test\", header_translations = header_translations, save_final_dataset = TRUE, HQfilter_FRQ = 0.05, HQfilter_HWE = 10^-4, HQfilter_cal = 0.99, HQfilter_imp = 0.4, NAfilter = TRUE) # filters for the QQ-plot QCresults <- QC_GWAS(\"test\", header_translations = header_translations, save_final_dataset = TRUE, HQfilter_FRQ = 0.01, HQfilter_HWE = 10^-6, HQfilter_cal = 0.95, HQfilter_imp = 0.3, QQfilter_FRQ = c(NA, 0.01, 0.03, 0.05, 3), QQfilter_HWE = c(NA, 10^-6, 10^-4), QQfilter_cal = c(NA, 0.95, 0.98, 0.99), QQfilter_imp = c(NA, 0.3, 0.5, 0.7, 0.9), NAfilter = TRUE) # HapMap allele reference -- but it does not work and should be # https://ftp.hapmap.org/hapmap/frequencies/2010-08_phaseII+III/allele_freqs_chr2_CEU_r28_nr.b36_fwd.txt.gz # based on https://bioinformatics.mdanderson.org/Software/VariantTools/mirror/annoDB/hapmap_CEU_freq.ann # add options method=\"curl\", extra=\"--insecure\" to download.file create_hapmap_reference(dir = \".\", download_hapmap = TRUE, download_subset = \"CEU\", filename = \"hapmap\", save_txt = FALSE, save_rdata = TRUE) # a new QC with HapMap QCresults <- QC_GWAS(\"test\", header_translations = header_translations, save_final_dataset = TRUE, HQfilter_FRQ = 0.01, HQfilter_HWE = 10^-6, HQfilter_cal = 0.95, HQfilter_imp = 0.3, QQfilter_FRQ = c(NA, 0.01, 0.03, 0.05, 3), QQfilter_HWE = c(NA, 10^-6, 10^-4), QQfilter_cal = c(NA, 0.95, 0.98, 0.99), QQfilter_imp = c(NA, 0.3, 0.5, 0.7, 0.9), NAfilter = TRUE, allele_ref_std = \"hapmap.RData\", allele_name_std = \"HapMap\", remove_mismatches = TRUE, check_ambiguous_alleles = FALSE) # An alternative allele reference QCresults <- QC_GWAS(\"test\", header_translations = header_translations, save_final_dataset = TRUE, HQfilter_FRQ = 0.01, HQfilter_HWE = 10^-6, HQfilter_cal = 0.95, HQfilter_imp = 0.3, QQfilter_FRQ = c(NA, 0.01, 0.03, 0.05, 3), QQfilter_HWE = c(NA, 10^-6, 10^-4), QQfilter_cal = c(NA, 0.95, 0.98, 0.99), QQfilter_imp = c(NA, 0.3, 0.5, 0.7, 0.9), NAfilter = TRUE, allele_ref_std = \"hapmap.RData\", allele_name_std = \"HapMap\", remove_mismatches = TRUE, allele_ref_alt = NULL, allele_name_alt = \"alternative\", update_alt = TRUE, update_savename = \"ref_alternative\", update_as_rdata = TRUE) # and QC with it QCresults <- QC_GWAS(\"test\", header_translations = header_translations, save_final_dataset = TRUE, HQfilter_FRQ = 0.01, HQfilter_HWE = 10^-6, HQfilter_cal = 0.95, HQfilter_imp = 0.3, QQfilter_FRQ = c(NA, 0.01, 0.03, 0.05, 3), QQfilter_HWE = c(NA, 10^-6, 10^-4), QQfilter_cal = c(NA, 0.95, 0.98, 0.99), QQfilter_imp = c(NA, 0.3, 0.5, 0.7, 0.9), NAfilter = TRUE, allele_ref_std = \"hapmap.RData\", allele_name_std = \"HapMap\", remove_mismatches = TRUE, allele_ref_alt = \"ref_alternative.RData\", allele_name_alt = \"alternative\", update_alt = TRUE, update_as_rdata = TRUE, backup_alt = TRUE) # automatic loading hapmap_ref <- read.table(\"hapmap_ref.txt\", header = TRUE, as.is = TRUE) alternative_ref <- read.table(\"alt_ref.txt\", header = TRUE, as.is = TRUE) QCresults <- QC_GWAS(\"test\", header_translations = \"headers.txt\", out_header = \"new_headers.txt\", allele_ref_std = hapmap_ref, allele_ref_alt = alternative_ref, update_alt = TRUE, update_as_rdata = FALSE, update_savename = \"alt_ref\") # automatic QC of multiple files QC_series( data_files= c(\"data1.txt\",\"data2.txt\",\"data3.txt\"), output_filenames = c(\"output1.txt\",\"output2.txt\",\"output3.txt\"), dir_data = \"preQC\", dir_output = \"postQC\", dir_references = \"QC_files\", header_translations = header_translations, save_final_dataset = TRUE, HQfilter_FRQ = 0.01, HQfilter_HWE = 10^-6, HQfilter_cal = 0.95, HQfilter_imp = 0.3, QQfilter_FRQ = c(NA, 0.01, 0.03, 0.05, 3), QQfilter_HWE = c(NA, 10^-6, 10^-4), QQfilter_cal = c(NA, 0.95, 0.98, 0.99), QQfilter_imp = c(NA, 0.3, 0.5, 0.7, 0.9), NAfilter = TRUE, allele_ref_std = \"ref_hapmap.RData\", allele_name_std = \"HapMap\", remove_mismatches = TRUE, allele_ref_alt = \"ref_alternative.RData\", allele_name_alt = \"alternative\", update_alt = TRUE, update_as_rdata = TRUE, backup_alt = TRUE) Note the changes required with the HapMap reference, i.e., download.file(url = paste0(\"https://ftp.hapmap.org/hapmap/frequencies/2010-08_phaseII+III/\", \"allele_freqs_chr\", dn, \"_\", download_subset, \"_r28_nr.b36_fwd.txt.gz\"), method = \"curl\", extra = \"--insecure\", destfile = paste0(dir, \"/allele_freqs_chr\", dn, \"_\", download_subset, \"_r28_nr.b36_fwd.txt.gz\")) ftp://ftp.ncbi.nlm.nih.gov/hapmap/frequencies/2010-08_phaseII+III/ might also work. SAIGE The address of GitHub repository is here, https://github.com/weizhouUMICH Information including installation is described here, https://github.com/weizhouUMICH/SAIGE/wiki/Genetic-association-tests-using-SAIGE. Locally, we therefore check for the desired gcc, cmake and boost and proceed as follows, module avail gcc module avail cmake module avail boost # gcc > 5.5 and cmake > 3.8.1 module load gcc-6.1.0-gcc-4.8.5-jusvegv cmake-3.8.1-gcc-4.8.5-zz55m7 # boost 1.58.0 and R 3.6.0 are described in Computationl_Statistics repository. export LD_LIBRARY_PATH=/rds-d4/user/jhz22/hpc-work/boost_1_58_0/stage/lib:$LD_LIBRARY_PATH # we actually use the binary disrtibution directly wget https://github.com/weizhouUMICH/SAIGE/archive/v0.35.8.2.tar.gz tar tvfz v0.35.8.2.tar.gz cd SAIGE-0.35.8.2 tar xvfz SAIGE_0.35.8.2_R_x86_64-pc-linux-gnu.tar.gz mv SAIGE /rds-d4/user/jhz22/hpc-work/R R --no-save <<END library(SAIGE) END Since the required packages Rcpp and RcppParallel are relatively easy to deal with, with which we then simply load the packague as usual. A recent description is given here, https://cambridge-ceu.github.io/csd3/applications/SAIGE.html . ensemblVEP There is no particular difficulty, simply use BiocManager::install(\"ensemblVEP\") .","title":"--- eQTL, epigenome-wide association study (EWAS). ---"},{"location":"AI/","text":"Artificial intelligence pytorch The home page is https://pytorch.github.io , and the repository itself https://github.com/pytorch/ with https://github.com/pytorch/examples . tensorflow The tensorflow repository is here, https://github.com/tensorflow/tensorflow , and it is relatively easy to install via pip, pip install tensorflow python <<END import tensorflow as tf hello = tf.constant('Hello, TensorFlow!') sess = tf.Session() print(sess.run(hello)) END Follow https://github.com/aymericdamien/TensorFlow-Examples for readily adaptible examples. Also https://github.com/apress/pro-deep-learning-w-tensorflow tensorQTL AI-derived implementation. https://github.com/broadinstitute/tensorqtl , https://github.com/broadinstitute/SignatureAnalyzer-GPU module load python/3.6 git clone git@github.com:broadinstitute/tensorqtl.git cd tensorqtl # set up virtual environment and install virtualenv venv source venv/bin/activate pip install -r install/requirements.txt . cd example wget https://personal.broadinstitute.org/francois/geuvadis/GEUVADIS.445_samples.GRCh38.20170504.maf01.filtered.nodup.bed wget https://personal.broadinstitute.org/francois/geuvadis/GEUVADIS.445_samples.GRCh38.20170504.maf01.filtered.nodup.bim wget https://personal.broadinstitute.org/francois/geuvadis/GEUVADIS.445_samples.GRCh38.20170504.maf01.filtered.nodup.fam wget https://personal.broadinstitute.org/francois/geuvadis/GEUVADIS.445_samples.covariates.txt wget https://personal.broadinstitute.org/francois/geuvadis/GEUVADIS.445_samples.expression.bed.gz # Jupyter notebook sed -i 's/filtered/filtered.nodup/g' tensorqtl_examples.ipynb # csd3 hostname jupyter notebook --ip=127.0.0.1 --no-browser --port 8081 # local host ssh -4 -L 8081:127.0.0.1:8081 -fN hostname.hpc.cam.ac.uk firefox <generated URL from jupyter notebook command above> & Note that a Parquet file is generated we use SparkR, module load spark/2.4.0-bin-hadoop2.7 followed by library(SparkR) sparkR.session() df <- read.parquet(\"GEUVADIS.445_samples.cis_qtl_pairs.chr18.parquet\") head(df) to get > dim(df) [1] 2927819 9 > head(df) phenotype_id variant_id tss_distance maf ma_samples 1 ENSG00000263006.6 chr18_10644_C_G_b38 -98421 0.01685393 15 2 ENSG00000263006.6 chr18_10847_C_A_b38 -98218 0.01910112 17 3 ENSG00000263006.6 chr18_11275_G_A_b38 -97790 0.02471910 22 4 ENSG00000263006.6 chr18_11358_G_A_b38 -97707 0.02471910 22 5 ENSG00000263006.6 chr18_11445_G_A_b38 -97620 0.02359551 21 6 ENSG00000263006.6 chr18_13859_G_C_b38 -95206 0.02471910 22 ma_count pval_nominal slope slope_se 1 15 0.5808729 -0.11776078 0.2131254 2 17 0.1428839 -0.29872555 0.2035047 3 22 0.7452308 0.05461900 0.1679810 4 22 0.7452308 0.05461900 0.1679810 5 21 0.6032759 0.08937798 0.1718505 6 22 0.7452308 0.05461900 0.1679810 An alternative is to tweak the R package arrow . The command-line counterpart is as follows, export plink_prefix_path=GEUVADIS.445_samples.GRCh38.20170504.maf01.filtered.nodup export expression_bed=GEUVADIS.445_samples.expression.bed.gz export covariates_file=GEUVADIS.445_samples.covariates.txt export prefix=GEUVADIS.445_samples python3 -m tensorqtl ${plink_prefix_path} ${expression_bed} ${prefix} \\ --covariates ${covariates_file} \\ --mode cis python3 -m tensorqtl ${plink_prefix_path} ${expression_bed} ${prefix} \\ --covariates ${covariates_file} \\ --mode trans Again one can read the Parquet format output. Taylor-Weiner et al (2019). Scaling computational genomics to millions of individuals with GPUs. Genome Biol 20:228, https://doi.org/10.1186/s13059-019-1836-7","title":"Artificial intelligence"},{"location":"AI/#artificial-intelligence","text":"","title":"Artificial intelligence"},{"location":"AI/#pytorch","text":"The home page is https://pytorch.github.io , and the repository itself https://github.com/pytorch/ with https://github.com/pytorch/examples .","title":"pytorch"},{"location":"AI/#tensorflow","text":"The tensorflow repository is here, https://github.com/tensorflow/tensorflow , and it is relatively easy to install via pip, pip install tensorflow python <<END import tensorflow as tf hello = tf.constant('Hello, TensorFlow!') sess = tf.Session() print(sess.run(hello)) END Follow https://github.com/aymericdamien/TensorFlow-Examples for readily adaptible examples. Also https://github.com/apress/pro-deep-learning-w-tensorflow","title":"tensorflow"},{"location":"AI/#tensorqtl","text":"AI-derived implementation. https://github.com/broadinstitute/tensorqtl , https://github.com/broadinstitute/SignatureAnalyzer-GPU module load python/3.6 git clone git@github.com:broadinstitute/tensorqtl.git cd tensorqtl # set up virtual environment and install virtualenv venv source venv/bin/activate pip install -r install/requirements.txt . cd example wget https://personal.broadinstitute.org/francois/geuvadis/GEUVADIS.445_samples.GRCh38.20170504.maf01.filtered.nodup.bed wget https://personal.broadinstitute.org/francois/geuvadis/GEUVADIS.445_samples.GRCh38.20170504.maf01.filtered.nodup.bim wget https://personal.broadinstitute.org/francois/geuvadis/GEUVADIS.445_samples.GRCh38.20170504.maf01.filtered.nodup.fam wget https://personal.broadinstitute.org/francois/geuvadis/GEUVADIS.445_samples.covariates.txt wget https://personal.broadinstitute.org/francois/geuvadis/GEUVADIS.445_samples.expression.bed.gz # Jupyter notebook sed -i 's/filtered/filtered.nodup/g' tensorqtl_examples.ipynb # csd3 hostname jupyter notebook --ip=127.0.0.1 --no-browser --port 8081 # local host ssh -4 -L 8081:127.0.0.1:8081 -fN hostname.hpc.cam.ac.uk firefox <generated URL from jupyter notebook command above> & Note that a Parquet file is generated we use SparkR, module load spark/2.4.0-bin-hadoop2.7 followed by library(SparkR) sparkR.session() df <- read.parquet(\"GEUVADIS.445_samples.cis_qtl_pairs.chr18.parquet\") head(df) to get > dim(df) [1] 2927819 9 > head(df) phenotype_id variant_id tss_distance maf ma_samples 1 ENSG00000263006.6 chr18_10644_C_G_b38 -98421 0.01685393 15 2 ENSG00000263006.6 chr18_10847_C_A_b38 -98218 0.01910112 17 3 ENSG00000263006.6 chr18_11275_G_A_b38 -97790 0.02471910 22 4 ENSG00000263006.6 chr18_11358_G_A_b38 -97707 0.02471910 22 5 ENSG00000263006.6 chr18_11445_G_A_b38 -97620 0.02359551 21 6 ENSG00000263006.6 chr18_13859_G_C_b38 -95206 0.02471910 22 ma_count pval_nominal slope slope_se 1 15 0.5808729 -0.11776078 0.2131254 2 17 0.1428839 -0.29872555 0.2035047 3 22 0.7452308 0.05461900 0.1679810 4 22 0.7452308 0.05461900 0.1679810 5 21 0.6032759 0.08937798 0.1718505 6 22 0.7452308 0.05461900 0.1679810 An alternative is to tweak the R package arrow . The command-line counterpart is as follows, export plink_prefix_path=GEUVADIS.445_samples.GRCh38.20170504.maf01.filtered.nodup export expression_bed=GEUVADIS.445_samples.expression.bed.gz export covariates_file=GEUVADIS.445_samples.covariates.txt export prefix=GEUVADIS.445_samples python3 -m tensorqtl ${plink_prefix_path} ${expression_bed} ${prefix} \\ --covariates ${covariates_file} \\ --mode cis python3 -m tensorqtl ${plink_prefix_path} ${expression_bed} ${prefix} \\ --covariates ${covariates_file} \\ --mode trans Again one can read the Parquet format output. Taylor-Weiner et al (2019). Scaling computational genomics to millions of individuals with GPUs. Genome Biol 20:228, https://doi.org/10.1186/s13059-019-1836-7","title":"tensorQTL"},{"location":"CRISPR/","text":"CRISPR ADAM2 R package for identification of essential genes using genome-wide CRISPR-Cas9 screens, https://github.com/DepMap-Analytics/ADAM2 CHOPCHOP CHOPCHOP is a python script that allows quick and customizable design of guide RNA. We support selecting target sites for CRISPR/Cas9, CRISPR/Cpf1, TALEN and NICKASE with wide range of customization. We even support Cas13 for isoform targeting. https://chopchop.cbu.uib.no/ https://bitbucket.org/valenlab/chopchop/src/master/ CRISPOR http://crispor.tefor.net/ , design, evaluate and clone guide sequences for the CRISPR/Cas9 system. CRISPR design http://crispr.dbcls.jp/ CRISPResso2 https://github.com/pinellolab/CRISPResso2 DeepSpCas9 http://deepcrispr.info/DeepSpCas9/ Kim HK, et al. (2019). SpCas9 activity prediction by DeepSpCas9, a deep learning\u2013based model with high generalization performance. Sci. Adv. 2019;5: eaax9249. FORECasT Scripts for processing and predicting CRISPR/Cas9-generated mutations. https://partslab.sanger.ac.uk/FORECasT Allen F, et al. (2019). Predicting the mutations generated by repair of Cas9-induced double-strand breaks. Nat Biotechnol 37: 64\u201372 GPP Web portal https://portals.broadinstitute.org/gpp/public/analysis-tools/sgrna-design inDelphi inDelphi is a machine learning algorithm that is aimed to assist scientists using CRISPR. http://indelphi.giffordlab.mit.edu/ SelfTarget https://github.com/felicityallen/SelfTarget SPROUT https://zou-group.github.io/SPROUT https://github.com/amirmohan/SPROUT Leenay RT, et al. (2019). Large dataset enables prediction of repair after CRISPR\u2013Cas9 editing in primary T cells. Nat Biotechnol","title":"CRISPR"},{"location":"CRISPR/#crispr","text":"","title":"CRISPR"},{"location":"CRISPR/#adam2","text":"R package for identification of essential genes using genome-wide CRISPR-Cas9 screens, https://github.com/DepMap-Analytics/ADAM2","title":"ADAM2"},{"location":"CRISPR/#chopchop","text":"CHOPCHOP is a python script that allows quick and customizable design of guide RNA. We support selecting target sites for CRISPR/Cas9, CRISPR/Cpf1, TALEN and NICKASE with wide range of customization. We even support Cas13 for isoform targeting. https://chopchop.cbu.uib.no/ https://bitbucket.org/valenlab/chopchop/src/master/","title":"CHOPCHOP"},{"location":"CRISPR/#crispor","text":"http://crispor.tefor.net/ , design, evaluate and clone guide sequences for the CRISPR/Cas9 system.","title":"CRISPOR"},{"location":"CRISPR/#crispr-design","text":"http://crispr.dbcls.jp/","title":"CRISPR design"},{"location":"CRISPR/#crispresso2","text":"https://github.com/pinellolab/CRISPResso2","title":"CRISPResso2"},{"location":"CRISPR/#deepspcas9","text":"http://deepcrispr.info/DeepSpCas9/ Kim HK, et al. (2019). SpCas9 activity prediction by DeepSpCas9, a deep learning\u2013based model with high generalization performance. Sci. Adv. 2019;5: eaax9249.","title":"DeepSpCas9"},{"location":"CRISPR/#forecast","text":"Scripts for processing and predicting CRISPR/Cas9-generated mutations. https://partslab.sanger.ac.uk/FORECasT Allen F, et al. (2019). Predicting the mutations generated by repair of Cas9-induced double-strand breaks. Nat Biotechnol 37: 64\u201372","title":"FORECasT"},{"location":"CRISPR/#gpp-web-portal","text":"https://portals.broadinstitute.org/gpp/public/analysis-tools/sgrna-design","title":"GPP Web portal"},{"location":"CRISPR/#indelphi","text":"inDelphi is a machine learning algorithm that is aimed to assist scientists using CRISPR. http://indelphi.giffordlab.mit.edu/","title":"inDelphi"},{"location":"CRISPR/#selftarget","text":"https://github.com/felicityallen/SelfTarget","title":"SelfTarget"},{"location":"CRISPR/#sprout","text":"https://zou-group.github.io/SPROUT https://github.com/amirmohan/SPROUT Leenay RT, et al. (2019). Large dataset enables prediction of repair after CRISPR\u2013Cas9 editing in primary T cells. Nat Biotechnol","title":"SPROUT"},{"location":"NGS/","text":"NGS Review Pabinger S, et al. (2014). A survey of tools for variant analysis of next-generation genome sequencing data. Brief Bioinformatics 15(2):256-278. NGS pipeline https://www.hgsc.bcm.edu/software/mercury Annotation https://sites.google.com/site/jpopgen/wgsa Case studies Lagana A, et al. (2018). Precision medicine for relapsed multiple myeloma on the basis of an integrative multiomics approach. JCO Prec Oncol. Data Suppl, http://ascopubs.org/doi/suppl/10.1200/PO.18.00019 Lu X-M, et al. (2018). Association of breast and ovarian cancers with predisposition genes identified by large-scale sequencing. JAMA Oncol , doi:10.1001/jamaoncol.2018.2956. Mestek-Boukhibar L, et al. (2018). Rapid Paediatric Sequencing (RaPS): comprehensive real-life workflow for rapid diagnosis of critically ill children. J Med Genet , doi:10.1136/jmedgenet-2018-105396 Castel SE, et al. (2018). Modified penetrance of coding variants by cis-regulatory variation contributes to disease risk. Nat Genet , https://doi.org/10.1038/s41588-018-0192-y. Dixon JR, et al. (2018). Integrative detection and analysis of structural variation in cancer genomes. Nat Genet , https://www.nature.com/articles/s41588-018-0195-8 Wood DE, et al. (2018). A machine learning approach for somatic mutation discovery. Sci. Transl. Med. 10, eaar7939 (2018) DOI: 10.1126/scitranslmed.aar7939 Agotron detection The following is according to https://github.com/ncrnalab/agotron_detector as described in Hansen TB (2018). Detecting Agotrons in Ago CLIPseq Data. in Vang \u00d8rom UA (ed) miRNA Biogenesis-Methods and Protocols, Chapter 17, 221-232. Springer. wget http://hgdownload.cse.ucsc.edu/goldenPath/hg19/bigZips/chromFa.tar.gz tar -zxvf chromFa.tar.gz cat *.fa > hg19.fa samtools faidx hg19.fa bowtie2-build hg19.fa hg19 # GSE78059 for srr in 008/SRR3177718/SRR3177718 009/SRR3177719/SRR3177719 000/SRR3177720/SRR3177720 001/SRR3177721/SRR3177721 002/SRR3177722/SRR3177722 003/SRR3177723/SRR3177723 do wget ftp.sra.ebi.ac.uk/vol1/fastq/SRR317/$srr.fastq.gz done trim_galore -A TCAGTCACTTCCAGC -length 18 *.fastq.gz for i in *_trimmed.fq.gz do echo $i bowtie2 -q --local -x hg19 -U $i | samtools sort - > $i.sort.bam samtools index $i.sort.bam done python UCSC_intron_retriever.py | python analyzer.py -g hg19.fa | Rscript annotater.R Note that it is easier to implement with prefetch as shown below. Alignment and variant calling tutorial See https://github.com/ekg/alignment-and-variant-calling-tutorial. Note that E.coli_K12_MG1655.fa is unavailable any more, instead we have to download it directly from NCBI, https://www.ncbi.nlm.nih.gov/nuccore/556503834, choose FASTA (text), to reach https://www.ncbi.nlm.nih.gov/nuccore/NC_000913.3?report=fasta&log$=seqview&format=text and save to a local file, whose empty lines have to be removed, see them with awk '(length($NF)==0){print NR}' E.coli_K12_MG1655.fa. The fastq-dump generates .fa files, which need to be compressed with gzip. bowtie-scaling https://github.com/BenLangmead/bowtie-scaling Exome sequencing analysis IMSGC (2018). Low-frequency and rare-coding variation contributes to multiple sclerosis risk. Cell . DOI:https://doi.org/10.1016/j.cell.2018.09.049 has associate software, https://github.com/cotsapaslab/IMSGCexomechip . CNV detection CN-Learn is a framework to integrate Copy Number Variant (CNV) predictions made by multiple algorithms using exome sequencing datasets. https://github.com/girirajanlab/CN_Learn Pounraja VK, et al. (2019) A machine-learning approach for accurate detection of copy-number variants from exome sequencing. Genome Res . SNP discovery The following reference discribes several pipelines for SNP discovery. Morin PA, Foote AD, Hill CM, Simon-Bouhet B, Lang AR, Louis M (2018). SNP Discovery from Single and Multiplex Genome Assemblies of Non-model Organisms, in Head SR, et al. (eds.), Next Generation Sequencing: Methods and Protocols, Chapter 9, 113-144, Springer. whose scripts are available from https://github.com/PAMorin/SNPdiscovery/ . See also https://github.com/sanger-pathogens/snp-sites and the following references, Martin J, Schackwitz W, Lipzen A (2018). Genomic Sequence Variation Analysis by Resequencing, in de Vries RP, Tsang A, Grigoriev IV (ed) Fungal Genomics-Methods and Protocols, 2e, Chapter 18, 229-239, Springer. Raghavachari N, Garcia-Reyero N (eds.) (2018), Gene Expression Analysis-Methods and Protocols, Springer. TSS Mejia-Guerra MK, et al. (2018). Genome-Wide TSS Identification in Maize. Chapter 14, 239-256, in Yamaguchi N (ed.), Plant Transcription Factors-Methods and Protocols, Springer Comparison of gene expression pipelines on RNA-seq sequencing data. http://statapps.ugent.be/tools/AppDGE/ GSNAP, MapSplice, RUM, STAR, RNA-seq pipeline # gsnap wget http://research-pub.gene.com/gmap/src/gmap-gsnap-2018-07-04.tar.gz tar xfz gmap-gsnap-2018-07-04.tar.gz cd gmap-2018-07-04 ./configure make sudo make install # mapsplice, the latest version from http://protocols.netlab.uky.edu/~zeng/MapSplice-v2.2.1.zip has compiling issue sudo `which conda` install mapsplice mapsplice.py # rum git clone https://github.com/itmat/rum cd rum perl Makefile.PL make sudo make install # STAR git clone https://github.com/alexdobin/STAR cd STAR/source make See https://github.com/sanger-pathogens/Bio-RNASeq for RNA-seq pipeline. Mendelian RNA-seq https://github.com/komalsrathi/MendelianRNA-seq The relevant installations: conda create --name mendelian-rnaseq-env source activate mendelian-rnaseq-env conda install -c bioconda snakemake conda install -c bioconda rna-seqc conda install -c bioconda gatk conda install -c biobuilds plink conda install -c bioconda star conda install -c bioconda picard conda install -c bioconda bwa conda install -c anaconda colorama conda install -c bioconda misopy sra-toolkit, tophat These are very straightforward, e.g., prefetch -v SRR3534842 fastq-dump --split-files --gzip SRR3534842 the SRR3534842.sra from prefetch is actually at $HOME/ncbi/public/sra which is split into SRR3534842_1.fastq.gz , SRR3534842_2.fastq.gz at the current directory. See https://www.biostars.org/p/111040/. However, the location may not desirable since it may create a huge .vdi files with VirtualBox -- to get around we do this cd $HOME mkdir -p /home/jhz22/D/work/ncbi/public/sra ln -sf /home/jhz22/D/work/ncbi where D is actually a shared folder at Windows. To run tophat , see https://ccb.jhu.edu/software/tophat/tutorial.shtml wget https://ccb.jhu.edu/software/tophat/downloads/test_data.tar.gz tar xvfz test_data.tar.gz cd test_data tophat -r 20 test_ref reads_1.fq reads_2.fq Software adVNTR It is a tool for genotyping Variable Number Tandem Repeats (VNTR) from sequence data, https://github.com/mehrdadbakhtiari/adVNTR. ANGSD ANGSD is a software for analyzing next generation sequencing data, http://www.popgen.dk/angsd/index.php/ANGSD. It is relatively straightforward with GitHub; after git clone https://github.com/ANGSD/angsd cd angsd make but the following change is needed on line 468 of misc/msHOT2glf.c : tmppch as in (tmppch=='\\0') should be *tmppch as in (*tmppch==''0') , suggested by the compiler. Ubuntu archive: bamtools, bcftools, bedops, bedtools, blast (ncbi-blast+), bowtie2, fastqc, fastx-toolkit, freebayes, hmmer, hisat2, picard-tools, rsem, sambamba, samtools, seqtk, sra-toolkit, subread, tophat, trinityrnaseq, vcftool, vowpal-wabbit Install with sudo apt install . See also https://github.com/lh3/seqtk . Besides notes above, this is also possible: bcftools wget https://github.com/samtools/bcftools/releases/download/1.9/bcftools-1.9.tar.bz2 tar jfx bcftools-1.9.tar.bz2 cd bcftools-1.9 ./configure --prefix=$HPC_WORK make make install It is necessary to set the environment variables to enable plugins, so we could generate a version at $HPC_WORK/bin instead, #!/usr/bin/bash export BCFTOOLS_PLUGINS=$HPC_WORK/bcftools-1.9/plugins $HPC_WORK/bcftools-1.9/bcftools \"$@\" We invoke bcftools +check_ploidy my.vcf.gz Interestingly, this also save space! bowtie2 The project home is https://sourceforge.net/projects/bowtie-bio , whereby wget https://sourceforge.net/projects/bowtie-bio/files/bowtie2/2.3.4.1/bowtie2-2.3.4.1-linux-x86_64.zip wget https://sourceforge.net/projects/bowtie-bio/files/bowtie2/2.3.4.1/bowtie2-2.3.4.1-source.zip unzip bowtie2-2.3.4.1-linux-x86_64.zip cd bowtie2-2.3.4.1-linux-x86_64/ The test is then self-contained, export BT2_HOME=/home/jhz22/D/genetics/bowtie2-2.3.4.1-linux-x86_64 $BT2_HOME/bowtie2-build $BT2_HOME/example/reference/lambda_virus.fa lambda_virus $BT2_HOME/bowtie2 -x lambda_virus -U $BT2_HOME/example/reads/reads_1.fq -S eg1.sam $BT2_HOME/bowtie2 -x $BT2_HOME/example/index/lambda_virus -1 $BT2_HOME/example/reads/reads_1.fq -2 $BT2_HOME/example/reads/reads_2.fq -S eg2.sam samtools view -bS eg2.sam > eg2.bam samtools sort eg2.bam -o eg2.sorted.bam samtools mpileup -uf $BT2_HOME/example/reference/lambda_virus.fa eg2.sorted.bam | bcftools view -Ov - > eg2.raw.bcf bcftools view eg2.raw.bcf Like samtools, etc. it is possible to involve sudo apt install bowtie2 . CNVkit Genome-wide copy number from high-throughput sequencing, available from https://cnvkit.readthedocs.io/en/stable/ cutadapt, TrimGalore A prerequesite is to install cython. git clone https://github.com/marcelm/cutadapt cd cutadapt sudo python setup.py install git clone https://github.com/FelixKrueger/TrimGalore DeepVariant It is a deep neural network to call genetic variants from next-generation DNA sequencing data, https://github.com/google/deepvariant. EasyQC http://www.niot.res.in/EasyQC/ Exomiser git clone https://github.com/exomiser/Exomiser cd Exomiser mvn package See also https://github.com/exomiser/exomiser-demo . fastq-splitter The scripts divides a large FASTQ file into a set of smaller equally sized files, http://kirill-kryukov.com/study/tools/fastq-splitter/ . fastx_toolkit, RSEM It is also available from https://github.com/agordon/fastx_toolkit along with https://github.com/agordon/libgtextutils , and do away with the notorious automake-1.14 problem associated with sources at http://hannonlab.cshl.edu/fastx_toolkit/download.html. However, line 105 of src/fasta_formatter/fasta_formatter.cpp requires usage() followed by exit(0); as suggested in the issue section. More oever, usage() is a void function so its own exit(0) is unnecessary. The GitHub pages for RSEM are https://github.com/deweylab/RSEM and https://deweylab.github.io/RSEM/ . It is also recommended that the Bioconductor package EBSeq be installed. freebayes Try git clone --recursive https://github.com/ekg/freebayes make sudo make install GATK The source is available from https://github.com/broadinstitute/gatk/ but it is more convenient to use https://github.com/broadinstitute/gatk/releases/. ln -s `pwd`/gatk $HOME/bin/gatk gatk --help gatk --list hisat2, sambamba, picard-tools, StringTie Except StringTie, this is overlapped with apt install above, brew tap brewsci/bio brew tap brewsci/science brew install hisat2 hisat2-build brew install sambamba brew install picard-tools brew install stringtie It could be useful with ``brew reinstall```. See Raghavachari N, Garcia-Reyero N (eds.) (2018), Gene Expression Analysis-Methods and Protocols, https://www.springer.com/us/book/9781493978335, Chapter 15, Springer. Nevertheless it may be slower, e.g., tophat, compared to sudo apt install . IGV The download can be seeded from http://data.broadinstitute.org/igv, e.g., http://data.broadinstitute.org/igv/projects/downloads/2.4/IGV_2.4.10.zip. Again the source code is from GitHub, https://github.com/igvteam/igv/. For developers, igv.js is very appealing. Jannovar From the GitHub repository, it is seen to use project object model (POM), an XML representation of a Maven project held in a file named pom.xml . We therefore install maven first, sudo apt install maven The installation then proceeds as follows, git clone https://github.com/charite/jannovar cd jannovar mvn package Other tasks such as compile, test, etc. are also possible. It is handy to use symbolic link, i.e., ln -s /home/jhz22/D/genetics/jannovar/jannovar-cli/target/jannovar-cli-0.24.jar $HOME/bin/Jannovar.jar java -jar $HOME/bin/Jannovar.jar db-list java -jar $HOME/bin/Jannovar.jar download -d hg19/refseq We may need to set memory size, e.g., java -Xms2G -Xmx4G -jar $HOME/bin/Jannovar.jar Melissa https://github.com/andreaskapou/Melissa MEthyLation Inference for Single cell Analysis (Melissa), is a Bayesian hierarchical method to quantify spatially-varying methylation profiles across genomic regions from single-cell bisulfite sequencing data (scBS-seq). Melissa clusters individual cells based on local methylation patterns, enabling the discovery of epigenetic diversities and commonalities among individual cells. The clustering also acts as an effective regularisation method for imputation of methylation on unassayed CpG sites, enabling transfer of information between individual cells. Kapourani C-A, Sanguinetti G (2019). Melissa: Bayesian clustering and imputation of single-cell methylomes, Genome Biology 20:61, https://doi.org/10.1186/s13059-019-1665-8 MONSTER http://galton.uchicago.edu/~mcpeek/software/MONSTER/ ( http://galton.uchicago.edu/~mcpeek/software/MONSTER/MONSTER_v1.3.tar.gz ) Jiang D, McPeek MS (2014). Robust Rare Variant Association Testing for Quantitative Traits in Samples with Related Individuals. Genetic Epidemiology 38(1):10-20 pindel The software can be obtained from https://github.com/genome/pindel . After htslib is installed, the canonical instruction is to issue git clone https://github.com/samtools/htslib cd htslib make sudo make install cd - git clone https://github.com/genome/pindel cd pindel ./INSTALL ../htslib It is 'standard' to have complaints about pindel.cpp, bddate.cpp and genotyping.cpp, for abs() rather than fabs() from the header file cmath have been used. The issue goes away when abs is replaced with fabs and in the case of bddata.cpp, it is also necessary to invoke the header, i.e., #include <cmath> rtg-tools It is available from https://www.realtimegenomics.com/products/rtg-tools and GitHub, git clone https://github.com/RealTimeGenomics/rtg-tools.git ant dir dist sambamba While the source contains ldc2, it is readily available with Ubuntu archive nevertheless failed to compile, so we proceed with instructions at the GitHub, e.g., export PATH=$HOME/ldc2-1.10.0-linux-x86_64/bin:$PATH export LIBRARY_PATH=$HOME/ldc2-1.10.0-linux-x86_64/lib for version 1.10.0. samtools To build from source, we do these, git clone https://github.com/samtools/htslib cd htslib make cd - git clone https://github.com/samtools/samtools cd samtools autoheader # Build config.h.in (this may generate a warning about # AC_CONFIG_SUBDIRS - please ignore it). autoconf -Wno-syntax # Generate the configure script ./configure # Needed for choosing optional functionality make make install Note bgzip and tabix are distributed with htslib. It is relatively easier to install from release, wget https://github.com/samtools/samtools/releases/download/1.9/samtools-1.9.tar.bz2 tar xjf samtools-1.9.tar.gz cd samtools-1.9 ./configure --prefix=/scratch/jhz22 make make install cd - cd htslib-1.9 ./configure --prefix=/scratch/jhz22 make make install where we install tabix as well. SnpEff, SnpSift, clinEff It is straightforward with the compiled version from sourceforge, which also includes clinEff. wget http://sourceforge.net/projects/snpeff/files/snpEff_latest_core.zip unzip snpEff_latest_core cd snpEff java -jar snpEff.jar databases java -jar snpEff.jar download GRCh38.76 wget http://sourceforge.net/projects/snpeff/files/databases/test_cases.tgz tar fvxz test_cases.tgz lists all the databases and download a particular one. Later, the test files are also downloaded and extracted. The following steps compile from source instead. git clone https://github.com/pcingola/SnpEff.git cd SnpEff mvn package mvn install cd lib # Antlr mvn install:install-file \\ -Dfile=antlr-4.5.1-complete.jar \\ -DgroupId=org.antlr \\ -DartifactId=antlr \\ -Dversion=4.5.1 \\ -Dpackaging=jar # BioJava core mvn install:install-file \\ -Dfile=biojava3-core-3.0.7.jar \\ -DgroupId=org.biojava \\ -DartifactId=biojava3-core \\ -Dversion=3.0.7 \\ -Dpackaging=jar # BioJava structure mvn install:install-file \\ -Dfile=biojava3-structure-3.0.7.jar \\ -DgroupId=org.biojava \\ -DartifactId=biojava3-structure \\ -Dversion=3.0.7 \\ -Dpackaging=jar cd - # SnpSift git clone https://github.com/pcingola/SnpSift.git cd SnpSift mvn package mvn install which gives target/SnpEff-4.3.jar and target/SnpSift-4.3.jar , respectively. Note that antlr4 is from GitHub, https://github.com/antlr/antlr4 . See also https://github.com/sanger-pathogens/SnpEffWrapper . subread It is available from http://subread.sourceforge.net/ . SViCT Short for Structural Variant detrction in Circulating Tumor DNA and is available from https://github.com/vpc-ccg/svict tagdust http://sourceforge.net/projects/tagdust/ Trinity RNA-Seq De novo Assembly Using Trinity, https://github.com/trinityrnaseq/trinityrnaseq/wiki . VarScan Hosted at https://github.com/dkoboldt/varscan , the .jar files are ready to use with git clone https://github.com/dkoboldt/varscan or from the repository releases. See http://varscan.sourceforge.net/ for further information. vcftools Assuming that we use zlib 1.2.8 from module zlib/1.2.8, we can do the following, wget https://github.com/vcftools/vcftools/releases/download/v0.1.16/vcftools-0.1.16.tar.gz tar xvfz vcftools-0.1.16.tar.gz module load zlib/1.2.8 ./configure --prefix=/scratch/jhz22 ZLIB_CFLAGS=\"-I/usr/local/Cluster-Apps/zlib/1.2.8/include\" ZLIB_LIBS=\"-L/usr/local/Cluster-Apps/zlib/1.2.8/lib -lz\" make make install To use vcf-concat, it is necessary to set the PERL5LIB environment variables, e.g., export PERL5LIB=/scratch/jhz22/share/perl5 WASP Allele-specific pipeline for unbiased read mapping and molecular QTL discovery, https://github.com/bmvdgeijn/WASP/ .","title":"NGS"},{"location":"NGS/#ngs","text":"","title":"NGS"},{"location":"NGS/#review","text":"Pabinger S, et al. (2014). A survey of tools for variant analysis of next-generation genome sequencing data. Brief Bioinformatics 15(2):256-278.","title":"Review"},{"location":"NGS/#ngs-pipeline","text":"https://www.hgsc.bcm.edu/software/mercury","title":"NGS pipeline"},{"location":"NGS/#annotation","text":"https://sites.google.com/site/jpopgen/wgsa","title":"Annotation"},{"location":"NGS/#case-studies","text":"Lagana A, et al. (2018). Precision medicine for relapsed multiple myeloma on the basis of an integrative multiomics approach. JCO Prec Oncol. Data Suppl, http://ascopubs.org/doi/suppl/10.1200/PO.18.00019 Lu X-M, et al. (2018). Association of breast and ovarian cancers with predisposition genes identified by large-scale sequencing. JAMA Oncol , doi:10.1001/jamaoncol.2018.2956. Mestek-Boukhibar L, et al. (2018). Rapid Paediatric Sequencing (RaPS): comprehensive real-life workflow for rapid diagnosis of critically ill children. J Med Genet , doi:10.1136/jmedgenet-2018-105396 Castel SE, et al. (2018). Modified penetrance of coding variants by cis-regulatory variation contributes to disease risk. Nat Genet , https://doi.org/10.1038/s41588-018-0192-y. Dixon JR, et al. (2018). Integrative detection and analysis of structural variation in cancer genomes. Nat Genet , https://www.nature.com/articles/s41588-018-0195-8 Wood DE, et al. (2018). A machine learning approach for somatic mutation discovery. Sci. Transl. Med. 10, eaar7939 (2018) DOI: 10.1126/scitranslmed.aar7939","title":"Case studies"},{"location":"NGS/#agotron-detection","text":"The following is according to https://github.com/ncrnalab/agotron_detector as described in Hansen TB (2018). Detecting Agotrons in Ago CLIPseq Data. in Vang \u00d8rom UA (ed) miRNA Biogenesis-Methods and Protocols, Chapter 17, 221-232. Springer. wget http://hgdownload.cse.ucsc.edu/goldenPath/hg19/bigZips/chromFa.tar.gz tar -zxvf chromFa.tar.gz cat *.fa > hg19.fa samtools faidx hg19.fa bowtie2-build hg19.fa hg19 # GSE78059 for srr in 008/SRR3177718/SRR3177718 009/SRR3177719/SRR3177719 000/SRR3177720/SRR3177720 001/SRR3177721/SRR3177721 002/SRR3177722/SRR3177722 003/SRR3177723/SRR3177723 do wget ftp.sra.ebi.ac.uk/vol1/fastq/SRR317/$srr.fastq.gz done trim_galore -A TCAGTCACTTCCAGC -length 18 *.fastq.gz for i in *_trimmed.fq.gz do echo $i bowtie2 -q --local -x hg19 -U $i | samtools sort - > $i.sort.bam samtools index $i.sort.bam done python UCSC_intron_retriever.py | python analyzer.py -g hg19.fa | Rscript annotater.R Note that it is easier to implement with prefetch as shown below.","title":"Agotron detection"},{"location":"NGS/#alignment-and-variant-calling-tutorial","text":"See https://github.com/ekg/alignment-and-variant-calling-tutorial. Note that E.coli_K12_MG1655.fa is unavailable any more, instead we have to download it directly from NCBI, https://www.ncbi.nlm.nih.gov/nuccore/556503834, choose FASTA (text), to reach https://www.ncbi.nlm.nih.gov/nuccore/NC_000913.3?report=fasta&log$=seqview&format=text and save to a local file, whose empty lines have to be removed, see them with awk '(length($NF)==0){print NR}' E.coli_K12_MG1655.fa. The fastq-dump generates .fa files, which need to be compressed with gzip.","title":"Alignment and variant calling tutorial"},{"location":"NGS/#bowtie-scaling","text":"https://github.com/BenLangmead/bowtie-scaling","title":"bowtie-scaling"},{"location":"NGS/#exome-sequencing-analysis","text":"IMSGC (2018). Low-frequency and rare-coding variation contributes to multiple sclerosis risk. Cell . DOI:https://doi.org/10.1016/j.cell.2018.09.049 has associate software, https://github.com/cotsapaslab/IMSGCexomechip .","title":"Exome sequencing analysis"},{"location":"NGS/#cnv-detection","text":"CN-Learn is a framework to integrate Copy Number Variant (CNV) predictions made by multiple algorithms using exome sequencing datasets. https://github.com/girirajanlab/CN_Learn Pounraja VK, et al. (2019) A machine-learning approach for accurate detection of copy-number variants from exome sequencing. Genome Res .","title":"CNV detection"},{"location":"NGS/#snp-discovery","text":"The following reference discribes several pipelines for SNP discovery. Morin PA, Foote AD, Hill CM, Simon-Bouhet B, Lang AR, Louis M (2018). SNP Discovery from Single and Multiplex Genome Assemblies of Non-model Organisms, in Head SR, et al. (eds.), Next Generation Sequencing: Methods and Protocols, Chapter 9, 113-144, Springer. whose scripts are available from https://github.com/PAMorin/SNPdiscovery/ . See also https://github.com/sanger-pathogens/snp-sites and the following references, Martin J, Schackwitz W, Lipzen A (2018). Genomic Sequence Variation Analysis by Resequencing, in de Vries RP, Tsang A, Grigoriev IV (ed) Fungal Genomics-Methods and Protocols, 2e, Chapter 18, 229-239, Springer. Raghavachari N, Garcia-Reyero N (eds.) (2018), Gene Expression Analysis-Methods and Protocols, Springer.","title":"SNP discovery"},{"location":"NGS/#tss","text":"Mejia-Guerra MK, et al. (2018). Genome-Wide TSS Identification in Maize. Chapter 14, 239-256, in Yamaguchi N (ed.), Plant Transcription Factors-Methods and Protocols, Springer","title":"TSS"},{"location":"NGS/#comparison-of-gene-expression-pipelines-on-rna-seq-sequencing-data","text":"http://statapps.ugent.be/tools/AppDGE/","title":"Comparison of gene expression pipelines on RNA-seq sequencing data."},{"location":"NGS/#gsnap-mapsplice-rum-star-rna-seq-pipeline","text":"# gsnap wget http://research-pub.gene.com/gmap/src/gmap-gsnap-2018-07-04.tar.gz tar xfz gmap-gsnap-2018-07-04.tar.gz cd gmap-2018-07-04 ./configure make sudo make install # mapsplice, the latest version from http://protocols.netlab.uky.edu/~zeng/MapSplice-v2.2.1.zip has compiling issue sudo `which conda` install mapsplice mapsplice.py # rum git clone https://github.com/itmat/rum cd rum perl Makefile.PL make sudo make install # STAR git clone https://github.com/alexdobin/STAR cd STAR/source make See https://github.com/sanger-pathogens/Bio-RNASeq for RNA-seq pipeline.","title":"GSNAP, MapSplice, RUM, STAR, RNA-seq pipeline"},{"location":"NGS/#mendelian-rna-seq","text":"https://github.com/komalsrathi/MendelianRNA-seq The relevant installations: conda create --name mendelian-rnaseq-env source activate mendelian-rnaseq-env conda install -c bioconda snakemake conda install -c bioconda rna-seqc conda install -c bioconda gatk conda install -c biobuilds plink conda install -c bioconda star conda install -c bioconda picard conda install -c bioconda bwa conda install -c anaconda colorama conda install -c bioconda misopy","title":"Mendelian RNA-seq"},{"location":"NGS/#sra-toolkit-tophat","text":"These are very straightforward, e.g., prefetch -v SRR3534842 fastq-dump --split-files --gzip SRR3534842 the SRR3534842.sra from prefetch is actually at $HOME/ncbi/public/sra which is split into SRR3534842_1.fastq.gz , SRR3534842_2.fastq.gz at the current directory. See https://www.biostars.org/p/111040/. However, the location may not desirable since it may create a huge .vdi files with VirtualBox -- to get around we do this cd $HOME mkdir -p /home/jhz22/D/work/ncbi/public/sra ln -sf /home/jhz22/D/work/ncbi where D is actually a shared folder at Windows. To run tophat , see https://ccb.jhu.edu/software/tophat/tutorial.shtml wget https://ccb.jhu.edu/software/tophat/downloads/test_data.tar.gz tar xvfz test_data.tar.gz cd test_data tophat -r 20 test_ref reads_1.fq reads_2.fq","title":"sra-toolkit, tophat"},{"location":"NGS/#software","text":"","title":"Software"},{"location":"NGS/#advntr","text":"It is a tool for genotyping Variable Number Tandem Repeats (VNTR) from sequence data, https://github.com/mehrdadbakhtiari/adVNTR.","title":"adVNTR"},{"location":"NGS/#angsd","text":"ANGSD is a software for analyzing next generation sequencing data, http://www.popgen.dk/angsd/index.php/ANGSD. It is relatively straightforward with GitHub; after git clone https://github.com/ANGSD/angsd cd angsd make but the following change is needed on line 468 of misc/msHOT2glf.c : tmppch as in (tmppch=='\\0') should be *tmppch as in (*tmppch==''0') , suggested by the compiler.","title":"ANGSD"},{"location":"NGS/#ubuntu-archive-bamtools-bcftools-bedops-bedtools-blast-ncbi-blast-bowtie2-fastqc-fastx-toolkit-freebayes-hmmer-hisat2-picard-tools-rsem-sambamba-samtools-seqtk-sra-toolkit-subread-tophat-trinityrnaseq-vcftool-vowpal-wabbit","text":"Install with sudo apt install . See also https://github.com/lh3/seqtk . Besides notes above, this is also possible:","title":"Ubuntu archive: bamtools, bcftools, bedops, bedtools, blast (ncbi-blast+), bowtie2, fastqc, fastx-toolkit, freebayes, hmmer, hisat2, picard-tools, rsem, sambamba, samtools, seqtk, sra-toolkit, subread, tophat, trinityrnaseq, vcftool, vowpal-wabbit"},{"location":"NGS/#bcftools","text":"wget https://github.com/samtools/bcftools/releases/download/1.9/bcftools-1.9.tar.bz2 tar jfx bcftools-1.9.tar.bz2 cd bcftools-1.9 ./configure --prefix=$HPC_WORK make make install It is necessary to set the environment variables to enable plugins, so we could generate a version at $HPC_WORK/bin instead, #!/usr/bin/bash export BCFTOOLS_PLUGINS=$HPC_WORK/bcftools-1.9/plugins $HPC_WORK/bcftools-1.9/bcftools \"$@\" We invoke bcftools +check_ploidy my.vcf.gz Interestingly, this also save space!","title":"bcftools"},{"location":"NGS/#bowtie2","text":"The project home is https://sourceforge.net/projects/bowtie-bio , whereby wget https://sourceforge.net/projects/bowtie-bio/files/bowtie2/2.3.4.1/bowtie2-2.3.4.1-linux-x86_64.zip wget https://sourceforge.net/projects/bowtie-bio/files/bowtie2/2.3.4.1/bowtie2-2.3.4.1-source.zip unzip bowtie2-2.3.4.1-linux-x86_64.zip cd bowtie2-2.3.4.1-linux-x86_64/ The test is then self-contained, export BT2_HOME=/home/jhz22/D/genetics/bowtie2-2.3.4.1-linux-x86_64 $BT2_HOME/bowtie2-build $BT2_HOME/example/reference/lambda_virus.fa lambda_virus $BT2_HOME/bowtie2 -x lambda_virus -U $BT2_HOME/example/reads/reads_1.fq -S eg1.sam $BT2_HOME/bowtie2 -x $BT2_HOME/example/index/lambda_virus -1 $BT2_HOME/example/reads/reads_1.fq -2 $BT2_HOME/example/reads/reads_2.fq -S eg2.sam samtools view -bS eg2.sam > eg2.bam samtools sort eg2.bam -o eg2.sorted.bam samtools mpileup -uf $BT2_HOME/example/reference/lambda_virus.fa eg2.sorted.bam | bcftools view -Ov - > eg2.raw.bcf bcftools view eg2.raw.bcf Like samtools, etc. it is possible to involve sudo apt install bowtie2 .","title":"bowtie2"},{"location":"NGS/#cnvkit","text":"Genome-wide copy number from high-throughput sequencing, available from https://cnvkit.readthedocs.io/en/stable/","title":"CNVkit"},{"location":"NGS/#cutadapt-trimgalore","text":"A prerequesite is to install cython. git clone https://github.com/marcelm/cutadapt cd cutadapt sudo python setup.py install git clone https://github.com/FelixKrueger/TrimGalore","title":"cutadapt, TrimGalore"},{"location":"NGS/#deepvariant","text":"It is a deep neural network to call genetic variants from next-generation DNA sequencing data, https://github.com/google/deepvariant.","title":"DeepVariant"},{"location":"NGS/#easyqc","text":"http://www.niot.res.in/EasyQC/","title":"EasyQC"},{"location":"NGS/#exomiser","text":"git clone https://github.com/exomiser/Exomiser cd Exomiser mvn package See also https://github.com/exomiser/exomiser-demo .","title":"Exomiser"},{"location":"NGS/#fastq-splitter","text":"The scripts divides a large FASTQ file into a set of smaller equally sized files, http://kirill-kryukov.com/study/tools/fastq-splitter/ .","title":"fastq-splitter"},{"location":"NGS/#fastx_toolkit-rsem","text":"It is also available from https://github.com/agordon/fastx_toolkit along with https://github.com/agordon/libgtextutils , and do away with the notorious automake-1.14 problem associated with sources at http://hannonlab.cshl.edu/fastx_toolkit/download.html. However, line 105 of src/fasta_formatter/fasta_formatter.cpp requires usage() followed by exit(0); as suggested in the issue section. More oever, usage() is a void function so its own exit(0) is unnecessary. The GitHub pages for RSEM are https://github.com/deweylab/RSEM and https://deweylab.github.io/RSEM/ . It is also recommended that the Bioconductor package EBSeq be installed.","title":"fastx_toolkit, RSEM"},{"location":"NGS/#freebayes","text":"Try git clone --recursive https://github.com/ekg/freebayes make sudo make install","title":"freebayes"},{"location":"NGS/#gatk","text":"The source is available from https://github.com/broadinstitute/gatk/ but it is more convenient to use https://github.com/broadinstitute/gatk/releases/. ln -s `pwd`/gatk $HOME/bin/gatk gatk --help gatk --list","title":"GATK"},{"location":"NGS/#hisat2-sambamba-picard-tools-stringtie","text":"Except StringTie, this is overlapped with apt install above, brew tap brewsci/bio brew tap brewsci/science brew install hisat2 hisat2-build brew install sambamba brew install picard-tools brew install stringtie It could be useful with ``brew reinstall```. See Raghavachari N, Garcia-Reyero N (eds.) (2018), Gene Expression Analysis-Methods and Protocols, https://www.springer.com/us/book/9781493978335, Chapter 15, Springer. Nevertheless it may be slower, e.g., tophat, compared to sudo apt install .","title":"hisat2, sambamba, picard-tools, StringTie"},{"location":"NGS/#igv","text":"The download can be seeded from http://data.broadinstitute.org/igv, e.g., http://data.broadinstitute.org/igv/projects/downloads/2.4/IGV_2.4.10.zip. Again the source code is from GitHub, https://github.com/igvteam/igv/. For developers, igv.js is very appealing.","title":"IGV"},{"location":"NGS/#jannovar","text":"From the GitHub repository, it is seen to use project object model (POM), an XML representation of a Maven project held in a file named pom.xml . We therefore install maven first, sudo apt install maven The installation then proceeds as follows, git clone https://github.com/charite/jannovar cd jannovar mvn package Other tasks such as compile, test, etc. are also possible. It is handy to use symbolic link, i.e., ln -s /home/jhz22/D/genetics/jannovar/jannovar-cli/target/jannovar-cli-0.24.jar $HOME/bin/Jannovar.jar java -jar $HOME/bin/Jannovar.jar db-list java -jar $HOME/bin/Jannovar.jar download -d hg19/refseq We may need to set memory size, e.g., java -Xms2G -Xmx4G -jar $HOME/bin/Jannovar.jar","title":"Jannovar"},{"location":"NGS/#melissa","text":"https://github.com/andreaskapou/Melissa MEthyLation Inference for Single cell Analysis (Melissa), is a Bayesian hierarchical method to quantify spatially-varying methylation profiles across genomic regions from single-cell bisulfite sequencing data (scBS-seq). Melissa clusters individual cells based on local methylation patterns, enabling the discovery of epigenetic diversities and commonalities among individual cells. The clustering also acts as an effective regularisation method for imputation of methylation on unassayed CpG sites, enabling transfer of information between individual cells. Kapourani C-A, Sanguinetti G (2019). Melissa: Bayesian clustering and imputation of single-cell methylomes, Genome Biology 20:61, https://doi.org/10.1186/s13059-019-1665-8","title":"Melissa"},{"location":"NGS/#monster","text":"http://galton.uchicago.edu/~mcpeek/software/MONSTER/ ( http://galton.uchicago.edu/~mcpeek/software/MONSTER/MONSTER_v1.3.tar.gz ) Jiang D, McPeek MS (2014). Robust Rare Variant Association Testing for Quantitative Traits in Samples with Related Individuals. Genetic Epidemiology 38(1):10-20","title":"MONSTER"},{"location":"NGS/#pindel","text":"The software can be obtained from https://github.com/genome/pindel . After htslib is installed, the canonical instruction is to issue git clone https://github.com/samtools/htslib cd htslib make sudo make install cd - git clone https://github.com/genome/pindel cd pindel ./INSTALL ../htslib It is 'standard' to have complaints about pindel.cpp, bddate.cpp and genotyping.cpp, for abs() rather than fabs() from the header file cmath have been used. The issue goes away when abs is replaced with fabs and in the case of bddata.cpp, it is also necessary to invoke the header, i.e., #include <cmath>","title":"pindel"},{"location":"NGS/#rtg-tools","text":"It is available from https://www.realtimegenomics.com/products/rtg-tools and GitHub, git clone https://github.com/RealTimeGenomics/rtg-tools.git ant dir dist","title":"rtg-tools"},{"location":"NGS/#sambamba","text":"While the source contains ldc2, it is readily available with Ubuntu archive nevertheless failed to compile, so we proceed with instructions at the GitHub, e.g., export PATH=$HOME/ldc2-1.10.0-linux-x86_64/bin:$PATH export LIBRARY_PATH=$HOME/ldc2-1.10.0-linux-x86_64/lib for version 1.10.0.","title":"sambamba"},{"location":"NGS/#samtools","text":"To build from source, we do these, git clone https://github.com/samtools/htslib cd htslib make cd - git clone https://github.com/samtools/samtools cd samtools autoheader # Build config.h.in (this may generate a warning about # AC_CONFIG_SUBDIRS - please ignore it). autoconf -Wno-syntax # Generate the configure script ./configure # Needed for choosing optional functionality make make install Note bgzip and tabix are distributed with htslib. It is relatively easier to install from release, wget https://github.com/samtools/samtools/releases/download/1.9/samtools-1.9.tar.bz2 tar xjf samtools-1.9.tar.gz cd samtools-1.9 ./configure --prefix=/scratch/jhz22 make make install cd - cd htslib-1.9 ./configure --prefix=/scratch/jhz22 make make install where we install tabix as well.","title":"samtools"},{"location":"NGS/#snpeff-snpsift-clineff","text":"It is straightforward with the compiled version from sourceforge, which also includes clinEff. wget http://sourceforge.net/projects/snpeff/files/snpEff_latest_core.zip unzip snpEff_latest_core cd snpEff java -jar snpEff.jar databases java -jar snpEff.jar download GRCh38.76 wget http://sourceforge.net/projects/snpeff/files/databases/test_cases.tgz tar fvxz test_cases.tgz lists all the databases and download a particular one. Later, the test files are also downloaded and extracted. The following steps compile from source instead. git clone https://github.com/pcingola/SnpEff.git cd SnpEff mvn package mvn install cd lib # Antlr mvn install:install-file \\ -Dfile=antlr-4.5.1-complete.jar \\ -DgroupId=org.antlr \\ -DartifactId=antlr \\ -Dversion=4.5.1 \\ -Dpackaging=jar # BioJava core mvn install:install-file \\ -Dfile=biojava3-core-3.0.7.jar \\ -DgroupId=org.biojava \\ -DartifactId=biojava3-core \\ -Dversion=3.0.7 \\ -Dpackaging=jar # BioJava structure mvn install:install-file \\ -Dfile=biojava3-structure-3.0.7.jar \\ -DgroupId=org.biojava \\ -DartifactId=biojava3-structure \\ -Dversion=3.0.7 \\ -Dpackaging=jar cd - # SnpSift git clone https://github.com/pcingola/SnpSift.git cd SnpSift mvn package mvn install which gives target/SnpEff-4.3.jar and target/SnpSift-4.3.jar , respectively. Note that antlr4 is from GitHub, https://github.com/antlr/antlr4 . See also https://github.com/sanger-pathogens/SnpEffWrapper .","title":"SnpEff, SnpSift, clinEff"},{"location":"NGS/#subread","text":"It is available from http://subread.sourceforge.net/ .","title":"subread"},{"location":"NGS/#svict","text":"Short for Structural Variant detrction in Circulating Tumor DNA and is available from https://github.com/vpc-ccg/svict","title":"SViCT"},{"location":"NGS/#tagdust","text":"http://sourceforge.net/projects/tagdust/","title":"tagdust"},{"location":"NGS/#trinity","text":"RNA-Seq De novo Assembly Using Trinity, https://github.com/trinityrnaseq/trinityrnaseq/wiki .","title":"Trinity"},{"location":"NGS/#varscan","text":"Hosted at https://github.com/dkoboldt/varscan , the .jar files are ready to use with git clone https://github.com/dkoboldt/varscan or from the repository releases. See http://varscan.sourceforge.net/ for further information.","title":"VarScan"},{"location":"NGS/#vcftools","text":"Assuming that we use zlib 1.2.8 from module zlib/1.2.8, we can do the following, wget https://github.com/vcftools/vcftools/releases/download/v0.1.16/vcftools-0.1.16.tar.gz tar xvfz vcftools-0.1.16.tar.gz module load zlib/1.2.8 ./configure --prefix=/scratch/jhz22 ZLIB_CFLAGS=\"-I/usr/local/Cluster-Apps/zlib/1.2.8/include\" ZLIB_LIBS=\"-L/usr/local/Cluster-Apps/zlib/1.2.8/lib -lz\" make make install To use vcf-concat, it is necessary to set the PERL5LIB environment variables, e.g., export PERL5LIB=/scratch/jhz22/share/perl5","title":"vcftools"},{"location":"NGS/#wasp","text":"Allele-specific pipeline for unbiased read mapping and molecular QTL discovery, https://github.com/bmvdgeijn/WASP/ .","title":"WASP"},{"location":"SL/","text":"Statistcal learning Deep learning Autoencoder Four types, https://github.com/nathanhubens/Autoencoders . The notMNIST dataset , https://www.datacamp.com/community/tutorials/autoencoder-keras-tutorial . Credit fraud example , https://blogs.rstudio.com/tensorflow/posts/2018-01-24-keras-fraud-autoencoder/ . PCA vs autoencoder Machine learning (as it is called in computer science literature) Apress, https://github.com/apress Packt, https://github.com/PacktPublishing Statistical learning and ISLR Hastie T, Tibshirani R, Friedman J (2009). The Elements of Statistical Learning: Data Mining, Inference, and Prediction . Springer. James G, Witten D, Hastie T, Tibshirani R (2013). An Introduction to Statistical Learning with Applications in R . Springer. James G, Witten D, Hastie T, Tibshirani R (2021). An Introduction to Statistical Learning with Applications in R, 2e , Springer. Chollet F, Allaire JJ (2017). Deep Learning with R , Manning. Source code , GitHub .","title":"Statistcal learning"},{"location":"SL/#statistcal-learning","text":"","title":"Statistcal learning"},{"location":"SL/#deep-learning","text":"","title":"Deep learning"},{"location":"SL/#autoencoder","text":"Four types, https://github.com/nathanhubens/Autoencoders . The notMNIST dataset , https://www.datacamp.com/community/tutorials/autoencoder-keras-tutorial . Credit fraud example , https://blogs.rstudio.com/tensorflow/posts/2018-01-24-keras-fraud-autoencoder/ . PCA vs autoencoder","title":"Autoencoder"},{"location":"SL/#machine-learning","text":"(as it is called in computer science literature) Apress, https://github.com/apress Packt, https://github.com/PacktPublishing","title":"Machine learning"},{"location":"SL/#statistical-learning-and-islr","text":"Hastie T, Tibshirani R, Friedman J (2009). The Elements of Statistical Learning: Data Mining, Inference, and Prediction . Springer. James G, Witten D, Hastie T, Tibshirani R (2013). An Introduction to Statistical Learning with Applications in R . Springer. James G, Witten D, Hastie T, Tibshirani R (2021). An Introduction to Statistical Learning with Applications in R, 2e , Springer. Chollet F, Allaire JJ (2017). Deep Learning with R , Manning. Source code , GitHub .","title":"Statistical learning and ISLR"},{"location":"misc/","text":"Miscellaneous software The source code is also available from the Ubuntu archive. There is a variety of packages in bioconda, https://bioconda.github.io/recipes.html# and conda-forge, https://conda-forge.org/feedstocks/. We can trick the test even with compiling errors, e.g., sudo apt install loki wget -qO- http://archive.ubuntu.com/ubuntu/pool/universe/l/loki/loki_2.4.7.4.orig.tar.gz | tar fvxz - cd loki ./configure # there are errors in compilng make cp /usr/bin/prep presrc cp /usr/bin/loki lokisrc cd test # we are actually fine make allegro This is according to https://www.decode.com/software/, sudo `which conda` install allegro Gudbjartsson DF, Thorvaldsson T, Kong A, Gunnarsson G, Ingolfsdottir A (2005). Allegro version 2. Nature Genetics 37:1015\u20131016 https://www.nature.com/articles/ng1005-1015?foxtrotcallback=true beast2-mcmc, beast2-mcmc-doc It is Bayesian MCMC phylogenetic inference. sudo apt install beast2-mcmc fastlink, fastlink-doc It is the old merry fastlink 4.1P, https://www.ncbi.nlm.nih.gov/CBBresearch/Schaffer/fastlink.html . sudo apt install fastlink loki, loki-doc It implements MCMC method for linkage analysis. available from https://www.stat.washington.edu/thompson/Genepi/Loki.shtml sudo apt install loki See usr/share/doc/loki-doc/ for documentation. merlin This is according to http://csg.sph.umich.edu/abecasis/merlin/index.html , sudo `which conda` install merlin plink1.9 This is recently available, sudo apt install plink1.9 plink1.9 --help --bfile simwalk2 This is according to http://www.genetics.ucla.edu/software/ , sudo `which conda` install simwalk2","title":"Miscellaneous software"},{"location":"misc/#miscellaneous-software","text":"The source code is also available from the Ubuntu archive. There is a variety of packages in bioconda, https://bioconda.github.io/recipes.html# and conda-forge, https://conda-forge.org/feedstocks/. We can trick the test even with compiling errors, e.g., sudo apt install loki wget -qO- http://archive.ubuntu.com/ubuntu/pool/universe/l/loki/loki_2.4.7.4.orig.tar.gz | tar fvxz - cd loki ./configure # there are errors in compilng make cp /usr/bin/prep presrc cp /usr/bin/loki lokisrc cd test # we are actually fine make","title":"Miscellaneous software"},{"location":"misc/#allegro","text":"This is according to https://www.decode.com/software/, sudo `which conda` install allegro Gudbjartsson DF, Thorvaldsson T, Kong A, Gunnarsson G, Ingolfsdottir A (2005). Allegro version 2. Nature Genetics 37:1015\u20131016 https://www.nature.com/articles/ng1005-1015?foxtrotcallback=true","title":"allegro"},{"location":"misc/#beast2-mcmc-beast2-mcmc-doc","text":"It is Bayesian MCMC phylogenetic inference. sudo apt install beast2-mcmc","title":"beast2-mcmc, beast2-mcmc-doc"},{"location":"misc/#fastlink-fastlink-doc","text":"It is the old merry fastlink 4.1P, https://www.ncbi.nlm.nih.gov/CBBresearch/Schaffer/fastlink.html . sudo apt install fastlink","title":"fastlink, fastlink-doc"},{"location":"misc/#loki-loki-doc","text":"It implements MCMC method for linkage analysis. available from https://www.stat.washington.edu/thompson/Genepi/Loki.shtml sudo apt install loki See usr/share/doc/loki-doc/ for documentation.","title":"loki, loki-doc"},{"location":"misc/#merlin","text":"This is according to http://csg.sph.umich.edu/abecasis/merlin/index.html , sudo `which conda` install merlin","title":"merlin"},{"location":"misc/#plink19","text":"This is recently available, sudo apt install plink1.9 plink1.9 --help --bfile","title":"plink1.9"},{"location":"misc/#simwalk2","text":"This is according to http://www.genetics.ucla.edu/software/ , sudo `which conda` install simwalk2","title":"simwalk2"},{"location":"pharmacogenomics/","text":"Pharmacogenomics Data sources Clinical Trials Web: https://ClinicalTrials.gov PharmGKB Web: https://api.pharmgkb.org pharos Web: https://pharos.nih.gov/ The Ligandable Human Proteome Web: http://polymorph.sgc.utoronto.ca/drugged_human_proteome/ Software Python/chembl_webresource_client Web: https://github.com/chembl/chembl_webresource_client pip install chembl_webresource_client UniProtID mapping API: https://www.uniprot.org/help/api_idmapping import urllib.parse import urllib.request url = 'https://www.uniprot.org/uploadlists/' params = { 'from': 'ACC+ID', 'to': 'CHEMBL_ID', 'format': 'tab', 'query': 'P40925 P40926 O43175 Q9UM73 P97793' } data = urllib.parse.urlencode(params) data = data.encode('utf-8') req = urllib.request.Request(url, data) with urllib.request.urlopen(req) as f: response = f.read() print(response.decode('utf-8')) R/Pi Location for database download: figshare . devtools::install_bioc(\"Pi\",build_vignettes = TRUE) library(Pi) browseVignettes(\"Pi\") Note that S4Vectors may conflict with the R-devel configurations. The figshare database as described in the paper can be examined as follows, gunzip pi_database.sql.gz mysql -p -u $USER -e \"create database pi;\" mysql -p -u $USER pi < pi_database.sql mysql -p -u $USER pi <<END show tables; desc pi_priority; desc pi_trait; desc pi_genomic; desc pi_drug; desc pi_category; desc pi_domain; desc pi_pdb; select * from pi_priority where trait='RA' and rank<=150 order by rank; select * from pi_priority where trait='RA' and crosstalk_node='Y' order by rank; END It is relatively easier to convert these as an R object , library(RMySQL) user <- \"my user name\" password <- \"mypassword\" mydb = dbConnect(MySQL(), user=user, password=password, dbname='pi') tbllist <- dbListTables(mydb) pi_category <- dbGetQuery(mydb, paste0(\"select * from pi_category;\")) pi_domain <- dbGetQuery(mydb, paste0(\"select * from pi_domain;\")) pi_drug <- dbGetQuery(mydb, paste0(\"select * from pi_drug;\")) pi_genomic <- dbGetQuery(mydb, paste0(\"select * from pi_genomic;\")) pi_pdb <- dbGetQuery(mydb, paste0(\"select * from pi_pdb;\")) pi_priority <- dbGetQuery(mydb, paste0(\"select * from pi_priority;\")) pi_trait <- dbGetQuery(mydb, paste0(\"select * from pi_trait;\")) # target category head(pi_category) # target superfamily druggable head(pi_domain) # target uniprot pdb_chain pdb chain pocket head(pi_pdb) # trait target rank rating nGene cGene eGene seed fGene pGene dGene gwas crosstalk_node num_neighbor approved phased druggable_category druggable_domain num_pdb num_pdb_with_druggable_pocket magnitude direction description head(pi_priority) traits <- c(\"AS\",\"CRO\",\"IGE\",\"MS\",\"RA\",\"T1D\",\"UC\") # trait target max_phase drug mechanism_of_action action_type source subset(pi_drug,trait%in%traits) # trait target type name snp snp_type pvalue subset(pi_genomic,trait%in%traits) save(pi_category,pi_domain,pi_drug,pi_genomic,pi_pdb,pi_priority,pi_trait,file=\"pi_database.rda\") R/dbparser https://cran.r-project.org/web/packages/dbparser/ R/rDGIdb https://www.bioconductor.org/packages/release/bioc/html/rDGIdb.html http://www.dgidb.org/ R/sunburstR https://github.com/timelyportfolio/sunburstR Techinical aspect: https://stackoverflow.com/questions/12926779/how-to-make-a-sunburst-plot-in-r-or-python Example: https://bl.ocks.org/kerryrodden/7090426 Reference Cotto KC, Wagner AH, Feng YY, Kiwala S, Coffman AC, Spies G, Wollam A, Spies NC, Griffith OL, Griffith M. DGIdb 3.0: a redesign and expansion of the drug-gene interaction database. Nucleic Acids Res. 2018 Jan 4;46(D1):D1068-D1073. doi: 10.1093/nar/gkx1143. PMID: 29156001; PMCID: PMC5888642. Fang H; ULTRA-DD Consortium, De Wolf H, Knezevic B, Burnham KL, Osgood J, Sanniti A, Lled\u00f3 Lara A, Kasela S, De Cesco S, Wegner JK, Handunnetthi L, McCann FE, Chen L, Sekine T, Brennan PE, Marsden BD, Damerell D, O'Callaghan CA, Bountra C, Bowness P, Sundstr\u00f6m Y, Milani L, Berg L, G\u00f6hlmann HW, Peeters PJ, Fairfax BP, Sundstr\u00f6m M, Knight JC. A genetics-led approach defines the drug target landscape of 30 immune-related traits. Nat Genet . 2019 Jul;51(7):1082-1091. doi: 10.1038/s41588-019-0456-1. Epub 2019 Jun 28. Wang J, Yazdani S, Han A, Schapira M. Structure-based view of the druggable genome. Drug Discovery Today 2020; 25:561-567.","title":"Pharmacogenomics"},{"location":"pharmacogenomics/#pharmacogenomics","text":"","title":"Pharmacogenomics"},{"location":"pharmacogenomics/#data-sources","text":"","title":"Data sources"},{"location":"pharmacogenomics/#clinical-trials","text":"Web: https://ClinicalTrials.gov","title":"Clinical Trials"},{"location":"pharmacogenomics/#pharmgkb","text":"Web: https://api.pharmgkb.org","title":"PharmGKB"},{"location":"pharmacogenomics/#pharos","text":"Web: https://pharos.nih.gov/","title":"pharos"},{"location":"pharmacogenomics/#the-ligandable-human-proteome","text":"Web: http://polymorph.sgc.utoronto.ca/drugged_human_proteome/","title":"The Ligandable Human Proteome"},{"location":"pharmacogenomics/#software","text":"","title":"Software"},{"location":"pharmacogenomics/#pythonchembl_webresource_client","text":"Web: https://github.com/chembl/chembl_webresource_client pip install chembl_webresource_client UniProtID mapping API: https://www.uniprot.org/help/api_idmapping import urllib.parse import urllib.request url = 'https://www.uniprot.org/uploadlists/' params = { 'from': 'ACC+ID', 'to': 'CHEMBL_ID', 'format': 'tab', 'query': 'P40925 P40926 O43175 Q9UM73 P97793' } data = urllib.parse.urlencode(params) data = data.encode('utf-8') req = urllib.request.Request(url, data) with urllib.request.urlopen(req) as f: response = f.read() print(response.decode('utf-8'))","title":"Python/chembl_webresource_client"},{"location":"pharmacogenomics/#rpi","text":"Location for database download: figshare . devtools::install_bioc(\"Pi\",build_vignettes = TRUE) library(Pi) browseVignettes(\"Pi\") Note that S4Vectors may conflict with the R-devel configurations. The figshare database as described in the paper can be examined as follows, gunzip pi_database.sql.gz mysql -p -u $USER -e \"create database pi;\" mysql -p -u $USER pi < pi_database.sql mysql -p -u $USER pi <<END show tables; desc pi_priority; desc pi_trait; desc pi_genomic; desc pi_drug; desc pi_category; desc pi_domain; desc pi_pdb; select * from pi_priority where trait='RA' and rank<=150 order by rank; select * from pi_priority where trait='RA' and crosstalk_node='Y' order by rank; END It is relatively easier to convert these as an R object , library(RMySQL) user <- \"my user name\" password <- \"mypassword\" mydb = dbConnect(MySQL(), user=user, password=password, dbname='pi') tbllist <- dbListTables(mydb) pi_category <- dbGetQuery(mydb, paste0(\"select * from pi_category;\")) pi_domain <- dbGetQuery(mydb, paste0(\"select * from pi_domain;\")) pi_drug <- dbGetQuery(mydb, paste0(\"select * from pi_drug;\")) pi_genomic <- dbGetQuery(mydb, paste0(\"select * from pi_genomic;\")) pi_pdb <- dbGetQuery(mydb, paste0(\"select * from pi_pdb;\")) pi_priority <- dbGetQuery(mydb, paste0(\"select * from pi_priority;\")) pi_trait <- dbGetQuery(mydb, paste0(\"select * from pi_trait;\")) # target category head(pi_category) # target superfamily druggable head(pi_domain) # target uniprot pdb_chain pdb chain pocket head(pi_pdb) # trait target rank rating nGene cGene eGene seed fGene pGene dGene gwas crosstalk_node num_neighbor approved phased druggable_category druggable_domain num_pdb num_pdb_with_druggable_pocket magnitude direction description head(pi_priority) traits <- c(\"AS\",\"CRO\",\"IGE\",\"MS\",\"RA\",\"T1D\",\"UC\") # trait target max_phase drug mechanism_of_action action_type source subset(pi_drug,trait%in%traits) # trait target type name snp snp_type pvalue subset(pi_genomic,trait%in%traits) save(pi_category,pi_domain,pi_drug,pi_genomic,pi_pdb,pi_priority,pi_trait,file=\"pi_database.rda\")","title":"R/Pi"},{"location":"pharmacogenomics/#rdbparser","text":"https://cran.r-project.org/web/packages/dbparser/","title":"R/dbparser"},{"location":"pharmacogenomics/#rrdgidb","text":"https://www.bioconductor.org/packages/release/bioc/html/rDGIdb.html http://www.dgidb.org/","title":"R/rDGIdb"},{"location":"pharmacogenomics/#rsunburstr","text":"https://github.com/timelyportfolio/sunburstR Techinical aspect: https://stackoverflow.com/questions/12926779/how-to-make-a-sunburst-plot-in-r-or-python Example: https://bl.ocks.org/kerryrodden/7090426","title":"R/sunburstR"},{"location":"pharmacogenomics/#reference","text":"Cotto KC, Wagner AH, Feng YY, Kiwala S, Coffman AC, Spies G, Wollam A, Spies NC, Griffith OL, Griffith M. DGIdb 3.0: a redesign and expansion of the drug-gene interaction database. Nucleic Acids Res. 2018 Jan 4;46(D1):D1068-D1073. doi: 10.1093/nar/gkx1143. PMID: 29156001; PMCID: PMC5888642. Fang H; ULTRA-DD Consortium, De Wolf H, Knezevic B, Burnham KL, Osgood J, Sanniti A, Lled\u00f3 Lara A, Kasela S, De Cesco S, Wegner JK, Handunnetthi L, McCann FE, Chen L, Sekine T, Brennan PE, Marsden BD, Damerell D, O'Callaghan CA, Bountra C, Bowness P, Sundstr\u00f6m Y, Milani L, Berg L, G\u00f6hlmann HW, Peeters PJ, Fairfax BP, Sundstr\u00f6m M, Knight JC. A genetics-led approach defines the drug target landscape of 30 immune-related traits. Nat Genet . 2019 Jul;51(7):1082-1091. doi: 10.1038/s41588-019-0456-1. Epub 2019 Jun 28. Wang J, Yazdani S, Han A, Schapira M. Structure-based view of the druggable genome. Drug Discovery Today 2020; 25:561-567.","title":"Reference"},{"location":"prottrans/","text":"Proteome and transcriptome CCprofiler/COPF https://github.com/CCprofiler/CCprofiler , https://sec-explorer.shinyapps.io/hela_cellcycle/ Heusel M, Bludau I, Rosenberger G, Hafen R, Frank M, Banaei-Esfahani A, Collins B, Gstaiger M, Aebersold R. Complex-centric proteome profiling by SEC-SWATH-MS. Mol Syst Biol 2019, 15:e8438 DOI: https://doi.org/10.15252/msb.20188438 Heusel et al. A global screen for assembly state changes of the mitotic proteome by SEC-SWATH-MS. Cell Systems 0220, 10:1\u201323, https://doi.org/10.1016/j.cels.2020.01.001 DOGMA DOGMA is a program for fast and easy quality assessment of transcriptome and proteome data based on conserved protein domains. See https://domainworld.uni-muenster.de/programs/dogma/ https://ebbgit.uni-muenster.de/domainWorld/DOGMA Immuno-SABER https://github.com/HMS-IDAC Saka SK, et al. (2019). Immuno-SABER enables highly multiplexed and amplified protein imaging in tissues. Nat Biotechnol , https://www.biorxiv.org/content/10.1101/401810v1. iProFun https://github.com/songxiaoyu/iProFun Song M, Ji J, Gleason KJ, Yang F, Martignetti JA, Chen LS, Wang P, Insights into impact of DNA copy number alteration and methylation on the proteogenomic landscape of human ovarian cancer via a multi-omics integrative analysis Mol Cell Proteomics MRMassaydb http://mrmassaydb.proteincentre.com/fdaassay/ Percolator Semi-supervised learning for peptide identification from shotgun proteomics dataset, and Single Cell Proteomics by Mass Spectrometry (SCOPE-MS). http://crux.ms/ , http://percolator.ms/ and https://github.com/percolator/percolator . McIlwain S, Tamura K, Kertesz-Farkas A, Grant CE, Diament B, Frewen B, Howbert JJ, Hoopmann MR, K\u00e4ll L, Eng JK, MacCoss MJ, Noble WS (2014). Crux: rapid open source protein tandem mass spectrometry analysis. J Proteome Res 13(10):4488-4491. Fondrie WE, Noble WS (2020). Machine learning strategy that leverages large data sets to boost statistical power in small-scale experiments. J Proteome Res PeCorA https://github.com/jessegmeyerlab/PeCorA PIVar https://github.com/WeiWenqing/PIVar Teng H, et al. Prevalence and architecture of posttranscriptionally impaired synonymous mutations in 8,320 genomes across 22 cancer types [published online ahead of print, 2020 Jan 17]. Nucleic Acids Res. 2020;gkaa019. doi:10.1093/nar/gkaa019 ProteomeXchange http://proteomecentral.proteomexchange.org/cgi/GetDataset , also PRotein IDEntification databases (PRIDE): https://www.ebi.ac.uk/ ProteoformAnalysis https://github.com/ibludau/ProteoformAnanlysis http://proteoformviewer.ethz.ch/ , http://proteoformviewer.ethz.ch/ProteoformExplorer_final_classes # In silico benchmark. install_github(\"CCprofiler/CCprofiler\", ref = \"proteoformLocationMapping\") R --no-save -q < InterlabBenchmark_final_paper.R # COPF analysis of the cell cycle SEC-SWATH-MS dataset ## E1709051521_feature_alignment.tsv wget -qO- http://ftp.pride.ebi.ac.uk/pride/data/archive/2019/05/PXD010288/SWATH_QueryResultGlobalAlignment_20170906213658025-1332538.zip | \\ unzip SWATH_QueryResultGlobalAlignment_20170906213658025-1332538.zip wget http://ftp.pride.ebi.ac.uk/pride/data/archive/2019/05/PXD010288/HeLaCCL2_SEC_annotation_full.xlsx ... Bludau I. et al. Systematic detection of functional proteoform groups from bottom-up proteomic datasets. Nat Comm 2021, 12:3810 QTLtools https://qtltools.github.io/qtltools/ RNAsnp https://rth.dk/resources/rnasnp/software ViennaRNA https://www.tbi.univie.ac.at/RNA/","title":"Proteome and transcriptome"},{"location":"prottrans/#proteome-and-transcriptome","text":"","title":"Proteome and transcriptome"},{"location":"prottrans/#ccprofilercopf","text":"https://github.com/CCprofiler/CCprofiler , https://sec-explorer.shinyapps.io/hela_cellcycle/ Heusel M, Bludau I, Rosenberger G, Hafen R, Frank M, Banaei-Esfahani A, Collins B, Gstaiger M, Aebersold R. Complex-centric proteome profiling by SEC-SWATH-MS. Mol Syst Biol 2019, 15:e8438 DOI: https://doi.org/10.15252/msb.20188438 Heusel et al. A global screen for assembly state changes of the mitotic proteome by SEC-SWATH-MS. Cell Systems 0220, 10:1\u201323, https://doi.org/10.1016/j.cels.2020.01.001","title":"CCprofiler/COPF"},{"location":"prottrans/#dogma","text":"DOGMA is a program for fast and easy quality assessment of transcriptome and proteome data based on conserved protein domains. See https://domainworld.uni-muenster.de/programs/dogma/ https://ebbgit.uni-muenster.de/domainWorld/DOGMA","title":"DOGMA"},{"location":"prottrans/#immuno-saber","text":"https://github.com/HMS-IDAC Saka SK, et al. (2019). Immuno-SABER enables highly multiplexed and amplified protein imaging in tissues. Nat Biotechnol , https://www.biorxiv.org/content/10.1101/401810v1.","title":"Immuno-SABER"},{"location":"prottrans/#iprofun","text":"https://github.com/songxiaoyu/iProFun Song M, Ji J, Gleason KJ, Yang F, Martignetti JA, Chen LS, Wang P, Insights into impact of DNA copy number alteration and methylation on the proteogenomic landscape of human ovarian cancer via a multi-omics integrative analysis Mol Cell Proteomics","title":"iProFun"},{"location":"prottrans/#mrmassaydb","text":"http://mrmassaydb.proteincentre.com/fdaassay/","title":"MRMassaydb"},{"location":"prottrans/#percolator","text":"Semi-supervised learning for peptide identification from shotgun proteomics dataset, and Single Cell Proteomics by Mass Spectrometry (SCOPE-MS). http://crux.ms/ , http://percolator.ms/ and https://github.com/percolator/percolator . McIlwain S, Tamura K, Kertesz-Farkas A, Grant CE, Diament B, Frewen B, Howbert JJ, Hoopmann MR, K\u00e4ll L, Eng JK, MacCoss MJ, Noble WS (2014). Crux: rapid open source protein tandem mass spectrometry analysis. J Proteome Res 13(10):4488-4491. Fondrie WE, Noble WS (2020). Machine learning strategy that leverages large data sets to boost statistical power in small-scale experiments. J Proteome Res","title":"Percolator"},{"location":"prottrans/#pecora","text":"https://github.com/jessegmeyerlab/PeCorA","title":"PeCorA"},{"location":"prottrans/#pivar","text":"https://github.com/WeiWenqing/PIVar Teng H, et al. Prevalence and architecture of posttranscriptionally impaired synonymous mutations in 8,320 genomes across 22 cancer types [published online ahead of print, 2020 Jan 17]. Nucleic Acids Res. 2020;gkaa019. doi:10.1093/nar/gkaa019","title":"PIVar"},{"location":"prottrans/#proteomexchange","text":"http://proteomecentral.proteomexchange.org/cgi/GetDataset , also PRotein IDEntification databases (PRIDE): https://www.ebi.ac.uk/","title":"ProteomeXchange"},{"location":"prottrans/#proteoformanalysis","text":"https://github.com/ibludau/ProteoformAnanlysis http://proteoformviewer.ethz.ch/ , http://proteoformviewer.ethz.ch/ProteoformExplorer_final_classes # In silico benchmark. install_github(\"CCprofiler/CCprofiler\", ref = \"proteoformLocationMapping\") R --no-save -q < InterlabBenchmark_final_paper.R # COPF analysis of the cell cycle SEC-SWATH-MS dataset ## E1709051521_feature_alignment.tsv wget -qO- http://ftp.pride.ebi.ac.uk/pride/data/archive/2019/05/PXD010288/SWATH_QueryResultGlobalAlignment_20170906213658025-1332538.zip | \\ unzip SWATH_QueryResultGlobalAlignment_20170906213658025-1332538.zip wget http://ftp.pride.ebi.ac.uk/pride/data/archive/2019/05/PXD010288/HeLaCCL2_SEC_annotation_full.xlsx ... Bludau I. et al. Systematic detection of functional proteoform groups from bottom-up proteomic datasets. Nat Comm 2021, 12:3810","title":"ProteoformAnalysis"},{"location":"prottrans/#qtltools","text":"https://qtltools.github.io/qtltools/","title":"QTLtools"},{"location":"prottrans/#rnasnp","text":"https://rth.dk/resources/rnasnp/software","title":"RNAsnp"},{"location":"prottrans/#viennarna","text":"https://www.tbi.univie.ac.at/RNA/","title":"ViennaRNA"},{"location":"scRNASeq/","text":"scRNA-Seq Bioconductor branch, http://bioconductor.org/packages/release/BiocViews.html#___SingleCell. benchmarks Abdelaal et al. A comparison of automatic cell identification methods for single-cell RNA sequencing data. Genome Biology (2019) 20:194, https://doi.org/10.1186/s13059-019-1795-z Single cell RNA-seq data analysis bundle . Popescu D-M et al. Decoding human fetal liver haematopoiesis. Nature https://doi.org/10.1038/s41586-019-1652-y . CReSCENT: CanceR Single Cell ExpressioN Toolkit https://crescent.cloud/ Mohanraj, S., et al. (2020). \"CReSCENT: CanceR Single Cell ExpressioN Toolkit.\" Nucleic Acids Research . SCANPY https://github.com/theislab/Scanpy SCENIC https://aertslab.org/#scenic and https://pyscenic.readthedocs.io/en/latest/. scMAGeCK https://bitbucket.org/weililab/scmageck/src/master/ Yang L, et al. scMAGeCK links genotypes with multiple phenotypes in single-cell CRISPR screens. Genome Biol 21, 19 (2020). https://doi.org/10.1186/s13059-020-1928-4 Seurat https://satijalab.org/seurat/ Stuart T, et al. Comprehensive Integration of Single-Cell Data. Cell. 2019;177(7):1888\u20131902.e21. doi:10.1016/j.cell.2019.05.031 DENDRO https://github.com/zhouzilu/DENDRO Zhou Z, et al. DENDRO: genetic heterogeneity profiling and subclone detection by single-cell RNA sequencing. Genome Biol 21, 10 (2020). https://doi.org/10.1186/s13059-019-1922-x souporcell https://github.com/wheaton5/souporcell Heaton H, et al. Souporcell: robust clustering of single-cell RNA-seq data by genotype without reference genotypes. Nat Methods (2020). https://doi.org/10.1038/s41592-020-0820-1","title":"scRNA-Seq"},{"location":"scRNASeq/#scrna-seq","text":"Bioconductor branch, http://bioconductor.org/packages/release/BiocViews.html#___SingleCell. benchmarks Abdelaal et al. A comparison of automatic cell identification methods for single-cell RNA sequencing data. Genome Biology (2019) 20:194, https://doi.org/10.1186/s13059-019-1795-z Single cell RNA-seq data analysis bundle . Popescu D-M et al. Decoding human fetal liver haematopoiesis. Nature https://doi.org/10.1038/s41586-019-1652-y .","title":"scRNA-Seq"},{"location":"scRNASeq/#crescent-cancer-single-cell-expression-toolkit","text":"https://crescent.cloud/ Mohanraj, S., et al. (2020). \"CReSCENT: CanceR Single Cell ExpressioN Toolkit.\" Nucleic Acids Research .","title":"CReSCENT: CanceR Single Cell ExpressioN Toolkit"},{"location":"scRNASeq/#scanpy","text":"https://github.com/theislab/Scanpy","title":"SCANPY"},{"location":"scRNASeq/#scenic","text":"https://aertslab.org/#scenic and https://pyscenic.readthedocs.io/en/latest/.","title":"SCENIC"},{"location":"scRNASeq/#scmageck","text":"https://bitbucket.org/weililab/scmageck/src/master/ Yang L, et al. scMAGeCK links genotypes with multiple phenotypes in single-cell CRISPR screens. Genome Biol 21, 19 (2020). https://doi.org/10.1186/s13059-020-1928-4","title":"scMAGeCK"},{"location":"scRNASeq/#seurat","text":"https://satijalab.org/seurat/ Stuart T, et al. Comprehensive Integration of Single-Cell Data. Cell. 2019;177(7):1888\u20131902.e21. doi:10.1016/j.cell.2019.05.031","title":"Seurat"},{"location":"scRNASeq/#dendro","text":"https://github.com/zhouzilu/DENDRO Zhou Z, et al. DENDRO: genetic heterogeneity profiling and subclone detection by single-cell RNA sequencing. Genome Biol 21, 10 (2020). https://doi.org/10.1186/s13059-019-1922-x","title":"DENDRO"},{"location":"scRNASeq/#souporcell","text":"https://github.com/wheaton5/souporcell Heaton H, et al. Souporcell: robust clustering of single-cell RNA-seq data by genotype without reference genotypes. Nat Methods (2020). https://doi.org/10.1038/s41592-020-0820-1","title":"souporcell"}]}